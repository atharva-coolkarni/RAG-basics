{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4208246",
   "metadata": {},
   "source": [
    "## RAG Pipeline: Data Ingestion to vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a123a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\RAG-basics\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6de0070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      "Processing: finetune.pdf\n",
      "  ✓ Loaded 15 pages\n",
      "\n",
      "Processing: incontext_learning.pdf\n",
      "  ✓ Loaded 19 pages\n",
      "\n",
      "Processing: metrics.pdf\n",
      "  ✓ Loaded 12 pages\n",
      "\n",
      "Processing: modular_rag.pdf\n",
      "  ✓ Loaded 17 pages\n",
      "\n",
      "Total documents loaded: 63\n"
     ]
    }
   ],
   "source": [
    "# Read all PDFs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab57c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Finetune-RAG: Fine-Tuning Language Models to\\nResist Hallucination in Retrieval-Augmented\\nGeneration\\nZhan Peng Lee\\nPints AI Labs\\nzhanpeng.lee@pints.co\\nAndre Lin∗\\nPints AI Labs\\nandre_lin@u.nus.edu\\nandrelim444@gmail.com\\nCalvin Tan\\nPints AI Labs\\ncalvin@pints.co\\nAbstract\\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to\\nimprove factuality in large language models (LLMs) by grounding their outputs in\\nretrieved documents. However, ensuring perfect retrieval of relevant information\\nremains challenging, and when irrelevant content is passed downstream to an LLM,\\nit can lead to hallucinations. In this work, we propose Finetune-RAG, a simple\\nand effective fine-tuning approach that features the first-of-its-kind RAG training\\ndataset constructed to mimic real-world imperfections. Experimental results show\\nthat Finetune-RAG improves factual accuracy by 21.2% over the base model. We\\nalso propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests\\nmodels under realistic imperfect retrieval scenarios. Our codebase2 and dataset3\\nare fully open sourced for community use.\\n1 Introduction\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of\\nnatural language processing tasks (Wang et al., 2023; Rozière et al., 2024; Cui et al., 2025; Yasunaga\\net al., 2022; Liu et al., 2024). However, their tendency to \"hallucinate\", that is, to produce fluent\\nbut factually incorrect information, remains a persistent challenge (Li et al., 2024a; Duan et al.,\\n2024; Zhang et al., 2023), particularly in high-stakes domains such as healthcare, law, and finance\\n(Agarwal et al., 2024; Dahl et al., 2024; Kang and Liu, 2023). To address this,Retrieval-Augmented\\nGeneration (RAG)has become a popular solution. Instead of relying solely on parametric memory,\\nRAG systems retrieve external documents and condition the model’s response on this evidence.\\nIn practice, retrieval accuracy in RAG is far from flawless. Retrieved documents may be outdated,\\nmisleading, or topically adjacent but factually incorrect. These errors can propagate downstream,\\nleading models to blend inaccurate context into fluent but false answers. This is especially concerning\\nin domains such as law, compliance, financial reporting, or medicine, where mistakes can have\\nwide-ranging repercussions.\\nMost prior work has addressed this issue from the retrieval perspective, focusing on improving\\nretrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024; Dong et al.,\\n2024; Zhou and Chen, 2025). In contrast, relatively little attention has been given to improving the\\nmodel’s ability to resist using the incorrect information.\\nIn this paper, we introduceFinetune-RAG, a method that directly targets hallucination by fine-tuning\\nthe model with imperfect RAG samples that mimic real-world retrieval scenarios. We constructed a\\n∗Work was done during an internship at Pints AI\\n2https://github.com/Pints-AI/Finetune-Bench-RAG\\n3https://huggingface.co/datasets/pints-ai/Finetune-RAG\\narXiv:2505.10792v3  [cs.CL]  3 Dec 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='diverse dataset covering legal documents, scientific literature, books, and web data, each paired with\\na plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta’s\\nLlama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: aBaseline\\nformatand aStructured XMLvariant. This setup allows us to assess generalization and prompt\\nsensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing\\nRAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the\\nLLM’s ability to extract coherent answers from relevant chunks.\\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye,\\nsuch as financial reports from a similarly named company or outdated information based on dates\\nindicated by document metadata. Through fine-tuning models with a controlled mixture of true and\\nfalse context placed alongside, we teach them to ground their answers exclusively in the reliable\\ninformation provided.\\nWe evaluated the effectiveness of Finetune-RAG usingBench-RAG, a custom benchmarking suite\\nwe have created that leveragesGPT-4o(OpenAI, 2024) as an automated judge to assess the accu-\\nracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG\\nsubstantially improves factual correctness while maintaining output quality across other dimensions,\\ndemonstrating that generation-time defenses are a viable complement to improved retrieval.\\nOur contributions are as follows:\\n• Fine-tuning Approach.We propose a novel fine-tuning strategy for RAG systems that\\nteaches models to ignore misleading context and generate answers based solely on factual\\ninput.\\n• Training Dataset.We release a curated, multi-domain dataset designed for hallucination\\nresistance training, with both factual and fictitious content.\\n• Evaluation Setup.We benchmark the effectiveness of our approach using GPT-4o-based\\nevaluations and show significant gains in factual accuracy without compromising helpfulness\\nor relevance.\\n• Open-source release.We make our code, models, dataset, and evaluation framework\\npublicly available to facilitate further research. They can be accessed in our open-source\\nrepository4 and dataset5.\\nBy fine-tuning LLMs on RAG examples containing both factual and fictitious documents, we show\\nthat it is possible to build models that can reliably choose truth over noise. Our dataset reflects noisy,\\ndomain-diverse retrieval as encountered in practice, making it a strong foundation for stress-testing\\nhallucination resistance in future RAG systems.\\n2 Background\\n2.1 Retrieval-Augmented Generation\\nRetrieval-Augmented Generation (RAG) augments large language models by incorporating external\\ndocuments into the generation process. Rather than relying solely on the model’s internal parameters,\\nRAG retrieves relevant passages from a knowledge base and feeds them, along with the user query,\\ninto the model to guide its response (Zhou et al., 2024).\\nA standard RAG system operates in two phases:\\n•Retrieval.A retriever model selects the top-kmost relevant documents for a given query.\\n• Generation.A language model generates a response conditioned on both the query and the\\nretrieved documents.\\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information,\\nwhich is especially useful in fast-changing or specialized fields. However, it also introduces new\\nfailure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).\\n4https://github.com/Pints-AI/Finetune-Bench-RAG\\n5https://huggingface.co/datasets/pints-ai/Finetune-RAG\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='2.2 Hallucination in Language Models\\nHallucination refers to the phenomenon where language models produce outputs that are factually\\nincorrect or unsupported by the input, resulting in unfaithful outputs (Rawte et al., 2023). In RAG\\nsystems, hallucination can be especially problematic when the model is presented with a mixture of\\nrelevant and irrelevant (or even misleading) context. Even with carefully worded prompts, models\\ncan inadvertently \"trust\" incorrect sources and generate plausible but wrong answers (Yoran et al.,\\n2024).\\nDespite the presence of external context, most current models lack mechanisms to actively filter or\\nignore misleading information once it is included in the prompt (Shi et al., 2023). Finetune-RAG\\nspecifically targets this weakness by training models to develop this filtering capability.\\n3 Related Works\\n3.1 Mitigating Hallucination with Synthetic Prompt Tuning\\nSYNTRA (Jones et al., 2023) reduces hallucinations in large language models by modifying the\\nmodel’s instructions rather than adjusting its internal weights. SYNTRA does this by attaching a\\nsmall, trainable embedding vector to the system message, which acts as an additional instruction\\nprefix. This vector is optimized using a synthetic task where hallucinations are easy to measure. For\\nexample, the model is prompted to return names starting with a specific letter from a visible list, and\\nany incorrect or invented names are counted as hallucinations. By learning to avoid such mistakes in\\na controlled setting, the model can generalize to reduce hallucinations in downstream tasks. However,\\nbecause SYNTRA focuses on modifying prompts and not the model’s internal reasoning, it does not\\nenable the model to distinguish between factual and misleading content, failing to address real-world\\nRAG scenarios (Barnett et al., 2024)(Shi et al., 2023).\\n3.2 Refusal-Aware Fine-Tuning\\nZhang et al. (2024) propose a fine-tuning method, R-Tuning, that teaches language models to express\\nuncertainty and decline to answer when a question falls outside their pre-trained knowledge. This is\\nachieved by identifying questions the model answers incorrectly during training and appending an\\nuncertainty statement such as “I am unsure” to those responses. The result is a model that behaves\\nmore conservatively and with improved confidence calibration. However, R-Tuning is designed for\\nclosed-book settings, where the model relies only on its internal knowledge without a RAG system.\\n3.3 Constrained Reasoning with Decompose-and-Query (D&Q)\\nCao et al. (2023) propose the Decompose-and-Query (D&Q) framework, which extends retrieval-\\naugmented generation (RAG) by teaching language models to break down complex queries, retrieve\\nrelevant information using external tools, and generate answers based on a structured knowledge\\nsource. In particular, D&Q introduces a curated question–answer (QA) base, which is a collection of\\nverified QA pairs that the model consults during reasoning. This setup helps reduce hallucinations\\nby constraining the model to reliable content and allowing it to backtrack when inconsistencies are\\ndetected.\\nHowever, the effectiveness of D&Q depends strongly on the quality and coverage of its QA base. In\\npractical RAG applications, where retrieved content can be noisy, ambiguous, or incomplete (Shi\\net al., 2023), relying on a fixed and curated source may become a limitation. Since the framework\\nlacks mechanisms to dynamically assess the reliability of new information, it remains susceptible to\\nhallucinations caused by misleading or inaccurate context.\\n4 Methodology\\nWe introduceFinetune-RAG, a fine-tuning method designed to train large language models (LLMs)\\nto distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG)\\nsetup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase,\\nFinetune-RAG focuses on improving the model’s generation behavior when faced with imperfect or\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='misleading inputs. Our core idea is to fine-tune the model using examples where both correct and\\nincorrect information are explicitly presented to model, allowing it to learn the ability to sift out the\\ncorrect information to use for its response.\\n4.1 Problem Setup\\nIn a typical RAG system, the model is given a user query q and a set of retrieved documents\\n{d1, d2, ..., dk} (Zhou et al., 2024). When any of the documents is irrelevant or misleading, the model\\nmay generate incorrect responses (Yoran et al., 2024).\\nIn Finetune-RAG, we simulate this scenario during training by constructing prompts that include:\\n• One correct (factual) document chunkd correct\\n• One fictitious (misleading) document chunkd fictitious\\n• A corresponding questionq\\n• A reference answera, written using onlyd correct as the reference\\nThe model is then trained using supervised fine-tuning to produce the answer a despite having access\\nto bothd correct andd fictitious in the input.\\nIn Bayesian modeling, we can think of the task as a conditional generation problem where the goal is\\nto maximize the probability of generating a truthful answer a given a question q and a mixed set of\\ncontexts (some correctd correct, some fictitiousd fictitious).\\nWe aim to model:\\nP(a|q, d correct, dfictitious)(1)\\nHowever, this is the observed conditional probability, and what we want the model to learn is to\\nignore dfictitious and generate the answer as if conditioned only on dcorrect. So our training objective is\\nto align to the following idealized posterior:\\nP∗(a|q, dcorrect, dfictitious)→P(a|q, d correct)(2)\\nIn other words, even though the model receives both correct and fictitious information, it must assign\\nzero (or negligible) attention/mass tod fictitious during decoding.\\n4.2 Prompt Construction\\nEach training example in Finetune-RAG is processed to include asystem messageand auser\\nmessage, following the standard instruction-tuning format (Ouyang et al., 2022) used in chat-style\\nlanguage models. The system message defines the behavior of the assistant, while the user message\\nprovides the question along with correct and fictitious information.\\n4.2.1 System Message\\nThe system message is consistent in all training examples. It instructs the assistant to rely solely on\\nthe provided context and discourages the use of prior knowledge or hallucination:\\n\"Some information is retrieved from the database as provided based on the\\nuser’s question. The assistant is to answer the question to the best of\\nhis/her ability, using only the information provided. The assistant must\\nnot add his/her own knowledge.\"\\n4.2.2 User Message\\nTo help the model distinguish between factual and fictitious context more effectively, we explore the\\nuse of XML-like (Bray et al., 1998) structured input. We hypothesize that introducing a consistent\\nand explicit hierarchy, where document chunks are clearly labeled and separated, can make it easier\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='for the model to parse and evaluate different sources of information. This is especially important in\\nRAG settings, where hallucinations often result from the model blending or misattributing content\\nacross documents. Our approach aligns with findings from recent work such as StructRAG (Li et al.,\\n2024b) and SRAG (Lin et al., 2025), which demonstrates that task-specific structured representations\\nsuch as tables or graphs can significantly improve the performance of LLMs on knowledge-intensive\\nreasoning tasks. Our use of XML aims to impose syntactic clarity and boundary enforcement at the\\ninput level.\\nTo test this, we compare two user message formats: an unstructuredBaseline Formatand a structured\\nXML Format. Both present a question along with two document chunks, one factual and one\\nfictitious, but differ in how the information is presented. Refer to Section 6.4 for the exact prompt\\nstructure.\\n5 Experimental Setup\\n5.1 Model\\nWe fine-tuned Meta’s Llama 3.1–8B-Instruct (Grattafiori et al., 2024), an instruct-tuned model that\\nsupports chat-style interaction and long context windows. We adapt the system and user message\\nformatting based on the chosen prompt structure described in Section 4.2.2.\\n5.2 Dataset and Preprocessing\\nOur dataset contains a total of 1,653 examples from diverse domains, such as legal documents,\\nscientific papers, news articles, and technical reports. For the complete structure of each example in\\nthe dataset, refer to Annex A.\\nEach example is formatted in both the baseline and XML structures. The dataset is then partitioned\\ninto training (80%), validation (10%), and test (10%) sets.\\n5.3 Hyperparameters\\nWe selected hyperparameter values that balance model performance with computational efficiency.\\nRefer to Table 1 for the complete set of hyperparameters used.\\nTable 1: Fine-tuning hyperparameters used on Llama 3.1-8B-Instruct\\nParameter Value\\nSteps 20\\nBatch size 64\\nLearning rate 2e-5\\nWarmup ratio 0.1\\nLR Scheduler Cosine decay\\nOptimizer AdamW\\nβ1 0.9\\nβ2 0.95\\nWeight decay 0.1\\nMixed precision BF16\\n5.4 Checkpoints and Reproducibility\\nWe have released the model checkpoints fine-tuned with both Baseline6 and XML7 formats on Hug-\\ngingFace. Each prompt structure has two repositories, and each repository contains five checkpoints,\\ntotaling 10 checkpoints each.\\n6https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-2\\n7https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-2\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='6 Evaluation\\nWe evaluate Finetune-RAG’s ability to generate factually accurate answers when presented with both\\ncorrect and fictitious context. Our evaluation framework focuses on measuring whether the model\\nis able toselectively use only the correct information, and we assess output quality across four key\\ndimensions.\\n6.1 Bench-RAG\\nWe adopt a custom benchmarking pipeline, namelyBench-RAG, usingGPT-4omodel (OpenAI,\\n2024) in a LLM-as-a-judge inspired by prior work(Zheng et al., 2023; Gu et al., 2025; Li et al., 2025).\\nUsing structured prompts to elicit consistent evaluations for each model output, we measure:\\n• Accuracy: A binary metric indicating whether the generated answer is factually correct and\\nbased solely on the correct chunk. (True/False)\\n• Helpfulness: A score from 1 to 10 assessing how useful the answer is in addressing the\\nuser’s question.\\n•Relevance: A score from 1 to 10 measuring how relevant the content is to the query.\\n•Depth: A score from 1 to 10 reflecting the level of detail or insight present in the answer.\\nEach generated output is rated using a structured prompt format, which requests scores across these\\ncategories and a brief justification. Refer to Appendix B for the full structure. This methodology\\ndraws from recent research demonstrating that LLMs can align closely with human preferences when\\nprompted properly, achieving high inter-rater agreement, i.e. multiple evaluators provide consistent\\nratings for the same outputs (Gu et al., 2025; Li et al., 2025).\\n6.2 Checkpoints Evaluated\\nFor each prompt structure, we evaluate all 10 model checkpoints saved during training (see Section\\n5.4). These checkpoints represent the model’s learning trajectory over the course of a single fine-\\ntuning epoch. At each checkpoint, we generate answers to the test dataset questions using both the\\ncorrect context dcorrect and the fictitious context dfictitious. The generated answers are then submitted to\\nthe evaluator for scoring. Refer to Appendix B.1 and B.2 for the structure of the prompt used for\\nevaluation.\\n6.3 Results\\nWe report quantitative results from our fine-tuning experiments scored across 4 dimensions:factual\\naccuracy, helpfulness, relevance, and depth. Evaluation was performed using GPT-4o (OpenAI,\\n2024) as an LLM judge, as described in Section 6.1. We then aggregate the scores of each sequence\\nin the test dataset to derive the final evaluation result for each checkpoint:\\n¯Accuracy=\\n \\n1\\nntest\\nntestX\\ni=1\\n1[Accuracyi =T rue]\\n!\\n×100%(3)\\n¯Helpfulness= 1\\nntest\\nntestX\\ni=1\\nHelpfulness i (4)\\n¯Relevance= 1\\nntest\\nntestX\\ni=1\\nRelevancei (5)\\n¯Depth= 1\\nntest\\nntestX\\ni=1\\nDepthi (6)\\nFigures 1 and 2 summarize performance trends across training steps. We observe consistent improve-\\nments in factual accuracy over time, particularly in the Baseline format. In most cases, gains in\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='accuracy are achieved without sacrificing helpfulness or relevance, and in later checkpoints, all four\\nmetrics reach strong levels of performance.\\nNotably, accuracy rises from 76.97% at step 0 to 98.18% at step 20 in the Baseline format, demon-\\nstrating the model’s increasing ability to ignore fictitious context. Helpfulness and depth also improve\\nsteadily, with a dip at the first generated checkpoint.\\nFigure 1: Evaluation results across training steps (Baseline format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. (%) Help Rel. Depth\\n0 76.97 8.81 9.55 8.32\\n2 67.88 7.08 7.48 6.76\\n4 91.52 8.08 8.47 7.15\\n6 93.94 9.58 9.83 8.81\\n8 96.36 9.38 9.61 8.55\\n10 97.58 9.33 9.62 8.51\\n12 96.36 9.52 9.78 8.80\\n14 96.97 9.73 9.91 9.01\\n16 97.589.789.959.06\\n18 97.58 9.77 9.95 9.05\\n2098.189.779.959.02 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\nFigure 2: Evaluation results across training steps (XML format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. Help Rel Depth\\n0 78.79 8.81 9.56 8.19\\n2 52.73 5.79 6.16 5.24\\n4 87.88 6.56 7.09 5.47\\n6 95.76 9.46 9.73 8.75\\n8 94.55 9.09 9.35 8.21\\n10 94.55 8.93 9.32 8.01\\n12 95.76 8.95 9.33 8.05\\n14 95.76 9.28 9.59 8.52\\n16 97.58 9.35 9.61 8.61\\n1897.589.28 9.50 8.50\\n20 96.979.40 9.64 8.64 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='6.4 Ablation: Effect of Prompt Structure\\nTo assess the impact of prompt formatting on hallucination resistance, we perform an ablation study\\ncomparing two versions of Finetune-RAG: one trained using theBaseline formatand another using\\na more structuredXML format. Both models were fine-tuned on the same dataset with identical\\nhyperparameters and evaluated using the same GPT-4o-based benchmarking pipeline.\\nPrompt Format DifferencesThe Baseline format presents context in a flat, unstructured layout,\\nwhile the XML format uses nested tags to explicitly delineate retrieved content blocks (see Section\\n4.2.2). We hypothesized that structured formatting might help the model better separate and reason\\nabout distinct chunks.\\nBaseline FormatThis format presents the retrieved content in a plain and direct layout:\\nFilename: {filename1}\\nInformation:\\n{content1}\\nFilename: {filename2}\\nInformation:\\n{content2}\\nQuestion: {question}\\nXML FormatThis version wraps the content in an XML-like structure for clearer boundaries:\\n<Results>\\n<Result>\\n<Filename>{filename1}</Filename>\\n<Information>{content1}</Information>\\n</Result>\\n<Result>\\n<Filename>{filename2}</Filename>\\n<Information>{content2}</Information>\\n</Result>\\n</Results>\\nQuestion: {question}\\nResultsAs shown in Figures 1 and 2, both models demonstrate strong improvements over time.\\nHowever, the Baseline model consistently achieves higher accuracy and better overall scores in the\\nlater checkpoints:\\n• At step 20, the Baseline-tuned model achieves an accuracy of 98.18%, compared to 96.97%\\nfor the XML-tuned model.\\n• The Baseline-tuned model also maintains slightly higher scores for helpfulness (9.77 vs\\n9.40) and depth (9.02 vs 8.64).\\nInterpretationThese results suggest that while XML-style formatting introduces clear structural\\nboundaries that aid human readers, it did not consistently outperform the simpler Baseline prompt. We\\noffer two possible explanations: (1) the model may have developed inductive biases from pretraining\\nthat favor interpreting flat, plain-text layouts, such as those seen in summaries or abstracts, and (2)\\nfine-tuning datasets used in LLaMA or similar models may have predominantly featured unstructured\\nprompts, making the model more adept at handling them.\\nThis suggests that while prompt formatting is an important factor, training data design and supervision\\nsignal play a larger role in hallucination resistance.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='7 Discussion\\nOur results show that Finetune-RAG significantly improves a model’s ability to resist hallucinations\\nin a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning\\nwith dual-context examples leads to consistent improvements in factual accuracy, while preserving\\nhelpfulness, relevance, and depth.\\n7.1 Inductive Bias Emergence in Structure-Agnostic Learning\\nA significant and perhaps unexpected result in our study is that models trained on unstructured\\nprompts (Baseline format) performed better, especially in factual accuracy, compared to those trained\\nwith structured XML prompts. This challenges the common belief that clear structure always aids\\nreasoning. Instead, it suggests a deeper learning process, which involves the development of stronger\\nbuilt-in tendencies for selecting content when structure is absent. This raises a potential area that can\\nbe further researched upon.\\n7.2 Limitations\\nDespite promising results, several limitations remain:\\n• Synthetic dataset generation: The fictitious content is generated using GPT-4o (OpenAI,\\n2024), which may introduce distributional artifacts that differ from real-world retrieval\\nerrors. Additionally, the size of the dataset can be further increased for effective fine-tuning\\nin larger models.\\n• Binary supervision: We treat hallucination as a binary decision at the generation level.\\nHowever, hallucination is often more nuanced, involving partial truths, omissions, or subtle\\nphrasing, which our current framework may not sufficiently address.\\n• Controlled context pairing: During training, each example includes exactly one correct\\nand one incorrect document chunk. This creates a simplified binary contrast that may not\\ngeneralize to real-world scenarios where multiple retrieved documents vary in quality. A\\nstronger training approach can be constructed using our existing dataset to create more\\nvaried and robust scenarios that the model can train on.\\n• Compute requirements: While our method is simpler and less resource-intensive than\\nalternatives such as full retraining or reinforcement learning, it still requires access to a\\nhigh-memory GPU (e.g., H100) to fine-tune long-context models with large batch sizes.\\nThis may limit accessibility for some users or institutions.\\n7.3 Future Work\\nThere are several promising extensions to Finetune-RAG that could further improve its robustness\\nand applicability:\\n• Training with more in-context RAG: Real-world retrieval often returns more than two\\ndocuments, and the context window of LLMs are increasing rapidly. At the time of our\\nwork, we focused on relatively low context window of 8k, which would realistically be used\\nfor two to three RAG documents using up to 3k context window. With increasing context\\nwindow, future work can explore training with more RAG chunks to optimize LLMs RAG\\nperformance even at high level of stresses caused by more retrieved chunks. To support this,\\nwe future-proofed our dataset by including two additional relevant chunks per example to\\nsupport generating more complex multi-document training scenarios.\\n• Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the\\ngeneration component, combining it with learned retrieval mechanisms such as reranker-\\naware retrievers or contrastively trained retrievers could lead to further improvements in\\nfactual accuracy and context filtering.\\n• Multimodal extensions: Hallucination is not limited to text-based models. Ex-\\ntending Finetune-RAG to multimodal settings, such as image-caption retrieval or\\ncode+documentation generation, may help build more robust grounded systems in other\\ndomains.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucina-\\ntion settings, future work should assess Finetune-RAG’s impact on end-to-end performance\\nin downstream RAG applications such as open-domain question answering, legal document\\nsummarization, and domain-specific information retrieval.\\n8 Conclusion\\nIn this work, we presentFinetune-RAG, a simple yet effective method for reducing hallucination in\\nRetrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on\\nretrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while\\nignoring misleading information, with no architectural changes required.\\nWe constructed a diverse training set and evaluate usingBench-RAG, a technique that leverages\\nGPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving\\nhelpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts\\nrobustness, with less structured formats sometimes aiding discrimination.\\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully\\nimprove hallucination resistance in noisy retrieval environments. We release our code, dataset, and\\ncheckpoints to support further research in this direction, and highlight future extensions including\\nmulti-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='References\\nAgarwal, V ., Jin, Y ., Chandra, M., Choudhury, M. D., Kumar, S., and Sastry, N. (2024). Medhalu:\\nHallucinations in responses to healthcare queries by large language models.\\nBarnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., and Abdelrazek, M. (2024). Seven failure\\npoints when engineering a retrieval augmented generation system.\\nBray, T., Paoli, J., Sperberg-McQueen, C. M., Maler, E., and Yergeau, F. (1998). Extensible markup\\nlanguage (xml) 1.0.https://www.w3.org/TR/REC-xml/. W3C Recommendation.\\nCao, H., An, Z., Feng, J., Xu, K., Chen, L., and Zhao, D. (2023). A step closer to comprehensive\\nanswers: Constrained multi-stage question decomposition with large language models.\\nCui, M., Gao, P., Liu, W., Luan, J., and Wang, B. (2025). Multilingual machine translation with open\\nlarge language models at practical scale: An empirical study.\\nDahl, M., Magesh, V ., Suzgun, M., and Ho, D. E. (2024). Large legal fictions: Profiling legal\\nhallucinations in large language models.Journal of Legal Analysis, 16(1):64–93.\\nDong, J., Fatemi, B., Perozzi, B., Yang, L. F., and Tsitsulin, A. (2024). Don’t forget to connect!\\nimproving rag with graph-based reranking.\\nDuan, H., Yang, Y ., and Tam, K. Y . (2024). Do llms know about hallucination? an empirical\\ninvestigation of llm’s hidden states.\\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., and et al.\\n(2024). The llama 3 herd of models.\\nGu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y ., Ma, S., Liu, H., Wang, S., Zhang,\\nK., Wang, Y ., Gao, W., Ni, L., and Guo, J. (2025). A survey on llm-as-a-judge.\\nJones, E., Palangi, H., Simões, C., Chandrasekaran, V ., Mukherjee, S., Mitra, A., Awadallah, A., and\\nKamar, E. (2023). Teaching language models to hallucinate less with synthetic tasks.\\nKang, H. and Liu, X.-Y . (2023). Deficiency of large language models in finance: An empirical\\nexamination of hallucination.\\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z., Bhattacharjee, A., Jiang, Y ., Chen, C.,\\nWu, T., Shu, K., Cheng, L., and Liu, H. (2025). From generation to judgment: Opportunities and\\nchallenges of llm-as-a-judge.\\nLi, J., Chen, J., Ren, R., Cheng, X., Zhao, W. X., Nie, J.-Y ., and Wen, J.-R. (2024a). The dawn after\\nthe dark: An empirical study on factuality hallucination in large language models.\\nLi, Z., Chen, X., Yu, H., Lin, H., Lu, Y ., Tang, Q., Huang, F., Han, X., Sun, L., and Li, Y . (2024b).\\nStructrag: Boosting knowledge intensive reasoning of llms via inference-time hybrid information\\nstructurization.\\nLin, T., Zhu, Y ., Luo, Y ., and Tang, N. (2025). Srag: Structured retrieval-augmented generation for\\nmulti-entity question answering over wikipedia graph.\\nLiu, Y ., Shi, K., He, K., Ye, L., Fabbri, A., Liu, P., Radev, D., and Cohan, A. (2024). On learning to\\nsummarize with large language models as references. In Duh, K., Gomez, H., and Bethard, S.,\\neditors,Proceedings of the 2024 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\\n8647–8664, Mexico City, Mexico. Association for Computational Linguistics.\\nOpenAI (2024). Gpt-4o system card.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\\nWelinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow\\ninstructions with human feedback.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Rawte, V ., Chakraborty, S., Pathak, A., Sarkar, A., Tonmoy, S. M. T. I., Chadha, A., Sheth, A. P., and\\nDas, A. (2023). The troubling emergence of hallucination in large language models – an extensive\\ndefinition, quantification, and prescriptive remediations.\\nRozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Sauvestre, R.,\\nRemez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori,\\nA., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T.,\\nand Synnaeve, G. (2024). Code llama: Open foundation models for code.\\nSawarkar, K., Mangal, A., and Solanki, S. R. (2024). Blended rag: Improving rag (retriever-\\naugmented generation) accuracy with semantic search and hybrid query-based retrievers. In2024\\nIEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),\\nvolume 24, page 155–161. IEEE.\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., Schärli, N., and Zhou, D. (2023). Large\\nlanguage models can be easily distracted by irrelevant context.\\nWang, Y ., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and Hoi, S. C. H. (2023). Codet5+: Open\\ncode large language models for code understanding and generation.\\nYasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J. (2022). Qa-gnn: Reasoning with\\nlanguage models and knowledge graphs for question answering.\\nYoran, O., Wolfson, T., Ram, O., and Berant, J. (2024). Making retrieval-augmented language models\\nrobust to irrelevant context.\\nZhang, H., Diao, S., Lin, Y ., Fung, Y . R., Lian, Q., Wang, X., Chen, Y ., Ji, H., and Zhang, T. (2024).\\nR-tuning: Instructing large language models to say ‘i don’t know’.\\nZhang, Y ., Li, Y ., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y ., Chen, Y ., Wang,\\nL., Luu, A. T., Bi, W., Shi, F., and Shi, S. (2023). Siren’s song in the ai ocean: A survey on\\nhallucination in large language models.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\\nE. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and\\nchatbot arena.\\nZhou, J. and Chen, L. (2025). Openrag: Optimizing rag end-to-end via in-context retrieval learning.\\nZhou, Y ., Liu, Y ., Li, X., Jin, J., Qian, H., Liu, Z., Li, C., Dou, Z., Ho, T.-Y ., and Yu, P. S. (2024).\\nTrustworthiness in retrieval-augmented generation systems: A survey.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='A Dataset Example Format\\nAligning with Section 4.1, each example in the dataset is structured as follows:\\n{\\n\"content\": <factual chunk>,\\n\"filename\": <original document filename>,\\n\"fictitious_content\": <misleading chunk>,\\n\"fictitious_filename\": <filename of misleading chunk>,\\n\"question\": <user query>,\\n\"answer\": <GPT-4o generated answer based only on correct content>,\\n}\\nB Bench-RAG Prompt Structure\\nGiven both the correct and fictitious document chunks, the fine-tuned model checkpoints are used\\nto generate answers for questions on the test dataset. The outputs are stored in a structured jsonl\\nformat, with each entry containing the following fields:\\n{\\n\"filename\": <original document filename>,\\n\"content\": <factual chunk>,\\n\"question\": <user query>,\\n\"response\": <model’s generated answer>\\n}\\nWith these output, we curate a prompt for the four measurements derived from our evaluation.\\nB.1 System Message for Evaluation\\nAccuracy\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the accuracy of the\\nresponse.\\nYou will check whether the response contains extra details not found\\nin the piece of information provided. If extra details are found,\\naccuracy is false. Otherwise, accuracy is true. Take note that if the\\nresponse partially addresses the question, but did not provide extra\\ndetails not found in the piece of information provided, the response\\nwill still be considered accurate (hence accuracy = true).\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the accuracy\\nwith true or false by strictly following this JSON format:\\n{\\n\"accuracy_explanation\":\\n<provide an explanation on accuracy, whether extra details\\noutside the content were found.>,\\n\"accuracy\": <true/false>\\n}\"\\nHelpfulness\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='below, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the helpfulness of the\\nresponse.\\nYou will check whether the AI assistant is helpful in answering the\\nquestion based on the response.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nhelpfulness on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"helpfulness_explanation\": <provide an explanation on helpfulness>,\\n\"helpfulness\": <score>\\n}\"\\nRelevance\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the relevance of the\\nresponse.\\nYou will check the relevance of the response by evaluating whether the\\nresponse fully addresses the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nrelevance on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"relevance_explanation\": <provide an explanation on relevance>,\\n\"relevance\": <score>\\n}\"\\nDepth\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the depth of the\\nresponse.\\nYou will check the depth of the response by evaluating the level of\\ndetail of the response in answering the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\ndepth on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"depth_explanation\": <provide an explanation on depth>,\\n\"depth\": <score>\\n}\"\\nB.2 User Message for Evaluation\\nAll measurements utilizes the same user message structure for evaluation. Note that the content used\\nis the correct content, rather than the fictitious one:\\n[The Start of Provided Information Extracted from a File]\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Filename: {filename}\\nInformation: {content}\\n[The End of Provided Information]\\n[Question]\\n{question}\\n[The Start of Assistant’s Response]\\n{response}\\n[The End of Assistant’s Response]\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Rethinking the Role of Demonstrations:\\nWhat Makes In-Context Learning Work?\\nSewon Min1,2 Xinxi Lyu1 Ari Holtzman1 Mikel Artetxe2\\nMike Lewis2 Hannaneh Hajishirzi1,3 Luke Zettlemoyer1,2\\n1University of Washington 2Meta AI 3Allen Institute for AI\\n{sewon,alrope,ahai,hannaneh,lsz}@cs.washington.edu\\n{artetxe,mikelewis}@meta.com\\nAbstract\\nLarge language models (LMs) are able to in-\\ncontext learn—perform a new task via infer-\\nence alone by conditioning on a few input-\\nlabel pairs (demonstrations) and making pre-\\ndictions for new inputs. However, there has\\nbeen little understanding of how the model\\nlearns and which aspects of the demonstra-\\ntions contribute to end task performance. In\\nthis paper, we show that ground truth demon-\\nstrations are in fact not required—randomly\\nreplacing labels in the demonstrations barely\\nhurts performance on a range of classiﬁcation\\nand multi-choce tasks, consistently over 12 dif-\\nferent models including GPT-3. Instead, we\\nﬁnd that other aspects of the demonstrations\\nare the key drivers of end task performance, in-\\ncluding the fact that they provide a few exam-\\nples of (1) the label space, (2) the distribution\\nof the input text, and (3) the overall format of\\nthe sequence. Together, our analysis provides\\na new way of understanding how and why\\nin-context learning works, while opening up\\nnew questions about how much can be learned\\nfrom large language models through inference\\nalone.\\n1 Introduction\\nLarge language models (LMs) have shown impres-\\nsive performance on downstream tasks by simply\\nconditioning on a few input-label pairs (demonstra-\\ntions); this type of inference has been referred to as\\nin-context learning (Brown et al., 2020). Despite in-\\ncontext learning consistently outperforming zero-\\nshot inference on a wide range of tasks (Zhao et al.,\\n2021; Liu et al., 2021), there is little understanding\\nof how it works and which aspects of the demon-\\nstrations contribute to end task performance.\\nIn this paper, we show that ground truth demon-\\nstrations are in fact not required for effective in-\\ncontext learning (Section 4). Speciﬁcally, replac-\\ning the labels in demonstrations with random labels\\nbarely hurts performance in a range of classiﬁca-\\ntion and multi-choice tasks (Figure 1). The result\\nMetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nMetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 1: Results in classiﬁcation (top) and multi-\\nchoice tasks (bottom), using three LMs with varying\\nsize. Reported on six datasets on which GPT-3 is eval-\\nuated; the channel method is used. See Section 4 for\\nthe full results. In-context learning performance drops\\nonly marginally when labels in the demonstrations are\\nreplaced by random labels.\\nis consistent over 12 different models including the\\nGPT-3 family (Radford et al., 2019; Min et al.,\\n2021b; Wang and Komatsuzaki, 2021; Artetxe\\net al., 2021; Brown et al., 2020). This strongly\\nsuggests, counter-intuitively, that the model does\\nnot rely on the input-label mapping in the demon-\\nstrations to perform the task.\\nFurther analysis investigates which parts of\\ndemonstrations actually do contribute to the perfor-\\nmance. We identify possible aspects of demonstra-\\ntions (e.g., the label space and the distribution of\\nthe input text) and evaluate a series of variants of\\nthe demonstrations to quantify the impact of each\\n(Section 5). We ﬁnd that: (1) the label space and\\nthe distribution of the input text speciﬁed by the\\ndemonstrations are both key to in-context learn-\\ning (regardless of whether the labels are correct\\nfor individual inputs); (2) specifying the overall\\nformat is also crucial, e.g., when the label space\\nis unknown, using random English words as la-\\nbels is signiﬁcantly better than using no labels; and\\narXiv:2202.12837v2  [cs.CL]  20 Oct 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='(3) meta-training with an in-context learning objec-\\ntive (Min et al., 2021b) magniﬁes these effects—the\\nmodels almost exclusively exploit simpler aspects\\nof the demonstrations like the format rather than\\nthe input-label mapping.\\nIn summary, our analysis provides a new way\\nof understanding the role of the demonstrations in\\nin-context learning. We empirically show that the\\nmodel (1) counter-intuitively does not rely on the\\nground truth input-label mapping provided in the\\ndemonstrations as much as we thought (Section 4),\\nand (2) nonetheless still beneﬁts from knowing the\\nlabel space and the distribution of inputs speciﬁed\\nby the demonstrations (Section 5). We also include\\na discussion of broader implications, e.g., what we\\ncan say about the model learning at test time, and\\navenues for future work (Section 6).\\n2 Related Work\\nLarge language models have been key to strong per-\\nformance in a wide range of downstream tasks (De-\\nvlin et al., 2019; Radford et al., 2019; Liu et al.,\\n2019; Raffel et al., 2020; Lewis et al., 2020). While\\nﬁnetuning has been a popular approach to transfer\\nto new tasks (Devlin et al., 2019), it is often imprac-\\ntical to ﬁnetune a very large model (e.g. ≥10B pa-\\nrameters). Brown et al. (2020) propose in-context\\nlearning as an alternative way to learn a new task.\\nAs depicted in Figure 2, the LM learns a new task\\nvia inference alone by conditioning on a concatena-\\ntion of the training data as demonstrations, without\\nany gradient updates.\\nIn-context learning has been the focus of signif-\\nicant study since its introduction. Prior work pro-\\nposes better ways of formulating the problem (Zhao\\net al., 2021; Holtzman et al., 2021; Min et al.,\\n2021a), better ways of choosing labeled exam-\\nples for the demonstrations (Liu et al., 2021; Lu\\net al., 2021; Rubin et al., 2021), meta-training\\nwith an explicit in-context learning objective (Chen\\net al., 2021; Min et al., 2021b), and learning to\\nfollow instructions as a variant of in-context learn-\\ning (Mishra et al., 2021b; Efrat and Levy, 2020;\\nWei et al., 2022a; Sanh et al., 2022). At the\\nsame time, some work reports brittleness and over-\\nsensitivity for in-context learning (Lu et al., 2021;\\nZhao et al., 2021; Mishra et al., 2021a).\\nRelatively less work has been done to understand\\nwhy in-context learning works. Xie et al. (2022)\\nprovide theoretical analysis that in-context learn-\\ning can be formalized as Bayesian inference that\\nCirculation revenue has increased by 5% in Finland.         \\\\n    Positive \\nPanostaja did not disclose the purchase price.                  \\\\n    Neutral \\nPaying off the national debt will be extremely painful.      \\\\n    Negative \\nThe acquisition will have an immediate positive impact.  \\\\n    ________\\n =\\nDemonstrations\\nLM\\nPositive\\nTest input\\nPrediction\\nFigure 2: An overview of in-context learning. The\\ndemonstrations consist of k input-label pairs from the\\ntraining data (k = 3in the ﬁgure).\\nModel # Params Public Meta-trained\\nGPT-2 Large 774M \\x13 \\x17\\nMetaICL 774M \\x13 \\x13\\nGPT-J 6B \\x13 \\x17\\nfairseq 6.7B† 6.7B \\x13 \\x17\\nfairseq 13B† 13B \\x13 \\x17\\nGPT-3 175B ‡ \\x17 \\x17\\nTable 1: A list of LMs used in the experiments:\\nGPT-2 (Radford et al., 2019), MetaICL (Min et al.,\\n2021b), GPT-J (Wang and Komatsuzaki, 2021), fairseq\\nLMs (Artetxe et al., 2021) and GPT-3 (Brown et al.,\\n2020). ‘Public’ indicates whether the model weights\\nare public; ‘Meta-trained’ indicates whether the model\\nis meta-trained with an in-context learning objective.\\n†We use dense models in Artetxe et al. (2021) and re-\\nfer them as fairseq LMs for convenience. ‡We use the\\nDavinci API (the base version, not the instruct version)\\nand assume it to be 175B, following Gao et al. (2021)\\nand Artetxe et al. (2021).\\nuses the demonstrations to recover latent concepts.\\nRazeghi et al. (2022) show that in-context learn-\\ning performance is highly correlated with term fre-\\nquencies in the pretraining data. To the best of our\\nknowledge, this paper is the ﬁrst that provides an\\nempirical analysis that investigates why in-context\\nlearning achieves performance gains over zero-shot\\ninference. We ﬁnd that the ground truth input-label\\nmapping in the demonstrations has only a marginal\\neffect, and measure the impact of ﬁner-grained as-\\npects of the demonstrations.\\n3 Experimental Setup\\nWe describe the experimental setup used in our\\nanalysis (Section 4 and 5).\\nModels. We experiment with 12 models in to-\\ntal. We include 6 language models (Table 1), all\\nof which are decoder-only, dense LMs. We use\\neach LM with two inference methods, direct and\\nchannel, following Min et al. (2021a). The sizes\\nof LMs vary from 774M to 175B. We include the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nDirect\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 3: Results when using no-demonstrations, demonstrations with gold labels, and demonstrations with ran-\\ndom labels in classiﬁcation (top) and multi-choice tasks (bottom). The ﬁrst eight models are evaluated on 16\\nclassiﬁcation and 10 multi-choice datasets, and the last four models are evaluated on 3 classiﬁcation and 3 multi-\\nchoice datasets. See Figure 11 for numbers comparable across all models. Model performance with random\\nlabels is very close to performance with gold labels (more discussion in Section 4.1).\\nlargest dense LM (GPT-3) and the largest publicly\\nreleased dense LM (fairseq 13B) at the time of con-\\nducting experiments. We also include MetaICL,\\nwhich is initialized from GPT-2 Large and then\\nmeta-trained on a collection of supervised datasets\\nwith an in-context learning objective, and ensure\\nthat our evaluation datasets do not overlap with\\nthose used at meta-training time.\\nEvaluation Data. We evaluate on 26 datasets,\\nincluding sentiment analysis, paraphrase detection,\\nnatural language inference, hate speech detection,\\nquestion answering, and sentence completion (full\\nlist and references provided in Appendix A).1 All\\ndatasets are classiﬁcation and multi-choice tasks.\\nWe use these datasets because they (1) are true\\nlow-resource datasets with less than 10K train-\\ning examples, (2) include well-studied bench-\\nmarks from GLUE (Wang et al., 2018) and Super-\\nGLUE (Wang et al., 2019a), and (3) cover diverse\\ndomains including science, social media, ﬁnance,\\nand more.\\nOther Details. We use k = 16 examples as\\ndemonstrations by default for all experiments in\\nthe paper, unless otherwise speciﬁed. Examples\\nare sampled at uniform from the training data.\\nWe choose a set of k training examples using\\n5 different random seeds and run experiments 5\\ntimes. For fairseq 13B and GPT-3, due to lim-\\nited resources, we experiment with a subset of 6\\n1For convenience, we use ‘labels’ to refer to the output for\\nthe task, though our datasets include non-classiﬁcation tasks.\\ndatasets2 and 3 random seeds. We report Macro-\\nF13 for classiﬁcation tasks and Accuracy for multi-\\nchoice tasks. We compute per-dataset average over\\nseeds, and then report macro-average over datasets.\\nWe use the minimal templates in forming an in-\\nput sequence from an example. We refer to Ap-\\npendix B for more details. All experiments are\\nreproducible from github.com/Alrope123/\\nrethinking-demonstrations.\\n4 Ground Truth Matters Little\\n4.1 Gold labels vs. random labels\\nTo see the impact of correctly-paired inputs and\\nlabels in the demonstrations—which we call the\\nground truth input-label mapping—we compare the\\nfollowing three methods.4\\nNo demonstrations is a typical zero-shot method\\nthat does not use any labeled data. A prediction\\nis made via argmaxy∈CP(y|x), where x is the test\\ninput and Cis a small discrete set of possible labels.\\nDemonstrations w/ gold labels are used in a typi-\\ncal in-context learning method with k labeled ex-\\namples (x1, y1)...(xk, yk). A concatenation of k\\ninput-label pairs is used to make a prediction via\\nargmaxy∈CP(y|x1, y1...xk, yk, x).\\n2Three classiﬁcation and three multi-choice: MRPC, RTE,\\nTweet_eval-hate, OpenbookQA, CommonsenseQA, COPA.\\n3Known to be better for imbalanced classes.\\n4Without loss of generality, all methods in Section 4 and 5\\nare described based on the direct method, but can be trivially\\nconverted to the channel method by ﬂipping x and y.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (Classification) GPT-J (Classification) MetaICL (Multi-choice) GPT-J (Multi-choice)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Accuracy (%)\\n100% correct 75% correct 50% correct 25% correct 0% correct No Demos\\nFigure 4: Results with varying number of correct labels in the demonstrations. Channel and Direct used for\\nclassiﬁcation and multi-choice, respectively. Performance with no demonstrations (blue) is reported as a reference.\\nDemonstrations w/ random labels are formed\\nwith random labels, instead of gold labels from\\nthe labeled data. Each xi (1 ≤ i ≤\\nk) is paired with ˜yi that is randomly sam-\\npled at uniform from C. A concatenation of\\n(x1, ˜y1)...(xk, ˜yk) is then used to make a predic-\\ntion via argmaxy∈CP(y|x1, ˜y1...xk, ˜yk, x).\\nResults are reported in Figure 3. First, using the\\ndemonstrations with gold labels signiﬁcantly im-\\nproves the performance over no demonstrations,5\\nas it has been consistently found in much of prior\\nwork (Brown et al., 2020; Zhao et al., 2021; Liu\\net al., 2021). We then ﬁnd that replacing gold la-\\nbels with random labels only marginally hurts\\nperformance. The trend is consistent over nearly\\nall models: models see performance drop in the\\nrange of 0–5% absolute. There is less impact in\\nreplacing labels in multi-choice tasks (1.7% on av-\\nerage) than in classiﬁcation tasks (2.6% absolute).\\nThis result indicates that the ground truth input-\\nlabel pairs are not necessary to achieve perfor-\\nmance gains. This is counter-intuitive, given that\\ncorrectly paired training data is critical in typical\\nsupervised training—it informs the model of the ex-\\npected input-label correspondence required to per-\\nform the downstream task. Nonetheless, the mod-\\nels do achieve non-trivial performance on the down-\\nstream tasks. This strongly suggests that the mod-\\nels are capable of recovering the expected input-\\nlabel correspondence for the task; however, it isnot\\ndirectly from the pairings in the demonstrations.\\nIt is also worth noting that there is particularly\\nlittle performance drop in MetaICL: 0.1–0.9% ab-\\nsolute. This suggests that meta-training with an\\nexplicit in-context learning objective actually en-\\ncourages the model to essentially ignore the input-\\n5There are some exceptions, e.g., in the classiﬁcation tasks,\\nDirect GPT-2, Direct GPT-J and Direct fairseq 6.7B models\\nare not signiﬁcantly better than random guessing on many\\ndatasets; Channel fairseq 13B has signiﬁcantly better no-\\ndemonstrations performance compared to demonstrations with\\ngold labels. We thus discuss the results from these models less\\nsigniﬁcantly for the rest of analysis.\\n0 4 8 16 32\\nk\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDemos w/ gold\\nDemos w/ random\\n0 4 8 16 32\\nk\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\nDemos w/ gold\\nDemos w/ random\\nFigure 5: Ablations on varying numbers of examples\\nin the demonstrations (k). Models that are the best un-\\nder 13B in each task category (Channel MetaICL and\\nDirect GPT-J, respectively) are used.\\nlabel mapping and exploit other components of the\\ndemonstrations (more discussion in Section 5.4).\\nIn Appendix C.2, we provide additional results\\nshowing that (1) selecting random labels from a\\ntrue distribution of labels (instead of a uniform\\ndistribution) reduces the gap even further, and (2)\\nthe trends may depend on the dataset, although the\\noverall trend is consistent over most datasets.\\n4.2 Ablations\\nFor additional ablations, we experiment with 5 clas-\\nsiﬁcation and 4 multi-choice datasets.6\\nDoes the number of correct labels matter? To\\nfurther examine the impact of correctness of la-\\nbels in the demonstrations, we conduct an ablation\\nstudy by varying the number of correct labels in the\\ndemonstrations. We evaluate “Demonstrations w/\\na% correct labels” (0 ≤a ≤100) which consist\\nof k ×a/100 correct pairs and k ×(1 −a/100)\\nincorrect pairs (see Algorithm 1 in Appendix B).\\nHere, a = 100 is the same as typical in-context\\nlearning, i.e., demonstrations w/ gold labels.\\nResults are reported in Figure 4. Model perfor-\\nmance is fairly insensitive to the number of correct\\nlabels in the demonstrations. In fact, always us-\\ning incorrect labels signiﬁcantly outperforms no-\\n6Classiﬁcation includes: MRPC, RTE, Tweet_eval-hate,\\nSICK, poem-sentiment; Multi-choice includes OpenbookQA,\\nCommonsenseQA, COPA and ARC.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (Classification) GPT-J (Classification) MetaICL (Multi-choice) GPT-J (Multi-choice)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Accuracy (%)\\nNo demos Gold labels Random labels No demos + T Gold labels + T Random labels + T\\nFigure 6: Results with minimal templates and manual templates. ‘+T’ indicates that manual templates are used.\\nChannel and Direct used for classiﬁcation and multi-choice, respectively.\\ndemonstrations, e.g., preserving 92%, 100% and\\n97% of improvements from using the demonstra-\\ntions with MetaICL in classiﬁcation, MetaICL in\\nmulti-choice, and GPT-J in multi-choice, respec-\\ntively. In contrast, GPT-J in classiﬁcation sees\\nrelatively signiﬁcant performance drop with more\\nincorrect labels, e.g., nearly 10% drop in perfor-\\nmance when always using incorrect labels. Still,\\nalways using incorrect labels is signiﬁcantly better\\nthan no demonstrations.\\nIs the result consistent with varying k? We\\nstudy the impact of the number of input-label pairs\\n(k) in the demonstrations. Results are reported in\\nFigure 5. First, using the demonstrations signiﬁ-\\ncantly outperforms the no demonstrations method\\neven with small k (k = 4), and performance drop\\nfrom using gold labels to using random labels is\\nconsistently small across varying k, in the range of\\n0.8–1.6%.7 Interestingly, model performance does\\nnot increase much as k increases when k ≥8, both\\nwith gold labels and with random labels. This is\\nin contrast with typical supervised training where\\nmodel performance rapidly increases ask increases,\\nespecially when k is small. We hypothesize that\\nlarger labeled data is beneﬁcial mainly for super-\\nvising the input-label correspondence, and other\\ncomponents of the data like the example inputs,\\nexample labels and the data format are easier to\\nrecover from the small data, which is potentially a\\nreason for minimal performance gains from larger\\nk (more discussion in Section 5).\\nIs the result consistent with better templates?\\nWhile we use minimal templates by default, we\\nalso explore manual templates, i.e., templates that\\nare manually written in a dataset-speciﬁc manner,\\ntaken from prior work (details in Appendix B). Fig-\\nure 6 shows that the trend—replacing gold labels\\nwith random labels barely hurting performance—\\nholds with manual templates. It is worth noting\\n7With an exception of 4.4% in classiﬁcation with k = 4,\\nlikely due to a high variance with a very small value of k.\\nCirculation revenue has increased by 5% in Finland.         \\\\n         Positive\\nFormat \\n(The use \\nof pairs)\\n =\\nDistribution of inputs Label spaceDemonstrations\\nTest example Input-label mapping\\nPanostaja did not disclose the purchase price.                  \\\\n         Neutral\\nPaying off the national debt will be extremely painful.      \\\\n         Negative\\nThe acquisition will have an immediate positive impact.  \\\\n         ?\\nFigure 7: Four different aspects in the demonstrations:\\nthe input-label mapping, the distribution of the input\\ntext, the label space, and the use of input-label pairing\\nas the format of the demonstrations.\\nthat using manual templates does not always out-\\nperform using minimal templates.\\n5 Why does In-Context Learning work?\\nSection 4 shows that the ground truth input-label\\nmapping in the demonstrations has little impact to\\nperformance gains from in-context learning. This\\nsection further examines what other aspects of the\\ndemonstrations lead to good performance of in-\\ncontext learning.\\nWe identify four aspects of the demonstrations\\n(x1, y1)...(xk, yk) that potentially provide learning\\nsignal (depicted in Figure 7).\\n1. The input-label mapping, i.e., whether each\\ninput xi is paired with a correct label yi.\\n2. The distribution of the input text , i.e., the\\nunderlying distribution that x1...xk are from.\\n3. The label space , i.e., the space covered by\\ny1...yk.\\n4. The format—speciﬁcally, the use of input-\\nlabel pairing as the format.\\nAs Section 4 does for the input-label mapping,\\nwe design a series of variants of the demonstrations\\nthat quantify the impact of each aspect in isolation\\n(Section 5.1–5.3). We then additionally discuss the\\ntrend of the models meta-trained with an in-context\\nlearning objective (Section 5.4). For all experi-\\nments, models are evaluated on ﬁve classiﬁcation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nFigure 8: Impact of the distribution of the inputs. Evaluated in classiﬁcation (top) and multi-choice (bottom). The\\nimpact of the distribution of the input text can be measured by comparing■ and ■. The gap is substantial, with an\\nexception in Direct MetaICL (discussion in Section 5.1).\\nand four multi-choice datasets as in Section 4.2.\\nSee Appendix B and Table 4 for implementation\\ndetails and example demonstrations, respectively.\\n5.1 Impact of the distribution of the input\\ntext\\nWe experiment with OOD demonstrations which\\ninclude out-of-distribution (OOD) text instead of\\nthe inputs from unlabeled training data. Specif-\\nically, a set of k sentences {xi,rand}k\\ni=1 are ran-\\ndomly sampled from an external corpus, and re-\\nplace x1...xk in the demonstrations. This variant\\nassesses the impact of the distribution of the input\\ntext, while keeping the label space and the format\\nof the demonstrations.\\nResults. Figure 8 shows that using out-of-\\ndistribution inputs instead of the inputs from the\\ntraining data signiﬁcantly drops the performance\\nwhen Channel MetaICL, Direct GPT-J or Channel\\nGPT-J are used, both in classiﬁcation and multi-\\nchoice, by 3–16% in absolute. In the case of Di-\\nrect GPT-J in multi-choice, it is even signiﬁcantly\\nworse than no demonstrations. Direct MetaICL\\nis an exception, which we think is the effect of\\nmeta-training (discussion in Section 5.4).\\nThis suggests that in-distribution inputs in the\\ndemonstrations substantially contribute to perfor-\\nmance gains. This is likely because conditioning on\\nthe in-distribution text makes the task closer to lan-\\nguage modeling, since the LM always conditioned\\non the in-distribution text during training.\\n5.2 Impact of the label space\\nWe also experiment with demonstrations w/ ran-\\ndom English words that use random English\\nwords as labels for all k pairs. Speciﬁcally, we\\nsample a random subset of English words Crand\\nwhere |Crand|= |C|, and randomly pair ˜yi ∈Crand\\nwith xi. This variant assesses the impact of the\\nlabel space, while keeping the distribution of the\\ninput text and the format of the demonstrations.\\nResults. Based on Figure 9, direct models and\\nchannel models exhibit different patterns. With di-\\nrect models, the performance gap between using\\nrandom labels within the label space and using ran-\\ndom English words is signiﬁcant, ranging between\\n5–16% absolute. This indicates that conditioning\\non the label space signiﬁcantly contributes to per-\\nformance gains. This is true even for multi-choice\\ntasks where there is no ﬁxed set of labels—we\\nhypothesize that multi-choice tasks still do have\\na particular distribution of the choices (e.g., ob-\\njects like “Bolts” or “Screws” in the OpenBookQA\\ndataset) that the model uses.\\nOn the other hand, removing the output space\\ndoes not lead to signiﬁcant drop in the channel\\nmodels: there is 0–2% drop in absolute, or some-\\ntimes even an increase. We hypothesize that this is\\nbecause the channel models only condition on the\\nlabels, and thus are not beneﬁting from knowing\\nthe label space. This is in contrast to direct models\\nwhich must generate the correct labels.\\n5.3 Impact of input-label pairing\\nSection 5.1 and 5.2 focus on variants which keep\\nthe format of the demonstrations as much as possi-\\nble. This section explores variants that change the\\nformat. While there are many aspects of the format,\\nwe make minimal modiﬁcations to remove the pair-\\nings of inputs to labels. Speciﬁcally, we evaluate\\ndemonstrations with no labels where the LM is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ Random English words\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nFigure 9: Impact of the label space. Evaluated in classiﬁcation (top) and multi-choice (bottom). The impact of\\nthe label space can be measured by comparing ■ and ■. The gap is signiﬁcant in the direct models but not in the\\nchannel models (discussion in Section 5.2).\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ Random labels only\\n■ Random English words\\n■ No labels\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x13\\n\\x17\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\n\\x13\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nFigure 10: Impact of the format, i.e., the use of the input-label pairs. Evaluated in classiﬁcation (top) and multi-\\nchoice (bottom). Variants of demonstrations without keeping the format ( ■ and ■) are overall not better than no\\ndemonstrations (■). Keeping the format is especially signiﬁcant when it is possible to achieve substantial gains\\nwith the label space but without the inputs (■ vs. ■ in Direct MetaICL), or with the input distribution but without\\nthe labels (■ vs. ■ in Channel MetaICL and Channel GPT-J). More discussion in Section 5.3.\\nconditioned on the concatenation of x1...xk, and\\ndemonstrations with labels onlywhere the LM is\\nconditioned on the concatenation of y1...yk. These\\nablations provide the no-format counterparts of the\\n‘demonstrations with random English words’ and\\n‘demonstrations with OOD inputs’, respectively.\\nResults. Based on Figure 10, removing the for-\\nmat is close to or worse than no demonstrations,\\nindicating the importance of the format. This is\\nlikely because conditioning on a sequence of input-\\nlabel pairs triggers the model to mimic the overall\\nformat and complete the new example as expected\\nwhen the test input is given.\\nMore interestingly, keeping the format plays a\\nsigniﬁcant role in retaining a large portion of per-\\nformance gains by only using the inputs or only\\nusing the labels. For instance, with Direct MetaICL,\\nit is possible to retain 95% and 82% of improve-\\nments from in-context learning (demonstrations\\nwith gold labels) by simply sampling random sen-\\ntences from a corpus and randomly pairing them\\nwith the label set (■ in Figure 10) in classiﬁcation\\nand multi-choice, respectively. Similarly, with the\\nchannel models, it is possible to retain 82%, 87%,\\n86% and 75% of improvements from in-context\\nlearning by simply pairing each input from the un-\\nlabeled training data with a random English word\\n(■ in Figure 10) in MetaICL classiﬁcation, GPT-\\nJ classiﬁcation, MetaICL multi-choice and GPT-J\\nmulti-choice, respectively. For all of these cases,\\nremoving inputs instead of using OOD inputs, or\\nremoving labels instead of using random English\\nwords is signiﬁcantly worse, indicating that keep-\\ning the format of the input-label pairs is key.\\n5.4 Impact of meta-training\\nDifferent from other models, MetaICL is trained\\nwith an in-context learning objective, in line with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='recent work that uses multi-task training on a\\nlarge collection of supervised datasets (called meta-\\ntraining) for generalization to new tasks (Agha-\\njanyan et al., 2021; Khashabi et al., 2020; Wei\\net al., 2022a; Sanh et al., 2022). We aim to better\\nunderstand the role of this meta-training in relation\\nwith our ﬁndings by closely examining the result of\\nMetaICL. In particular, we observe that the patterns\\nwe see so far are signiﬁcantly more evident with\\nMetaICL than with other models. For instance, the\\nground truth input-label mapping matters even less,\\nand keeping the format of the demonstrations mat-\\nters even more. There is nearly zero inﬂuence of\\nthe input-label mapping and the input distribution\\nin Direct MetaICL, and the input-label mapping\\nand the output space in Channel MetaICL.\\nBased on this observation, we hypothesize that\\nmeta-training encourages the model to exclu-\\nsively exploit simpler aspects of the demonstra-\\ntions and to ignore others. This is based on our\\nintuition that (1) the input-label mapping is likely\\nharder to exploit, (2) the format is likely easier to\\nexploit, and (3) the space of the text that the model\\nis trained to generate is likely easier to exploit than\\nthe space of the text that the model conditions on.8\\n6 Discussion & Conclusion\\nIn this paper, we study the role of the demonstra-\\ntions with respect to the success of in-context learn-\\ning. We ﬁnd that the ground truth input-label map-\\nping in the demonstrations matters signiﬁcantly\\nless than one might think—replacing gold labels\\nwith random labels in the demonstrations only\\nmarginally lowers the performance. We then iden-\\ntify a series of aspects in the demonstrations and\\nexamine which aspect actually contributes to per-\\nformance gains. Results reveal that (1) gains are\\nmainly coming from independent speciﬁcation of\\nthe input space and the label space, (2) the models\\ncan still retain up to 95% of performance gains by\\nusing either the inputs only or the label set only if\\nthe right format is used, and (3) meta-training with\\nan in-context learning objective magniﬁes these\\ntrends. Together, our ﬁndings lead to a set of\\nbroader indications about in-context learning, as\\nwell as avenues for future work.\\nDoes the model learn at test time? If we take\\na strict deﬁnition of learning: capturing the input-\\n8That is, the direct model exploits the label space better\\nthan the input distribution, and the channel model exploits the\\ninput distribution better than the label space.\\nlabel correspondence given in the training data,\\nthen our ﬁndings suggest that LMs do not learn\\nnew tasks at test time. Our analysis shows that the\\nmodel may ignore the task deﬁned by the demon-\\nstrations and instead use prior from pretraining.\\nHowever, learning a new task can be interpreted\\nmore broadly: it may include adapting to speciﬁc\\ninput and label distributions and the format sug-\\ngested by the demonstrations, and ultimately get-\\nting to make a prediction more accurately. With\\nthis deﬁnition of learning, the model does learn\\nthe task from the demonstrations. Our experiments\\nindicate that the model does make use of aspects of\\nthe demonstrations and achieve performance gains.\\nCapacity of LMs. The model performs a down-\\nstream task without relying on the input-label corre-\\nspondence from the demonstrations. This suggests\\nthat the model has learned the (implicit notion of)\\ninput-label correspondence from the language mod-\\neling objective alone, e.g., associating a positive\\nreview with the word ‘positive’. This is in line\\nwith Reynolds and McDonell (2021) who claim\\nthat the demonstrations are for task location and\\nthe intrinsic ability to perform the task is obtained\\nat pretraining time.9\\nOn one hand, this suggests that the language\\nmodeling objective has led to great zero-shot ca-\\npacity, even if it is not always evident from the\\nnaive zero-shot accuracy. On the other hand, this\\nsuggests that in-context learning may not work on\\na task whose input-label correspondence is not al-\\nready captured in the LM. This leads to the research\\nquestion of how to make progress in NLP problems\\nthat in-context learning does not solve: whether\\nwe need a better way of extracting the input-label\\nmappings that are already stored in the LM, a bet-\\nter variant of the LM objective that learns a wider\\nrange of task semantics, or explicit supervision\\nthrough ﬁne-tuning on the labeled data.\\nConnection to instruction-following models.\\nPrior work has found it promising to train the model\\nthat reads the natural language description of the\\ntask (called instructions) and performs a new task\\nat inference (Mishra et al., 2021b; Efrat and Levy,\\n2020; Wei et al., 2022a; Sanh et al., 2022). We\\nthink the demonstrations and instructions largely\\nhave the same role to LMs, and hypothesize that our\\n9However, while Reynolds and McDonell (2021) claims\\nthat the demonstrations are thus unnecessary, we think using\\nthe demonstrations is actually the most unambiguous and the\\neasiest way to prompt the model to perform a task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='ﬁndings hold for instruction-following models: the\\ninstructions prompt the model to recover the capac-\\nity it already has, but do not supervise the model to\\nlearn novel task semantics. This has been partially\\nveriﬁed by Webson and Pavlick (2022) who showed\\nthat the model performance does not degrade much\\nwith irrelevant or misleading instructions. We leave\\nmore analysis on instruction-following models for\\nfuture work.\\nSigniﬁcantly improved zero-shot performance.\\nOne of our key ﬁndings is that it is possible to\\nachieve nearly k-shot performance without using\\nany labeled data, by simply pairing each unlabeled\\ninput with a random label and using it as the demon-\\nstrations. This means our zero-shot baseline level\\nis signiﬁcantly higher than previously thought. 10\\nFuture work can further improve the zero-shot per-\\nformance with relaxed assumptions in access to the\\nunlabeled training data.\\nLimitation\\nEffect of types of tasks and datasets. This pa-\\nper focuses on the tasks from established NLP\\nbenchmarks that have real natural language inputs.\\nSynthetic tasks with more limited inputs may actu-\\nally use the ground truth labels more, as observed\\nby Rong (2021).\\nWe report macro-level analysis by examining the\\naverage performance over multiple NLP datasets,\\nbut different datasets may behave differently. Ap-\\npendix C.2 discusses this aspect, including ﬁnd-\\nings that there are larger gaps between using the\\nground truth labels and using the random labels\\nin some dataset-model pairs (e.g., in the most\\nextreme case, nearly 14% absolute on the ﬁnan-\\ncial_phrasebank dataset with GPT-J). Since the ﬁrst\\nversion of our paper, Kim et al. (2022) showed\\nthat using negated labels substantially lowers the\\nperformance in classiﬁcation. 11 We believe it is\\nimportant to understand to what extend the model\\nneeds the ground truth labels to successfully per-\\nform in-context learning.\\nExtensions to generation. Our experiments are\\nlimited to classiﬁcation and multi-choice tasks. We\\nhypothesize that ground truth output may not be\\n10We take the perspective that using the unlabeled training\\ndata is permitted (Kodirov et al., 2015; Wang et al., 2019b;\\nSchick and Schütze, 2021).\\n11Note that Kim et al. (2022) estimate the random label per-\\nformance by interpolating with the performance using negated\\nlabels, while our paper samples the random labels at uniform.\\nnecessary for in-context learning in the open-set\\ntasks such as generation, but leave this to future\\nwork. Extending of our experiments to such tasks\\nis not trivial, because it requires a variation of the\\noutput which has incorrect input-output correspon-\\ndence while keeping the correct output distribution\\n(which is important based on our analysis in Sec-\\ntion 5).\\nSince the ﬁrst version of our paper, Madaan and\\nYazdanbakhsh (2022) conducted a similar analy-\\nsis with the chain of thought prompting (Wei et al.,\\n2022b) which generates a rationale to perform com-\\nplex tasks such as math problems. Madaan and\\nYazdanbakhsh (2022) show that, while simply us-\\ning a random rationale in the demonstrations (e.g.,\\npairing with a rationale from a different example)\\nsigniﬁcantly degrades the performance, other types\\nof counterfactual rationales (e.g., wrong equations)\\ndo not degrade the performance as much as we\\nthought. We refer to Madaan and Yazdanbakhsh\\n(2022) for more discussions on what aspects of the\\nrationale matter or do not matter.\\nAcknowledgements\\nWe thank Gabriel Ilharco, Julian Michael, Oﬁr\\nPress, UW NLP members and anonymous review-\\ners for their comments in the paper. This research\\nwas supported by NSF IIS-2044660, ONR N00014-\\n18-1-2826, a Sloan fellowship and gifts from AI2.\\nReferences\\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivas-\\ntava, Xilun Chen, Luke Zettlemoyer, and Sonal\\nGupta. 2021. Muppet: Massive multi-task rep-\\nresentations with pre-ﬁnetuning. arXiv preprint\\narXiv:2101.11038.\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\\net al. 2021. Efﬁcient large scale language mod-\\neling with mixtures of experts. arXiv preprint\\narXiv:2112.10684.\\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\\nDanilo Giampiccolo, Bernardo Magnini, and Idan\\nSzpektor. 2006. The second pascal recognising tex-\\ntual entailment challenge. In Proceedings of the sec-\\nond PASCAL challenges workshop on recognising\\ntextual entailment.\\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\\npinosa Anke, and Leonardo Neves. 2020. TweetE-\\nval: Uniﬁed benchmark and comparative evaluation\\nfor tweet classiﬁcation. In Findings of EMNLP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\\nGiampiccolo. 2009. The ﬁfth pascal recognizing tex-\\ntual entailment challenge. In TAC.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\\nV oss, Gretchen Krueger, Tom Henighan, Rewon\\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\\nClemens Winter, Chris Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In\\nNeurIPS.\\nMichael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-\\nnandez, and Doug Downey. 2019. CODAH: An\\nadversarially-authored question answering dataset\\nfor common sense. In Proceedings of the 3rd Work-\\nshop on Evaluating Vector Space Representations\\nfor NLP.\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,\\nand He He. 2021. Meta-learning via language model\\nin-context tuning. arXiv preprint arXiv:2110.07814.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\\nAshish Sabharwal, Carissa Schoenick, and Oyvind\\nTafjord. 2018. Think you have solved question an-\\nswering? try arc, the ai2 reasoning challenge. ArXiv.\\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\\n2005. The pascal recognising textual entailment\\nchallenge. In Machine Learning Challenges Work-\\nshop.\\nOna de Gibert, Naiara Perez, Aitor García-Pablos, and\\nMontse Cuadros. 2018. Hate Speech Dataset from a\\nWhite Supremacy Forum. In Proceedings of the 2nd\\nWorkshop on Abusive Language Online (ALW2).\\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\\ndith Tonhauser. 2019. The commitmentbank: Inves-\\ntigating projection in naturally occurring discourse.\\nProceedings of Sinn und Bedeutung.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In NAACL.\\nT. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leippold.\\n2020. Climate-fever: A dataset for veriﬁcation of\\nreal-world climate claims. ArXiv.\\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\nAvia Efrat and Omer Levy. 2020. The turking test: Can\\nlanguage models understand instructions? arXiv\\npreprint arXiv:2010.11982.\\nL Gao, S Biderman, S Black, L Golding, T Hoppe,\\nC Foster, J Phang, H He, A Thite, N Nabeshima,\\net al. 2021. The pile: an 800gb dataset of diverse\\ntext for language modeling 2020. arXiv preprint\\narXiv:2101.00027.\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\\nand Bill Dolan. 2007. The third pascal recognizing\\ntextual entailment challenge. In Proceedings of the\\nACL-PASCAL workshop on textual entailment and\\nparaphrasing.\\nAndrew Gordon, Zornitsa Kozareva, and Melissa\\nRoemmele. 2012. SemEval-2012 task 7: Choice\\nof plausible alternatives: An evaluation of common-\\nsense causal reasoning. In The First Joint Confer-\\nence on Lexical and Computational Semantics (Se-\\nmEval).\\nAri Holtzman, Peter West, Vered Schwartz, Yejin Choi,\\nand Luke Zettlemoyer. 2021. Surface form compe-\\ntition: Why the highest probability answer isn’t al-\\nways right. In EMNLP.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\\nnaneh Hajishirzi. 2020. UniﬁedQA: Crossing for-\\nmat boundaries with a single qa system. In Findings\\nof EMNLP.\\nTushar Khot, Peter Clark, Michal Guerquin, Peter\\nJansen, and Ashish Sabharwal. 2020. Qasc: A\\ndataset for question answering via sentence compo-\\nsition. In AAAI.\\nJunyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho,\\nHwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\\nKang Min Yoo, and Taeuk Kim. 2022. Ground-truth\\nlabels matter: A deeper look into input-label demon-\\nstrations. arXiv preprint arXiv:2205.12685.\\nElyor Kodirov, Tao Xiang, Zhenyong Fu, and Shao-\\ngang Gong. 2015. Unsupervised domain adaptation\\nfor zero-shot learning. In Proceedings of the IEEE\\ninternational conference on computer vision.\\nHector J. Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2012. The winograd schema challenge. In\\nProceedings of the Thirteenth International Confer-\\nence on Principles of Knowledge Representation\\nand Reasoning.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In ACL.\\nQuentin Lhoest, Albert Villanova del Moral, Yacine\\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\\nPatry, Angelina McMillan-Major, Philipp Schmid,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Sylvain Gugger, Clément Delangue, Théo Matus-\\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\\ntac, Thibault Goehringer, Victor Mustar, François\\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\\nDatasets: A community library for natural language\\nprocessing. In EMNLP: System Demonstrations.\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\\nLawrence Carin, and Weizhu Chen. 2021. What\\nmakes good in-context examples for gpt- 3? arXiv\\npreprint arXiv:2101.06804.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\nRobert L Logan IV , Ivana Balaževic, Eric Wallace,\\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\\n2021. Cutting down on prompts and parameters:\\nSimple few-shot learning with language models.\\narXiv preprint arXiv:2106.13353.\\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\\nRiedel, and Pontus Stenetorp. 2021. Fantastically\\nordered prompts and where to ﬁnd them: Overcom-\\ning few-shot prompt order sensitivity.arXiv preprint\\narXiv:2104.08786.\\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\\nand patterns: For effective chain of thought, it takes\\ntwo to tango. arXiv preprint arXiv:2209.07686.\\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-\\nlenius, and Pyry Takala. 2014. Good debt or bad\\ndebt: Detecting semantic orientations in economic\\ntexts. J. Assoc. Inf. Sci. Technol.\\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\\nelli. 2014. A SICK cure for the evaluation of com-\\npositional distributional semantic models. In LREC.\\nClara H. McCreery, Namit Katariya, Anitha Kannan,\\nManish Chablani, and Xavier Amatriain. 2020. Ef-\\nfective transfer learning for identifying similar ques-\\ntions: Matching user questions to covid-19 faqs.\\nIn Proceedings of the 26th ACM SIGKDD Interna-\\ntional Conference on Knowledge Discovery & Data\\nMining.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\\nSabharwal. 2018. Can a suit of armor conduct elec-\\ntricity? a new dataset for open book question answer-\\ning. In EMNLP.\\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2021a. Noisy channel language\\nmodel prompting for few-shot text classiﬁcation.\\narXiv preprint arXiv:2108.04106.\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2021b. MetaICL: Learning to\\nlearn in context. arXiv preprint.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\\nChoi, and Hannaneh Hajishirzi. 2021a. Refram-\\ning instructional prompts to gptk’s language. arXiv\\npreprint arXiv:2109.07830.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\\nHannaneh Hajishirzi. 2021b. Cross-task generaliza-\\ntion via natural language crowdsourcing instructions.\\narXiv preprint arXiv:2104.08773.\\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\\nand Grigorios Tsoumakas. 2020. Ethos: an online\\nhate speech detection dataset. ArXiv.\\nSebastian Nagel. 2016. CC-News. http:\\n//web.archive.org/save/http:\\n//commoncrawl.org/2016/10/\\nnews-dataset-available.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nblog.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. Journal of Machine Learning Research.\\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\\nand Sameer Singh. 2022. Impact of pretraining term\\nfrequencies on few-shot reasoning. arXiv preprint\\narXiv:2202.07206.\\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\\ngramming for large language models: Beyond the\\nfew-shot paradigm. In Extended Abstracts of the\\n2021 CHI Conference on Human Factors in Com-\\nputing Systems.\\nFrieda Rong. 2021. Extrapolating to unnatu-\\nral language processing with gpt-3’s in-context\\nlearning: The good, the bad, and the myste-\\nrious. https://ai.stanford.edu/blog/\\nin-context-learning.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun\\nRaja, Manan Dey, M Saiful Bari, Canwen Xu, Ur-\\nmish Thakker, Shanya Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\\nJiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\\nGao, Tali Bers, Thomas Wolf, and Alexander M.\\nRush. 2022. Multitask prompted training enables\\nzero-shot task generalization. In ICLR.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Timo Schick and Hinrich Schütze. 2021. It’s not just\\nsize that matters: Small language models are also\\nfew-shot learners. In NAACL-HLT.\\nEmily Sheng and David Uthus. 2020. Investigating so-\\ncietal biases in a poetry composition system. In Pro-\\nceedings of the Second Workshop on Gender Bias in\\nNatural Language Processing.\\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\\nand Claire Cardie. 2019. DREAM: A challenge data\\nset and models for dialogue-based reading compre-\\nhension. TACL.\\nOyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau\\nYih, and Ashish Sabharwal. 2019a. Quarel: A\\ndataset and models for answering questions about\\nqualitative relationships. In AAAI.\\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter\\nClark. 2019b. QuaRTz: An open-domain dataset of\\nqualitative relationship questions. In EMNLP.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A ques-\\ntion answering challenge targeting commonsense\\nknowledge. In NAACL-HLT.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman. 2019a. Superglue: A\\nstickier benchmark for general-purpose language un-\\nderstanding systems. In NeurIPS.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R Bowman. 2018.\\nGlue: A multi-task benchmark and analysis plat-\\nform for natural language understanding. In Black-\\nboxNLP Workshop: Analyzing and Interpreting Neu-\\nral Networks for NLP.\\nBen Wang and Aran Komatsuzaki. 2021. GPT-\\nJ-6B: A 6 Billion Parameter Autoregressive\\nLanguage Model. https://github.com/\\nkingoflolz/mesh-transformer-jax.\\nWei Wang, Vincent W Zheng, Han Yu, and Chunyan\\nMiao. 2019b. A survey of zero-shot learning: Set-\\ntings, methods, and applications. ACM Transactions\\non Intelligent Systems and Technology (TIST).\\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\\nbased models really understand the meaning of their\\nprompts? In NAACL-HLT.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\\ndrew M Dai, and Quoc V Le. 2022a. Finetuned lan-\\nguage models are zero-shot learners. In ICLR.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. arXiv preprint arXiv:2201.11903.\\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\\nand Tengyu Ma. 2022. An explanation of in-context\\nlearning as implicit bayesian inference. In ICLR.\\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\\nCrossﬁt: A few-shot learning challenge for cross-\\ntask generalization in nlp. In EMNLP.\\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\\nSameer Singh. 2021. Calibrate before use: Improv-\\ning few-shot performance of language models. In\\nICML.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='A Full Datasets\\nWe include 26 datasets as follows: ﬁ-\\nnancial_phrasebank (Malo et al., 2014),\\npoem_sentiment (Sheng and Uthus, 2020),\\nmedical_questions_pairs (McCreery et al., 2020),\\nglue-mrpc (Dolan and Brockett, 2005), glue-\\nwnli (Levesque et al., 2012), climate_fever (Diggel-\\nmann et al., 2020), glue-rte (Dagan et al., 2005;\\nBar-Haim et al., 2006; Giampiccolo et al.,\\n2007; Bentivogli et al., 2009), superglue-\\ncb (de Marneffe et al., 2019), sick (Marelli et al.,\\n2014) , hate_speech18 (de Gibert et al., 2018),\\nethos-national_origin (Mollas et al., 2020), ethos-\\nrace (Mollas et al., 2020), ethos-religion (Mollas\\net al., 2020), tweet_eval-hate (Barbieri et al., 2020),\\ntweet_eval-stance_atheism (Barbieri et al., 2020),\\ntweet_eval-stance_feminist (Barbieri et al., 2020),\\nquarel (Tafjord et al., 2019a), openbookqa (Mi-\\nhaylov et al., 2018), qasc (Khot et al., 2020), com-\\nmonsense_qa (Talmor et al., 2019), ai2_arc (Clark\\net al., 2018), codah (Chen et al., 2019), superglue-\\ncopa (Gordon et al., 2012), dream (Sun et al.,\\n2019), quartz-with_knowledge (Tafjord et al.,\\n2019b), quartz-no_knowledge (Tafjord et al.,\\n2019b). The choice of datasets is made following\\nlow-resource datasets in Min et al. (2021b), with\\nthe exact same set of k-shot train data using 5\\nrandom seeds. We use the HuggingFace version\\nof the data (Lhoest et al., 2021) and use the\\ndevelopment data for evaluation, following Ye\\net al. (2021). See Table 2 for statistics.\\nB Experimental Details\\nExample template We follow Ye et al. (2021);\\nMin et al. (2021b); Logan IV et al. (2021) in us-\\ning the minimal format to transform the input to a\\nsequence (e.g. a concatenation of multiple inputs)\\nand using the label words from each dataset as it is.\\nWe also explore manual templates taken from prior\\nwork (Holtzman et al., 2021; Zhao et al., 2021) as\\nreported in Section 4.2, although we ﬁnd that using\\nthese templates is not consistently better than using\\nminimal templates. We thus run main experiments\\nwith minimal templates. Example templates are\\nprovided in Table 3.\\nFormat of the demonstrations We follow the\\nstandard of each model for formatting the demon-\\nstrations, either from exploration in prior work or\\nthe example code provided in the ofﬁcial tutorial.\\nFor GPT-2, we separate the input and the label,\\nDataset # Train # Eval\\nTask category: Sentiment analysis\\nﬁnancial_phrasebank 1,811 453\\npoem_sentiment 892 105\\nTask category: Paraphrase detection\\nmedical_questions_pairs 2,438 610\\nglue-mrpc 3,668 408\\nTask category: Natural language inference\\nglue-wnli 635 71\\nclimate_fever 1,228 307\\nglue-rte 2,490 277\\nsuperglue-cb 250 56\\nsick 4,439 495\\nTask category: Hate speech detection\\nhate_speech18 8,562 2,141\\nethos-national_origin 346 87\\nethos-race 346 87\\nethos-religion 346 87\\ntweet_eval-hate 8,993 999\\ntweet_eval-stance_atheism 461 52\\ntweet_eval-stance_feminist 597 67\\nTask category: Question answering\\nquarel 1,941 278\\nopenbookqa 4,957 500\\nqasc 8,134 926\\ncommonsense_qa 9,741 1,221\\nai2_arc 1,119 299\\nTask category: Sentence completion\\ncodah 1665 556\\nsuperglue-copa 400 100\\ndream 6116 2040\\nquartz-with_knowledge 2696 384\\nquartz-no_knowledge 2696 384\\nTable 2: 26 datasets used for experiments, classiﬁed\\ninto 6 task categories. # Train and # Test indicate the\\nnumber of training and test examples of the dataset.\\nNote that # train is based on the original training dataset\\nbut we use k random samples for k-shot evaluation.\\nand each demonstration example with a space. For\\nMetaICL, GPT-J and GPT-3, we separate the input\\nand the label with a newline (\\\\n), and each demon-\\nstration example with three newlines. For fairseq\\nmodels, we use a newline to separate the input and\\nthe label as well as each demonstration example.\\nDetails in variants of the demonstrations For\\n“demonstrations w/ a% accurate labels” ( 0 ≤\\na ≤ 100), we use k ×a/100 correct pairs and\\nk ×(1 −a/100) incorrect pairs in a random order,\\nas described in Algorithm 1. For “OOD demon-\\nstrations”, we use CC-News (Nagel, 2016) as an\\nexternal corpus. We consider the length of the text\\nduring sampling, so that sampled sentences have\\nsimilar length to the test input. For “demonstrations\\nwith random English words”, we use pypi.org/\\nproject/english-words for the set of En-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nDirect\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 11: Results of No-demonstration, Gold demonstration and Random demonstration on 3 classiﬁcation\\ndatasets (top) and 3 multi-choice datasets (bottom). Details in Section 4.1. This ﬁgure is for providing numbers\\nthat are comparable across models—full results with more datasets are reported in Figure 3.\\nAlgorithm 1 Forming the demonstrations with an\\naccuracy of a%.\\n1: procedure FORM DEMONS ({(xi, yi)}k\\ni=1, a)\\n2: D ←[] // demonstration to be formed\\n3: n ←k ×a/100 // number of correct pairs\\n4: G← Sample(Range(1, k), n)\\n5: for i ∈Range(1, k) do\\n6: if i ∈G then // add correct pair\\n7: D.append((xi, yi))\\n8: else // add incorrect pair\\n9: D.append((xi, Sample(C−yi)))\\n10: return D\\nglish words, which consists of 61,569 words.\\nTable 4 provides a list of example demonstra-\\ntions for each method used in Section 5.\\nC More Experimental Results\\nC.1 Gold labels vs. random labels\\nFigure 11 shares the same interface as Figure 3, but\\nall models are evaluated on 3 classiﬁcation and 3\\nmulti-choice datasets and are thus comparable to\\neach other.\\nC.2 Random labels from true distribution of\\nlabels & Task breakdown\\nIn Section 4, random labels are sampled from the\\nlabel space from a uniform distribution. We ex-\\nperiment with another variant of demonstrations in\\nthe classiﬁcation tasks, where labels are randomly\\nsampled from the true distribution of labels on the\\ntraining data. This may have large impact if labels\\nare far from uniform on the training data. Results\\nindicate that performance drop from using gold\\nlabels is further reduced compared to using uni-\\nformly random labels: with Channel MetaICL, the\\ngap is reduced from 1.9% to 1.3% absolute, and\\nwith Channel GPT-J, the gap is reduced from 5.0%\\nto 3.5% absolute.\\nFigure 12 shows performance gap between using\\ngold labels and using random labels per dataset. We\\nﬁnd that the trend that the gap is smaller than pre-\\nviously thought is consistant across most datasets.\\nNonetheless, there are a few outlier datasets where\\nperformance gap is non-negligible, such as ﬁnan-\\ncial_phrasebank and a few hate speech detection\\ndatasets. Future work may investigate on which\\ntasks the model makes more use of the correctly\\npaired training data.\\nC.3 More variants of the demonstrations\\nWe explored demonstrations with a con-\\nstant label where all labels in the demon-\\nstrations are replaced with a constant text,\\n“answer”. Speciﬁcally, a prediction is made via\\nargmaxy∈CP(y|x1, answer...xk, answer, x).\\nThis can be viewed as another way to remove the\\nimpact of the label space while keeping the impact\\nof the distribution of the input text. However,\\nresults are consistently worse than the results\\nof demonstrations with random English labels.\\nWe think this is because constant labels actually\\nchange the format of the demonstrations, since\\nthey can be viewed as part of a separator between\\ndifferent demonstration examples.\\nWe also exploreddemonstrations with the test\\ninput where all inputs in the demonstrations are\\nreplaced with the test input, each paired with a ran-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='dom label. Speciﬁcally, a prediction is made via\\nargmaxy∈CP(y|x, ˜y1...x, ˜yk, x), where ˜yi (1 ≤\\ni ≤k) is randomly sampled at uniform from C.\\nThis variant is seemingly a reasonable choice given\\nthat it satisﬁes the condition that the inputs in the\\ndemonstrations come from the same distribution\\nas the test input (since they are identical), and us-\\ning random labels is as good as using gold labels.\\nNonetheless, we ﬁnd that this variant is signiﬁ-\\ncantly worse than most other methods with demon-\\nstrations. We think this is because using the con-\\nstant input for all demonstration example signiﬁ-\\ncantly changes the format of the sequence, since the\\ninput can be viewed as part of a separator between\\ndifferent demonstration examples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='financial_phrasebank\\ntweet_eval-stance_feminist\\npoem_sentiment-new\\nsuperglue-cb\\nethos-national_origin\\ntweet_eval-stance_atheism\\nsick\\nhate_speech18\\nglue-wnli\\nethos-religion\\nethos-race\\ncodah quarel\\ncommonsense_qa\\nmedical_questions_pairs\\nopenbookqa\\ntweet_eval-hate\\nquartz-with_knowledge\\nsuperglue-copa-new\\nclimate_fever\\nqasc dream ai2_arc\\nglue-mrpc\\nquartz-no_knowledge\\nglue-rte\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nPerformance drop (Channel MetaICL, uniform)\\nfinancial_phrasebank\\nsuperglue-cb\\npoem_sentiment-new\\nethos-race\\nsick\\ntweet_eval-stance_feminist\\ntweet_eval-hatehate_speech18ethos-religion\\nglue-rte\\nclimate_fever\\nethos-national_origin\\nglue-wnli\\nquartz-with_knowledge\\ncommonsense_qa\\nqasc\\nquartz-no_knowledge\\nopenbookqa\\ndream\\ntweet_eval-stance_atheism\\nai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nglue-mrpc\\nquarel codah\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nPerformance drop (Channel GPT -J, uniform)\\ntweet_eval-stance_feminist\\npoem_sentiment-newfinancial_phrasebank\\ntweet_eval-stance_atheism\\nethos-national_origin\\nsuperglue-cbethos-religion\\nglue-wnli\\nsick codah\\nclimate_fever\\nquarel\\ntweet_eval-hate\\nethos-race\\ncommonsense_qa\\nglue-mrpc\\nhate_speech18\\nmedical_questions_pairs\\nopenbookqa\\nquartz-with_knowledge\\nsuperglue-copa-new\\nqasc dream ai2_arc\\nquartz-no_knowledge\\nglue-rte\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\nPerformance drop (Channel MetaICL, true distribution)\\nsick\\npoem_sentiment-new\\nethos-race\\ntweet_eval-hate\\ntweet_eval-stance_feminist\\nfinancial_phrasebank\\nhate_speech18\\ntweet_eval-stance_atheism\\nclimate_fever\\nethos-national_origin\\nglue-rte\\nethos-religion\\nglue-wnli\\nquartz-with_knowledge\\ncommonsense_qa\\nqasc\\nglue-mrpc\\nquartz-no_knowledge\\nopenbookqa\\ndream ai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nquarel codah\\nsuperglue-cb\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nPerformance drop (Channel GPT -J, true distribution)\\nFigure 12: Performance gap from using the demonstrations with gold labels to using the demonstrations with\\nrandom labels. Datasets are sorted in descending order. The top two ﬁgures use random labels that are sampled\\nat uniform, with Channel MetaICL and Channel GPT-J, respectively. The bottom two ﬁgures use random labels\\nthat are sampled from a true distribution of labels on the training data, with Channel MetaICL and Channel GPT-J,\\nrespectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Dataset Type Example\\nMRPC Minimal sentence 1: Cisco pared spending to compensate for sluggish sales . [SEP] sentence 2: In\\nresponse to sluggish sales , Cisco pared spending . \\\\n {equivalent|not_equivalent}\\nManual Cisco pared spending to compensate for sluggish sales . \\\\n The question is: In response to\\nsluggish sales , Cisco pared spending . True or False? \\\\n The answer is:{True|False}\\nRTE Minimal sentence 1: The girl was found in Drummondville. [SEP] sentence 2: Drummondville\\ncontains the girl. \\\\n {entailment|not_entailment}\\nManual The girl was found in Drummondville. \\\\n The question is: Drummondville contains the\\ngirl. True or False? \\\\n The answer is:{True|False}\\nTweet_eval-hate Minimal The Truth about #Immigration \\\\n {hate |non-hate}\\nManual Tweet: The Truth about #Immigration \\\\n Sentiment: {against |favor}\\nSICK Minimal sentence 1: A man is screaming. [SEP] sentence 2: A man is scared. \\\\n\\n{contradiction|entailment|neutral}\\nManual A man is screaming. \\\\n The question is: A man is scared. True or False? \\\\n The answer is:\\n{False|True|Not sure}\\npoem-sentiment Minimal willis sneered: \\\\n {negative |no_impact|positive}\\nManual willis sneered: \\\\n The sentiment is: {negative |no_impact|positive}\\nOpenbookQA Minimal What creates a valley? \\\\n {feet |rock|water|sand}\\nManual The question is: What creates a valley? \\\\n The answer is: {feet |rock|water|sand}\\nCommonsenseQA Minimal What blocks sunshine? \\\\n {summer |park|desktop|sea|moon}\\nManual The question is: What blocks sunshine? \\\\n The answer is: {summer |park|desktop|sea|moon}\\nCOPA Minimal Effect: I coughed. \\\\n {Cause: I inhaled smoke. |Cause: I lowered my voice.}\\nManual I coughed because {I inhaled smoke. |I lowered my voice.}\\nARC Minimal Which biome has the most vegetation? \\\\n {desert |forest|grassland|tundra}\\nManual The question is: Which biome has the most vegetation? \\\\n The answer is: {desert |forest|\\ngrassland|tundra}\\nTable 3: A list of minimal templates taken from Ye et al. (2021); Min et al. (2021b) and manual templates taken\\nfrom Holtzman et al. (2021); Zhao et al. (2021). Details provided in Appendix B. See Figure 6 for discussion in\\nempirical results. The input and the label are in the red text and in the blue text, respectively. Note that |is used to\\nseparate different options for the labels.\\nDemos\\nw/ gold labels\\n(Format \\x13 Input distribution \\x13 Label space \\x13 Input-label mapping \\x13)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n positive\\nPanostaja did not disclose the purchase price. \\\\n neutral\\nDemos\\nw/ random labels\\n(Format \\x13 Input distribution \\x13 Label space \\x13 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n neutral\\nPanostaja did not disclose the purchase price. \\\\n negative\\nOOD Demos\\nw/ random labels\\n(Format \\x13 Input distribution \\x17 Label space \\x13 Input-label mapping \\x17)\\nColour-printed lithograph. Very good condition. Image size: 15 x 23 1/2 inches. \\\\n neutral\\nMany accompanying marketing claims of cannabis products are often well-meaning. \\\\n negative\\nDemos\\nw/ random English words\\n(Format \\x13 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n unanimity\\nPanostaja did not disclose the purchase price. \\\\n wave\\nDemos\\nw/o labels\\n(Format \\x17 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008.\\nPanostaja did not disclose the purchase price.\\nDemos\\nlabels only\\n(Format \\x17 Input distribution \\x17 Label space \\x13 Input-label mapping \\x17)\\npositive\\nneutral\\nTable 4: Example demonstrations when using methods in Section 5. The ﬁnancial_phrasebank dataset with C=\\n{“positive”, “neutral”, “negative”}is used. Red text indicates the text is sampled from an external corpus; blue\\ntext indicates the labels are randomly sampled from the label set; purple text indicates a random English word.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Rethinking the Role of Demonstrations:\\nWhat Makes In-Context Learning Work?\\nAnonymous ACL submission\\nWe are glad that all reviewers ﬁnd that the001\\npaper is novel (8jk5, LQ6N, 92YB, 7E5P), of002\\ninterest to the broader NLP community (LQ6N,003\\n92YB, 7E5P), supported by solid experiments004\\n(8jk5, LQ6N, 92YB, 7E5P), and well-written (8jk5,005\\nLQ6N, 92YB). Reviewers gave comments on more006\\ndiscussion, limitations and avenues for future work.007\\nWe will incorporate them in the next version of the008\\npaper.1009\\nAdding discussion on robustness of LMs (8jk5,010\\n7E5P): We think the fact that LMs do not use011\\nthe input-label correspondence does not necessar-012\\nily mean that LMs are robust to other aspects of013\\nthe demonstration. Nonetheless, it will be an inter-014\\nesting avenue for future work, given that LMs are015\\nhighly sensitive to small changes in the demonstra-016\\ntions (??).017\\nAdding discussion with respect to the model018\\nsize (8jk5): Absolute differences are similar019\\nacross different model sizes (Figure 3), but since020\\nthe large models have higher absolute performance,021\\nthe relative differences are larger with larger mod-022\\nels.023\\nWhen our ﬁndings hold or do not hold (8jk5):024\\nWe believe that the ﬁndings will hold only when025\\nsome notion of input-label correspondences are026\\nalready captured during pre-training—for tasks027\\nwhose input-label correspondences during pretrain-028\\ning is sparse, the demonstrations with random la-029\\nbels are highly unlikely to work. This has been030\\nshown by ? in a synthetic setup, and future work031\\ncan revisit this with more natural data.032\\nRisk in applying in-context learning, Rec-033\\nommendation to practitioners (8jk5, LQ6N):034\\nSince the models do not capture the correspondence035\\nfrom the demonstrations, it is possible that models036\\nare simply relying on some notation of input-label037\\ncorrespondence during pre-training. Thus, apply-038\\ning in-context learning for problems that were not039\\n1We answered reviewers’ questions on the OpenReview\\npage, and address higher-level comments here.\\nin the pretraining data would be potentially risky, 040\\nand practitioners may want to collect the labeled 041\\ndata and ﬁne-tune the model. 042\\nWhere do the gains from demonstrations come 043\\nfrom? (92YB): We think gains from demonstra- 044\\ntions are mainly due to the speciﬁcation of the in- 045\\nput distribution and the label space rather than the 046\\ninput-label correspondence, as Section 5 indicates. 047\\nWe will clarify this in the next version of the paper. 048\\nConcrete answer to “why” using random labels 049\\nkeeps reasonable performance (LQ6N): We 050\\nthink it is highly likely to be because the model 051\\nhas been exposed to some notion of input-label cor- 052\\nrespondence during pre-training, so that the demon- 053\\nstrations play a role of triggering them at inference. 054\\nConsideration when training large LMs 055\\n(LQ6N): Due to compute limitations, we were 056\\nnot be able to provide recommendations that 057\\nare empirically supported. Future work may 058\\ninvestigate aspects of language model pretraining 059\\nthat affect the ﬁndings, including the pretraining 060\\ndata and the objective. 061\\nWord ordering matters? (7E5P): We think it 062\\nis an important avenue for future work. For this 063\\npaper, we did not include it in our scope due to 064\\nrequiring multiple times more experiments. 065\\nMore task types, e.g., generation (7E5P): Ex- 066\\ntending this work to generation tasks is not very 067\\ntrivial because designing the demonstrations that do 068\\nnot keep the input-output correspondence but keep 069\\nthe output distribution is difﬁcult. For instance, if 070\\nwe simply replace the output with a random out- 071\\nput as we did in the classiﬁcation tasks, it will 072\\ndestroy both the input-output correspondence and 073\\nthe output distribution. We hope future work can 074\\ninvestigate more in this direction. 075\\nStronger link with instruction-following models 076\\n(7E5P): We will add discussion on ? who ﬁnd 077\\nthat instructions that are irrelevant or even mislead- 078\\ning lead to performance gains as much as “good” 079\\n1\\narXiv:2202.12837v2  [cs.CL]  20 Oct 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='instructions do.080\\nLimitation081\\nThis paper focuses on the tasks from established082\\nNLP benchmarks that have real natural language083\\ninputs. Synthetic tasks with more limited inputs084\\nmay actually use the ground truth labels more, as085\\nobserved by ?. Our paper mainly includes macro-086\\nlevel analysis by examining the average perfor-087\\nmance over multiple NLP datasets, but different088\\ndatasets may behave differently. Appendix dis-089\\ncusses this aspect, including ﬁndings that there are090\\nlarger gaps between using the ground truth labels091\\nand using the random labels in some dataset-model092\\npairs (e.g., in the most extreme case, nearly 14%093\\nabsolute on the ﬁnancial_phrasebank dataset with094\\nGPT-J). We believe it is important to understand in095\\nwhich cases the ground truth labels matter or not,096\\nwhich we leave for future work. Furthermore, our097\\nwork is limited to classiﬁcation and multi-choice098\\ntasks. Extension to the open-set tasks such as gener-099\\nation is not trivial, since it is unclear how to remove100\\nthe input-output correspondence while keeping the101\\ncorrect output distribution. We leave the extension102\\nfor future work.103\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Benchmarking of Retrieval Augmented Generation: A \\nComprehensive Systematic Literature Review on Evaluation \\nDimensions, Evaluation Metrics and Datasets \\nSimon Knollmeyer1,* a , Oğuz Caymazer2,* b , Leonid Koval1c , Muhammad Uzair Akmal1d ,  \\nSaara Asif1e , Selvine G. Mathias1f  and Daniel Großmann1g  \\n1Technische Hochschule Ingolstadt, AImotion Bavaria, Esplanade 10, Ingolstadt, Germany \\n2University of Münster, Department of Information Systems, Münster, Germany \\n{Simon.Knollmeyer, Leonid.Koval, MuhammadUzair.Akmal, Saara.Asif, SelvineGeorge.Mathias, \\n \\nKeywords: Large Language Model,  Retrieval Augmented Generation,  Evaluation Dimensions, Evaluation Metrics, \\nDatasets, Systematic Literature Review. \\nAbstract: Despite the rapid advancements in the field of Large Language Models (LLM), traditional benchmarks have \\nproven to be inadequate for asse ssing the performance of Retrie val Augmented Generation (RAG) systems. \\nTherefore, this paper presents a comprehensive systematic literature review of evaluation dimensions, metrics, \\nand datasets for RAG systems. Thi s review identifies key evalua tion dimensions such as context relevance, \\nfaithfulness, answer relevance,  correctness, and citation quali ty. For each evaluation dimension, several \\nmetrics and evaluators are propo sed on how to assess them. This  paper synthesizes the findings from 12 \\nrelevant papers and presents a concept matrix that categorizes each evaluation approach. The results provide \\na foundation for the development of robust evaluation frameworks and suitable datasets that are essential for \\nthe effective implementation and deployment of RAG systems in real-world applications. \\n1 INTRODUCTION \\nThe rapid evolution of Artificial Intelligence (AI) \\nespecially in the field of Large Language Models \\n(LLMs) attracts widespread attention due to their \\ngroundbreaking achievements in solving complex \\nproblems even surpassing the performance of humans \\nin certain fields (Benbya et al., 2024; Bubeck et al., \\n2023; OpenAI et al., 2023). The speed of AI \\ndevelopment in the area of LLMs outpaced methods \\nto assess their performance and accuracy, leading to \\nmajor flaws in existing traditional benchmarks to \\nevaluate LLMs output through reliable metrics  \\nimpeding their adoption (Hammond, 2024).  \\n \\na  https://orcid.org/0009-0002-1429-6992 \\nb  https://orcid.org/0009-0003-4096-3784 \\nc  https://orcid.org/0000-0003-4845-6579 \\nd  https://orcid.org/0009-0007-3961-1174 \\ne  https://orcid.org/0009-0006-1284-5635 \\nf  https://orcid.org/0000-0002-6549-0763 \\ng  https://orcid.org/0000-0002-7388-5757 \\n* co-authors of the paper, contributed equally \\nDespite their potential, LLMs face substantial \\nchallenges, particularly in fully grasping contextual \\nfactors such as unique technical requirements within \\na specific industries, yet understanding these factors \\nis essential for effective decision-making (Benbya et \\nal., 2024). Even the most powerful models such as \\nGPT-4 struggle with hallucinations, lack of the ability \\nto update itself, and limited context (Bubeck et al., \\n2023; OpenAI et al., 2023). Several researchers point \\nout that these LLMs seem to rather memorize \\nfrequently occurring information encountered during \\ntheir pre-training and struggle with infrequent \\ninformation, i.e., which would typically occur in a \\nspecific industry (Kandpal et al., 2022; Mallen et al., \\n2022). \\nKnollmeyer, S., Caymazer, O., Koval, L., Akmal, M., Asif, S., Mathias, S. and Großmann, D.\\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation Metrics and Datasets.\\nDOI: 10.5220/0013065700003838\\nPaper published under CC license (CC BY -NC-ND 4.0)\\nIn Proceedings of the 16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K 2024) - Volume 3: KMIS, pages 137-148\\nISBN: 978-989-758-716-0; ISSN: 2184-3228\\nProceedings Copyright© 2024 by SCITEPRESS – Science and Technology Publications, Lda.\\n137'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='M o s t  p r o m i s i n g  a n d  c o m m o n  t o  s o l v e  t h i s  \\nproblem is to augment LLM with non-parametric less \\ncommon knowledge by providing them retrieved text \\nchunks from an external database (Asai et al., 2024; \\nY. Gao et al., 2023; Mallen et al., 2022; Wang et al., \\n2023; Zhang et al., 2023). This approach is known as \\nRetrieval Augmented Generation (RAG), and there \\nare several different RAG paradigms, ranging from \\nso-called naïve RAG to more advanced ones (Asai et \\nal., 2023; Y. Gao et al., 2023; Ma et al., 2023). \\nResearch results suggest that LLM RAGs outperform \\nLLMs, particularly in long-t ail knowledge questions \\n(Asai et al., 2023; Izacard et al., 2022; Ma et al., 2023; \\nMallen et al., 2022; Wang et al., 2023). \\nHowever, there is still uncertainty about the \\naccuracy of such approaches due to the lack of \\ncomprehensive evaluation frameworks to provide \\nevaluation dimensions and metrics to assess RAG \\nLLMs (Y. Gao et al., 2023; Wang et al., 2023). Thus, \\nwe address the following research questions (RQ): \\n \\n• R Q 1 :  H o w  t o  e v a l u a t e  a  R A G - e n h a n c e d  \\nLLM comprehensively across different \\ndimensions and metrics? \\n• RQ2: What type of datasets are available for \\napplying the dimensions and metrics?  \\n \\n Therefore, the research contribution of this paper \\nl i e s  i n  a d d r e s s i n g  t h e  c u r r e n t  r e s e a r c h  g a p  b y  \\nproviding a systematic overview of how to evaluate \\nRAG pipelines comprehensively, offering insights \\ninto the development of robust evaluation \\ndimensions, metrics and possible datasets (Y. Gao et \\nal., 2023; Hammond, 2024; Wang et al., 2023). \\nThe paper is structured as follows: The next \\nsection introduces RAG. Section three explains the \\nchosen research method. Section four presents the \\nevaluation framework. Finally, the last section \\nprovides the conclusions. \\n2 RETRIEV AL AUGMENTED \\nGENERATION \\nTraditional pre-trained LLMs such as GPT and BERT \\nencode knowledge within their parameters, but \\nstruggle with tasks requi ring specific factual \\nknowledge which is not present in their parameters \\n(Y. Gao et al., 2023; Lewis et al., 2020). This problem \\nis evident in the fact that even the most powerful \\nmodels such as GPT-4 struggle with made-up facts \\nknown as hallucinations, a lack of ability to self-\\nupdate and limited context (Bubeck et al., 2023; \\nOpenAI et al., 2023). Even further increasing the size \\nof their parameters, i.e., the training dataset, in which \\nthe knowledge appears to be stored to include more \\ninformation will be likely insufficient to address the \\nissue of long-tail knowledge (Kandpal et al., 2022; \\nMallen et al., 2022). \\nTherefore, Lewis et al. (2020) proposed RAG by \\ncombining non-parametric memory with parametric \\nmemory that uses a dense vector index of external \\ndocuments such as Wikipedia articles that can be \\ndynamically accessed using a retriever. Research \\nresults comparing the performance of RAG LLM \\nwith standalone LLM, suggest for the former superior \\nperformance (Asai et al., 2023; Izacard et al., 2022; \\nLewis et al., 2020; Ma et al., 2023; Mallen et al., \\n2022; Wang et al., 2023). \\nA naïve RAG pipeline involves three main steps. \\nFirstly, the documents containing specific \\ninformation are indexed (L ewis et al., 2020). The \\nmost common method is to split the documents into \\nsmaller sections so-called chunks and store their \\nembedding in a vector database (Y. Gao et al., 2023). \\nI n  t h e  s e c o n d  s t e p ,  a  g i v e n  i n p u t  q u e r y  i s  l i k e w i s e  \\nembedded and then compared with the passages in the \\nvector database by calcu lating the similarity, \\nreturning a set of top-ranked chunks that are most \\nrelevant for the query (Y. Gao et al., 2023; Karpukhin \\net al., 2020). In the final step, the retrieved content \\nand the query are combined and prompted into an \\nLLM so that it can provide a coherent answer (Y. Gao \\net al., 2023; Lewis et al., 2020). \\nThis naïve setup can be modified by applying \\ndifferent advanced methods relating to pre- or post-\\nretrieval (Asai et al., 2023; Y. Gao et al., 2023; Ma et \\nal., 2023). For instance, Ma et al. (2023) propose \\nquery rewriting as an advanced pre-retrieval method \\nand report performance improvements.  \\nDespite the use of advanced methods, the RAG \\napproach can still be divided into the outlined steps of \\na naïve RAG for evaluation. However, there is a lack \\nof evaluation dimensions and metrics on how to \\nanalyse and assess such systems, e.g., which evaluation \\ndimensions to consider and what kind of metrics to \\ncalculate for which step (Y. Gao et al., 2023). \\n3 RESEARCH METHOD \\nThe Systematic Literature Review (SLR) is a well-\\nknown and established research method within \\nInformation Systems (IS)  research for reviewing \\nscientific articles based on a search process (Bell et \\nal., 2019; Paré et al., 2016). The term \"systematic\" \\nmeans that the research steps should be \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n138'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='understandable, reproducible, and grounded in a \\nstructured process that minimizes potential researcher \\nbias by providing a clear audit trail for decisions and \\nconclusions (Bell et al., 2019). SLR is especially \\nvaluable when summarizing and comparing \\nfragmented knowledge on a certain topic (Bell et al., \\n2019). Existing literature reveals a significant gap in \\ncomprehensive evaluation frameworks for assessing \\nRAG systems (Y. Gao et al., 2023; Wang et al., 2023). \\nTherefore, given the emerging and unexplored \\nresearch on evaluation dimensions for RAG, the SLR \\nis particularly appropriate. \\nThis paper ensures rigorous documentation by \\nusing the literature search process proposed by \\nBrocke et al. (2009), extended with the \\nrecommendations of Paré et al. (2016). It also follows \\nthe recommendation of Webster and Watson (2002) \\nto use a concept matrix for structuring and comparing \\nthe results. The complete adopted literature search \\nprocess is illustrated in Figure 1. \\n \\n \\nFigure 1: Systematic Literature Review process. \\nThe search process incl uded conducting a pre-\\nstudy on Google Scholar by a detailed screening of \\ntitles, abstracts and full texts to identify papers \\nspecifically addressing evaluation metrics for RAG \\nsystems (Y. Gao et al., 2023; Wang et al., 2023). \\nThese insights resulted in the following search string: \\n \\n• TITLE-ABS-KEY (“Retrieval Augmented \\nGeneration” AND “Evaluation Metric”) \\n \\nThe following inclusion and exclusion criteria \\nwere applied to filter relevant papers from the search \\nresults in Google Scholar, Scopus and the IS \\nconferences ICIS and ECIS: \\nInclusion Criteria: \\n• Papers from academic journals, conferences, \\nor gray literature.  \\n• Published in English. \\n \\nExclusion Criteria: \\n• Duplicates across databases. \\n• Minimal relevance to evaluation metrics, or \\nlack of focus on RAG systems. \\n• No guidelines on metric implementation or \\napplication at the different RAG steps. \\n \\nThe search process commen ced with an abstract \\nscreening of the initial results, followed by a full-text \\nreview of the selected papers. Forward and backward \\ncitation searches were subsequently performed on \\nrelevant studies to identify additional literature. This \\ncomprehensive approach yielded a final sample of 12 \\npapers, as illustrated in Figure 1. \\nIn the final step, overarching categories from the \\nfinal sample were synthesized into a concept matrix \\n(Webster & Watson, 2002). Relevant concepts on \\nevaluation dimensions were identified and mapped to \\nthe RAG steps (cf. Section 2) to provide an accurate \\noverview on how to evaluate each RAG phase. \\nThe concept matrix is shown in Table 1. It \\ncategorizes the sampled papers based on predictive \\nevaluation criteria and dataset characteristics. The \\nformer includes the columns \"Retrieval\" and \\n\"Generation\" relating to the RAG steps. The \\n\"Evaluator\" column indi cates whether lexical \\nmatching, semantic similarity or LLM as a judge is \\nused for evaluation. The dataset characteristics \\ninclude single-hop and multi-hop reasoning tasks, \\nsynthetic datasets (triples), and open-domain question \\nanswering (QA). Each paper is marked (\"X\") to show \\nthe criteria it addresses, p roviding a comprehensive \\noverview of their focus areas. In addition, the \\nfrequency of occurrence in the literature can be used \\nto determine how widespread or accepted a metric is. \\nEach proposed metric for the evaluation dimensions \\ndepending on the evaluator and the requirements for \\nthe data to calculate it are summarized in Table 2. \\n4  RESULTS \\nThis section starts with examining the first column of \\nthe concept matrix: predictive evaluation , which \\ninvolves assessing the performance of the RAG \\nsystem in retrieving accurat e context and effectively \\nutilizing it to generate responses (Guinet et al., 2024).  \\n \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n139'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 1: Concept Matrix with Evaluation Dimensions and Datasets. \\n \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n140'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 2: Summary of proposed Evaluation Dimensions, the corresponding Metrics and Dataset Requirements. \\n \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n141'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='The proposed evaluation pipeline outlines (cf. \\nFigure 2) the process of assessing the RAG approach \\nacross various evaluation dimensions and focuses on \\nthe retrieval and generation stages of a typical RAG \\nsystem. The evaluation pr ocess starts with the \\nretrieval step, emphasizing context relevance as a \\ncritical dimension to assess how effectively relevant \\ninformation is retrieved. \\nSubsequently, the focus then shifts to the \\ngeneration step , examining the evaluation \\ndimensions of answer relevance, correctness, \\nfaithfulness, a n d  citation quality to determine the \\naccuracy and reliability of the generated responses. \\nEach evaluation dimension is carefully defined, and \\nquantifying metrics are proposed according to the \\nsampled papers.   \\nHow and what type of metric is ultimately used to \\ncalculate the respective evaluation dimension \\ndepends on the chosen evaluator. Lexical matching \\nmetrics focus on exact word matching and simple \\nstatistical calculations, such as keyword frequency or \\nposition-based measures like Mean Reciprocal Rank \\n(MRR), i.e., these metrics assess how closely the \\nwords in the documents match the query without \\nconsidering deeper meanings.  \\nSemantic similarity metrics, on the other hand, \\ngo beyond surface-level text comparison to \\nunderstand the underlying meanings and concepts by \\ncomparing their semanti cal similarity based on \\ncontext and conceptual relationships between words \\nand sentences. This approac h captures the intent of \\nthe query and the documents, evaluating relevance \\nthrough semantic similarity rather than just keyword \\noccurrence.  \\nFinally, LLM as a judge uses a LLM to evaluate \\ncontent by making it context-aware, prompting the \\nmodel to consider the coherence, factuality, and \\nrelevance of the information based on its \\ncomprehensive understanding of language and \\nknowledge. Therefore, the choice of the evaluator \\ndetermines the type of metric applied to assess each \\nevaluation dimension, depending on whether the \\nfocus is on exact word matching, conceptual \\nsimilarity, or a nuanced, context-aware judgment by \\nan advanced LLM.  \\nThe upcoming sub-sections follows the rationale \\nof first explaining the evaluation dimension and then \\nthe respective evaluator by detailing how to calculate \\nthe metric proposed in the sampled papers. \\n4.1 Context Relevance \\nThe evaluation dimension of context relevance  \\npertains to the retrieval step and assesses the degree \\nto which the retrieved context contains only the \\nnecessary information to answer the query, reducing \\ncomputational costs and improving efficiency by \\nminimizing irrelevant content (Es et al., 2023; Saad-\\nFalcon et al., 2023; Yu et al., 2024). Additionally, \\nwhen retrieved passages are too    LLMs often \\nstruggle to effectively utilize the information, \\nparticularly if the relevant details are embedded in the \\nm i d d l e  o f  t h e  p a s s a g e  ( E s  e t  a l . ,  2 0 2 3 ) .  H e n c e ,  \\nconcise query-relevant passages significantly \\nimprove the LLM generation quality (Es et al., 2023; \\nYu et al., 2024).  \\nRecall@k and MRR@k a r e  k e y  lexical metrics \\nfor evaluating the retrieval performance in RAG \\nsystems (Rackauckas et al., 2024; Yu et al., 2024). \\nEach metric provides a different perspective on the \\neffectiveness\\n of the retrieval process. Recall@k \\n \\nFigure 2: Evaluation Pipeline with Evaluation Dimensions in a naïve RAG setup. \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n142'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='measures how many relevant passages are captured \\nwithin the top k retrieved chunks, even if some \\nirrelevant ones are included. The formula is as follows:  \\n \\nRecall@k = |Relevant Passages ∩ k-Passages|\\n|Relevant Passages|  (1)\\n \\nMRR@k calculates context relevance by \\nemphasizing the rank of the first relevant passage \\nacross multiple queries (Rackauckas et al., 2024). If a \\nrelevant passage appears in the top k results, its \\ncontribution to the MRR@k score is the inverse of its \\nrank, e.g., a passage ranked two contributes as ½ with \\nMRR@5 (Rackauckas et al., 2024). If no relevant \\npassage is found in the top k the contribution is zero. \\nThe formula for MRR@k is: \\n \\nMRR@k =1\\n|Q| \\u0dcd 1\\nrank୧\\n|୕|\\n୧ୀଵ\\n (2)\\n \\nBy using the LLM as a judge , it is possible to \\ncalculate an estimated context relevance score. Given \\na query q and its retrieved context c(q), the LLM is \\nprompted to extract a subset of sentences (𝑆\\n\\u0bd8௫௧) from \\nc(q) that are relevant to answering q by using the \\nfollowing prompt: \\n \\n\"Please extract relevant sentences from the \\nprovided context that can potentially help answer the \\nfollowing question. If no relevant sentences are \\nfound, or if you believe the question cannot be \\nanswered from the given context, return the phrase \\n\\'Insufficient Information\\'. While extracting candidate \\nsentences, you\\'re not allowed to make any changes to \\nsentences from the given context.\" (Es et al., 2023) \\n \\nThe prompt instructs the LLM to select only the \\nsentences that it considers relevant to q without \\nchanging the content. The Context Relevance Score \\n(CRS) is calculated by dividing the relevant \\nsentences extracted (𝑆\\n\\u0bd8௫௧) from the context c(q) by the \\ntotal number of sentences. This can be expressed with \\nthe following formula (Es et al., 2023): \\n \\nCRS = Number of extracted sentences Sୣ୶୲\\nTotal number of sentences in c(q)  (3)\\n \\nThe CRS indicates the proportion of the context \\nthat is relevant. A higher score indicates that a greater \\nproportion of the retrieved context is focused and \\nrelevant for answering the query, while a lower score \\nindicates that much of the retrieved context contains \\nirrelevant information (Es et al., 2023; Saad-Falcon et \\nal., 2023; Yu et al., 2024). \\n4.2 Faithfulness \\nThe evaluation dimensions faithfulness refers to the \\ngeneration step and evaluates how well an LLM\\'s \\nresponse is grounded in the retrieved context, i.e., all \\ninformation in the response  can be directly inferred \\nfrom it (Adlakha et al., 2023; Es et al., 2023; Hu et \\nal., 2024). This evaluation dimension is crucial to \\nidentify possible hallucinations in the answer of \\nLLMs ensuring factual correctness (Adlakha et al., \\n2023; Es et al., 2023; Ravi et al., 2024). For instance, \\nAdlakha et al. (2023) found that GPT-4 had the \\nhighest agreement with human annotations, followed \\nby GPT-3.5 and k-precision.  \\nAs the primary lexical metric  Adlakha et al. \\n(2023) propose k-precision to evaluate the degree of \\nfaithfulness since it has the highest agreement with \\nhuman judgements. It can be calculated as the \\nproportion of tokens in the LLMs response that are \\npresent in the retrieved context, i.e., it is the overlap of \\nmatching tokens with the retrieved context divided by \\nthe total number of tokens in the response (Adlakha et \\nal., 2023). Hence, the formula is as follows: \\n \\nK-Precision = Matched Tokens\\nResponse Tokens (4)\\n \\nAnother way of calculating the faithfulness is by \\nusing LLMs such as GPT-4/3.5 as evaluators. The \\nLLMs are prompted to judge whether the response \\nand the retrieved context match from an ordinal scale \\nof “fully”, “partially”, or “not at all” (Adlakha et al., \\n2023). In the same way, Ravi et al. (2024) propose to \\nassess the responses through an evaluator LLM \\nwhether the responses are supported, contradicted or \\nnot supported by the retrieved context. In order to \\nquantify this ordinal scale, we can assign numerical \\nscores depending on the degree of support and take \\nthe average of all individu al response scores, i.e., \\nassign 1 for “fully”, 0.5 for “partially”, and 0 for “not \\nat all”. The formula for calculating the Faithfulness \\nCoefficient (FC) is as follows: \\n \\nFC = ∑ Faithfulness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (5)\\n \\nA similar method is used by Hu et al. (2024), who \\npropose a framework that e xtracts “claim triplets” \\n(subject, predicate, object) to represent fine-grained \\nknowledge assertions within the LLM response. The \\npurpose of this extraction is to break down the answer \\ninto specific atomic cla ims that can be checked \\nindividually (Hu et al., 2024; Min et al., 2023). A \\njudging LLM evaluates each triplet as “entailment” \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n143'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"(supported), “contradiction” (contradicted), or \\n“neutral” (unsupported) (Hu et al., 2024). Other \\nauthors also follow this approach of breaking down \\nthe statements from the LLM response into atomic \\nfacts to obtain a fine-grained measure of the \\nfaithfulness degree (Min et al., 2023). \\nEs et al. (2023) propose a process in which the \\nanswer 𝑎\\n௦(𝑞)  is considered faithful to the context \\n𝑐(𝑞) if the statements in the LLM response can be \\ndirectly inferred from the retrieved context. The \\nprocess begins by using a LLM as a judge  t o  \\ndecompose the LLM response into a set of statements, \\n𝑆(𝑎௦(𝑞)) , which involves breaking down longer \\nsentences into shorter ones (Es et al., 2023). For each \\nstatement 𝑠\\u0bdc in 𝑆, the judging LLM verifies if it can \\nbe inferred from the given context 𝑐(𝑞)  using a \\nverification function 𝑣(𝑠\\u0bdc,𝑐 (𝑞)) (Es et al., 2023). The \\njudging LLM assesses whether each statement is \\nsupported by the information in the retrieved context \\nand provides a “yes” or “no” verdict for each \\nstatement (Es et al., 2023). The Faithfulness Score \\n(𝐅𝐒) is then calculated as the  ratio of the number of \\nsupported statements 𝑉  to the total number of \\nstatements S, which can be expressed as follows: \\n \\nFS = |V|\\n|S| (6)\\n4.3 Answer Relevance \\nThe evaluation dimensions answer relevance refers \\nto the generation step and assesses whether the LLM \\nr e s p o n s e  i s  d i r e c t l y  a d d r e s s i n g  t h e  q u e r y  ( E s  e t  a l . ,  \\n2023; Rackauckas et al., 2024; Saad-Falcon et al., \\n2023; Yu et al., 2024). This evaluation dimension \\npenalizes incomplete or redundant answers, \\nregardless of factuality (Es et al., 2023; Rackauckas \\net al., 2024). \\nBy using LLM as a judge  it is possible to \\ncalculate an estimation of the answer relevance (Es et \\nal., 2023; Rackauckas et al., 2024; Saad-Falcon et al., \\n2023). Given a generated answer 𝑎\\n௦(𝑞), the judging \\nLLM is prompted to generate 𝑛 potential questions 𝑞\\u0bdc \\nthat could be answered by using 𝑎௦(𝑞)  (Es et al., \\n2023). This is done using the following prompt: \\n \\nGenerate a question for the given answer: \\nanswer:[answer] (Es et al., 2023) \\n \\nSubsequently, text embeddings for all generated \\nquestions 𝑞\\u0bdc  and the original query 𝑞 are created to \\ncalculate in the next step the cosine similarity \\nbetween their embeddings (Es et al., 2023). The \\nAnswer Relevance Score (ARS)  is obtained by \\naveraging the similarity  between the generated \\nquestions 𝑞\\n\\u0bdc  and the original query 𝑞  using the \\nfollowing formula: \\n \\nARS = 1\\nn \\u0dcds i m(q, q୧)\\n୬\\n୧ୀଵ\\n (7)\\n \\nwhere 𝑠𝑖𝑚(𝑞, 𝑞\\u0bdc) represents the cosine similarity \\nbetween the embeddings of the generated questions \\nq୧  and the original query q (Es et al., 2023). This \\nmetric effectively measures how well the generated \\nanswer matches the intent and content of the original \\nquestion (Es et al., 2023). \\n4.4 Correctness \\nThe evaluation dimension correctness r e f e r s  t o t he  \\ngeneration step and evaluates whether the LLM’s \\nresponse accurately matches the “golden passage” \\nprovided by human annotators (Adlakha et al., 2023; \\nT .  G a o  e t  a l . ,  2 0 2 3 ) .  T h i s  m e t r i c  focuses on the \\nfactual accuracy  of the information by comparing \\nthe LLM response with a reference answer (Adlakha \\net al., 2023; T. Gao et al., 2023; Guinet et al., 2024; \\nRackauckas et al., 2024). \\nAs a primary lexical metric  Adlakha et al. \\n(2023) propose using recall as it correlates well with \\nhuman annotations. Traditional metrics like Exact \\nMatch (EM), F1, and ROUGE are often too strict due \\nto their focus on exact word matching (Adlakha et al., \\n2023). Recall measures how much of the reference \\nanswer's essential content is captured in the model's \\nresponse without penalizing additional information \\n(Adlakha et al., 2023). \\nSome authors find that semantic similarity  \\nmetrics like BERTScore is less effective than recall \\nfor correctness due to lower alignment with human \\nannotations (Adlakha et al., 2023; T. Gao et al., \\n2023). These metrics do not account for factual \\naccuracy or logical consistency, as responses can be \\ntextually similar but factually incorrect (Adlakha et \\nal., 2023; T. Gao et al., 2023). \\nBy using LLMs such as GPT-3.5 and GPT-4 as \\nevaluators to judge the correctness of responses by \\nprompting them with the question, the reference \\nanswer, and the LLMs response to determine whether \\nthe model response is correct, partially correct, or \\nincorrect (Adlakha et al., 2 023; Rackauckas et al., \\n2024). This approach seems to yield the highest \\nagreement with human annotations (Adlakha et al., \\n2023; Rackauckas et al., 2024). Correctness can be \\nquantified by assigning scores: 1 for “fully correct”, \\n0.5 for “partially”, and 0 for “incorrect”. The \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n144\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Correctness Coefficient (CC)  is the average of all \\nresponse scores. Therefore, the formula is as follows: \\n \\nCC = ∑  Correctness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (8)\\n4.5 Citation Quality \\nThe evaluation dimension citation quality focuses on \\nassessing whether an LLM correctly cites its sources \\nwhen generating text (T. Gao et al., 2023). Citation \\nquality is calculated using two metrics: citation \\nrecall and citation precision. Citation recall ensures \\nthat every piece of information in the generated \\nresponse is fully supported by the cited passages, \\nwhile citation precision checks whether all cited \\npassages are relevant and necessary for the statements \\nmade (T. Gao et al., 2023).  \\nTo perform this evaluation automatically, the \\nLLM acts as a judge, which is prompted with a chain-\\nof-thought method instead of simple lexical matching \\n(T. Gao et al., 2023). The LLM checks if the \\nconcatenated text from the cited passages semantically \\nsupports the generated statements (T. Gao et al., 2023). \\nFor citation recall, it evaluates whether all generated \\nstatements are substantiated by the citations. A \\nstatement receives a recall score of 1 if it is fully \\nsupported by at least one citation, otherwise, it receives \\na score of 0 (T. Gao et al., 2023). For citation precision, \\nthe LLM identifies any \"irrelevant\" citations, i.e., those \\nthat do not independently support a statement or are not \\nnecessary when other citations already provide full \\nsupport (T. Gao et al., 2023) . A citation receives a \\nprecision score of 1 if it is relevant and contributes to \\nthe statement\\'s support and 0 if it is irrelevant (T. Gao \\net al., 2023). \\nThe robustness of these metrics is validated by \\ntheir strong correlation  with human judgements  \\n(T. Gao et al., 2023). By averaging the citation recall \\nand precision scores across all statements and \\ncitations in the generated response, an overall citation \\nquality score can be calculated, providing a \\ncomprehensive measure of how accurately and \\nappropriately an LLM uses citations in its output. \\n5 DATASETS & APPLICATION \\nBuilding on the concept matrix\\'s evaluation \\ndimensions presented above, it\\'s essential to consider \\nhow different datasets relate to the evaluators of these \\ndimensions and its corresponding evaluation metrics. \\nThe choice between open-domain QA datasets and \\nsynthetic datasets like RAGAS or ARES, along with \\nthe type of reasoning required (single-hop vs. multi-\\nhop), plays a crucial role in ensuring the robustness \\nand reliability of these evaluations. \\nOpen-domain QA datasets such as SQuAD2.0, \\nHotpotQA, and Natural Questions are based on \\nWikipedia articles. These are mainly suitable for \\nlexical matching  enable comparisons across RAG \\nsystems (Kwiatkowski et al., 2019; Rajpurkar et al., \\n2018; Yang et al., 2018). These datasets often include \\n“golden passages” which make them ideal for \\nevaluating correctness and faithfulness by providing \\na factual reference (Kwiatkowski et al., 2019; \\nRajpurkar et al., 2018; Yang et al., 2018). For \\ninstance, SQuAD2.0 includes unanswerable queries \\nrequiring RAG systems to recognize when there is \\ninsufficient information to provide a valid answer \\n(Rajpurkar et al., 2018; Rau et al., 2024). Also \\ncalculating context relevance  is straightforward \\nbecause the datasets provide clear ground truth in \\nterms of which passages are relevant, making them \\nideal for recall-oriented metrics like Recall@k o r  \\nMRR@k (Adlakha et al., 2023; Hu et al., 2024). \\nHowever, evaluating answer relevance and citation \\nquality is more challenging with open-domain QA \\ndatasets since these typically focus on finding a single \\ncorrect answer rather than assessing nuanced citation \\npractices or multi-source relevance (Kwiatkowski et \\nal., 2019; Rajpurkar et al., 2018; Yang et al., 2018). \\nSynthetic datasets such as RAGAS and ARES \\nare specifically designed to evaluate the effectiveness \\nof RAG systems by minimizing reliance on human \\nannotations (Es et al., 2023; Saad-Falcon et al., 2023). \\nThese frameworks often use synthetic datasets that \\nonly require query-context-response triples, making \\nthem suitable to evaluate every evaluation dimension  \\n( E s  e t  a l . ,  2 0 2 3 ;  H u  e t  a l . ,  2 0 2 4 ;  M i n  e t  a l . ,  2 0 2 3 ;  \\nSaad-Falcon et al., 2023). Synthetic datasets \\ncombined with LLM judges align well with human \\nannotations, outperforming lexical and semantic \\nsimilarity metrics (Adlakha et al., 2023; Saad-Falcon \\net al., 2023). Additionally, this approach is model-\\nagnostic, allowing flexible use across different LLMs \\nand setups (Es et al., 2023; Saad-Falcon et al., 2023). \\nThis adaptability ensures that RAG systems can \\nbe effectively assessed and fine-tuned for diverse and \\ncomplex queries, enhanci ng their performance in \\npractical, real-world setting. Table 3 summarizes the \\nkey differences between the datasets by comparing \\nthem. \\nIn terms of reasoning, single-hop and multi-hop \\nqueries require different approaches and datasets \\nSingle-hop reasoning involves deriving an answer \\nfrom\\n a single piece of evidence, i.e., one retrieved  \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n145'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 3: Comparing Open Domain QA Datasets with Synthetic Datasets for evaluating RAG. \\nAspect O pen Domain QA Dataset S ynthetic Dataset\\nExamples SQuAD2.0, HotpotQA, Natural Questions, \\nQAMPARI RAGAS, ARES \\nReasoning Type \\nSingle-hop (e.g., SQuAD2.0, Natural \\nQuestions) \\nMulti-hop (e.g., HotpotQA, QAMPARI)\\nSingle-hop and adaptable to Multi-hop \\nEvaluation \\nDimensions \\nCorrectness, Faithfulness, Context Relevance \\n(basic) \\nCorrectness, Faithfulness, Context Relevance (multi-\\nretrieval), Answer Relevance, Citation Quality \\nEvaluators Lexical Matching Semantic S imilarity, LLM as a Judge \\nStrengths High reliability for correctness and \\nfaithfulness due to golden-passages \\nModel and vendor agnostic, adaptable for various \\nqueries and rapid evaluation of RAG without the need \\nfor human annotations or gold-passages \\nLimitations \\nLess effective in evaluating multi-source \\ncitations, complex context relevance, and \\nanswer relevance in multi-hop\\nAssociated costs related to token usage and potential \\nlatency due to LLM judging, different performances \\ndepending on the employed LLM model \\n \\npassage, and is well-suited for the evaluation \\ndimensions correctness, faithfulness, and basic \\ncontext relevance . Popular single-hop datasets are \\nNatural Questions (Kwiatkowski et al., 2019) and \\nSQuAD2.0 (Rajpurkar et al., 2018), where the \\nrelevant information is contained within a single \\npassage, allowing the calculation of metrics \\npredominantly with lexical matching or simple \\nsemantic similarity, e.g., focusing on metrics such as \\nprecision and recall (Adlakha et al., 2023; Ravi et al., \\n2024). In contrast, multi-hop reasoning requires to \\nconnect multiple pieces of retrieval, usually from \\ndifferent documents or distant parts of the same \\ndocument, to be combined in order to obtain a correct \\nanswer. This approach is better suited for evaluating \\ncontext relevance  for multiple retrieval, answer  \\nrelevance, i.e., how the combined context informs the \\nanswer, and citation quality that correctly attributes \\ninformation to multiple sources (Adlakha et al., 2023; \\nEs et al., 2023; T. Gao et al., 2023; Hu et al., 2024). \\nOpen-domain QA datasets like HotpotQA (Yang et \\nal., 2018) or QAMPARI (Amouyal et al., 2022) are \\nspecifically designed for multi-hop reasoning. These \\nrequire synthesizing information from multiple \\nretrieved contexts, which involves understanding \\ncomplex connections and contextual relevance that \\ngo beyond surface-level comparisons. Employing \\nLLMs as a judge for this evaluation is the most \\nsuitable option since LLMs can comprehend the \\ncombination of multiple contexts better by making \\nmore nuanced judgement than simple lexical \\nmatching or semantical similarity of concepts (Es et \\nal., 2023; T. Gao et al., 2023; Min et al., 2023; Saad-\\nFalcon et al., 2023). \\n \\n \\n6 PRACTICAL APPLICATION \\nIn order to apply evaluation dimensions and select the \\nappropriate datasets, it is necessary to understand \\ndataset requirements. For instance, the evaluation \\ndimension faithfulness requires data regarding the \\nretrieved passages of the RAG to be tested and a \\nreference retrieval (golden retrieval), but these are \\noften not available in real-world operations. \\nTherefore, synthetic datas ets that are applicable \\nreference-free would be more suitable for practical \\noperation, and they also have a high alignment with \\nhuman annotations (Es et al., 2023; Saad-Falcon et \\nal., 2023).  \\nIt is also necessary to select suitable metrics \\naccording to the objective of optimizing the RAG \\nsystem. For this purpose, a distinction should first be \\nmade as to whether the performance of the retrieval \\nor generation step should be considered. A suitable \\nmetric is then selected according to a specific \\nproblem. For example, if the factuality of the answer \\nis to be increased, correctness is a more suitable \\nmetric than faithfulness.  \\nFinally, it is important to build an automated, \\nrobust and reliable evaluation pipeline that can be \\nused to evaluate the RAG system (Es et al., 2023). \\n7 CONCLUSION \\nThis paper proposes a comprehensive evaluation \\nframework specifically for RAG by conducting an \\nSLR and providing an extensive overview of \\ncurrently existing evaluati on approaches. Since the \\nintroduction of RAG in 2020 (Lewis et al., 2020), it \\nhas taken considerable time for methods to be \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n146'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='developed and established in the literature for \\nevaluating these approaches. Despite the rigorous \\nmethodology, it remains possible that some papers \\nwere overlooked due to the rapid pace of \\ndevelopments in the field.  \\nWith reference to RQ1, our evaluation framework \\nintroduces robust evaluation dimensions and metrics \\nto assess the different steps within RAG. Moreover, \\nthis paper advances the understanding of RAG \\nevaluation by providing reliable dimensions and \\nmetrics (Y. Gao et al., 2023; Wang et al., 2023). \\nFurthermore, Section 5 gives a comprehensive \\nsummary about RQ2 by providing an overview of the \\navailable datasets to apply the proposed evaluation \\ndimensions and metrics and what kind of \\nrequirements to consider.  \\nAs a future avenue of research, a practical \\napplication of these metrics should be conducted to \\nvalidate their use and alignment with human \\npreferences. In addition, the provision of all the \\nmetrics presented could be examined within a \\nframework to simplify practical application. \\nACKNOWLEDGEMENTS \\nThe presented paper was produced as part of the \\nresearch project MoFaPro. This project is funded by \\nthe AUDI AG. The present  approach was developed \\nwithin the institute \"AImotion Bavaria\" at the \\nTechnische Hochschule Ingolstadt. This work is part \\nof Oğuz Caymazer\\'s master\\'s thesis and was \\nconducted during his internship at the AUDI AG. \\nREFERENCES \\nAdlakha, V., Behnamghader, P., Lu, x. H., Meade, n., & \\nReddy, S. (2023). Evaluating correctness and \\nfaithfulness of instruction-following models for \\nquestion answering. Https://doi.org/10.48550/arxiv.2 \\n307.16877 \\nAmouyal, S. J., Wolfson, T ., Rubin, O., Yoran, O., \\nHerzig, J., & Be rant, J. (2022). QAMPARI: An Open-\\ndomain Question Answering Benchmark for Questions \\nwith Many Answers from Multiple Paragraphs. \\nhttps://doi.org/10.48550/arXiv.2205.12665 \\nAsai, A., Wu, Z., Wang, Y [Yizhong], Sil, A., & \\nHajishirzi, H. (2023). Self-RAG: Learning to Retrieve, \\nGenerate, and Critique through Self-Reflection. \\nhttps://doi.org/10.48550/arXiv.2310.11511 \\nAsai, A., Zhong, Z., Chen, D [Danqi], Koh, P. W., \\nZettlemoyer, L., Hajishirzi, H., & Yih, W. (2024). \\nReliable, Adaptable, and Attributable Language \\nModels with Retrieval. https://doi.org/10.48 \\n550/arXiv.2403.03187 \\nBell, E., Bryman, A., & Harley, B. (2019). Business \\nresearch methods  (Fifth edition). Oxford University \\nPress.  \\nBenbya, H., Strich, F., & Ta mm, T. (2024). Navigating \\nGenerative Artificial Intelligence Promises and Perils \\nfor Knowledge and Creative Work. Journal of the \\nAssociation for Information Systems , 25(1), 23–36. \\nhttps://doi.org/10.17705/1jais.00861 \\nBrocke, J., Simons, A., Niehaves, B., Riemer, K., \\nPlattfaut, R., & Cleven, A. (2009). Reconstructing the \\nGiant: On the Importance of Rigour in Documenting \\nthe Literature Search Process. In European Conference \\non Information Systems (Chair), 17th European Conf. \\non Information Systems . https://www.wi.uni-\\nmuenster.de/research/publications/3069 \\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., \\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., \\nLundberg, S., Nori, H., Palangi, H., Ri beiro, M. T., & \\nZhang, Y [Yi]. (2023). Sparks of Artificial General \\nIntelligence: Early experiments with GPT-4. \\nhttps://doi.org/10.48550/arXiv.2303.12712 \\nEs, S., James, J., Espinosa-Anke, L., & Schockaert, S. \\n(2023). RAGAS: Automated Evaluation of Retrieval \\nAugmented Generation. https://doi.org/10.48550/a \\nrXiv.2309.15217 \\nGao, T., Yen, H., Yu, J., & Chen, D [Danqi]. (2023). \\nEnabling Large Language Models to Generate Text \\nwith Citations. https://doi.org/10.48550/arXiv.2305. \\n14627 \\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y \\n[Yi], Sun, J., Wang, M., & Wang, H. (2023). Retrieval-\\nAugmented Generation for Large Language Models: A \\nSurvey. https://doi.org/10.48550/arXiv.2312.10997 \\nGuinet, G., Omidvar-Tehrani, B., Deoras, A., & Callot, L. \\n(2024). Automated Evaluation of Retrieval-Augmented \\nLanguage Models with Task-Specific Exam \\nGeneration. https://github.com/amazon-science/auto-\\nrag-eval https://doi.org/10.48550/arXiv.2405.13622 \\nHammond, G. (2024, April 10). Speed of AI development \\nstretches risk assessments to breaking point. Financial \\nTimes. https://www.ft.com/content/499c8935-f46e-\\n4ec8-a8e2-19e07e3b0438 \\nHu, X [Xiangkun], Ru, D., Qiu, L., Guo, Q., Zhang, T., \\nXu, Y., Luo, Y., Liu, P., Zhang, Y [Yue], & Zhang, Z \\n[Zheng]. (2024). RefChecker: Reference-based Fine-\\ngrained Hallucination Checker and Benchmark for \\nLarge Language Models. https://doi.org/10.48550/a \\nrXiv.2405.14486 \\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., \\nPetroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., \\nRiedel, S., & Grave, E. (2022). Atlas: Few-shot \\nLearning with Retrieval Augmented Language Models. \\nhttps://doi.org/10.48550/arXiv.2208.03299 \\nKandpal, N., Deng, H., R oberts, A., Wa llace, E., & \\nRaffel, C. (2022). Large Language Models Struggle to \\nLearn Long-Tail Knowledge. https://doi.org/10.48 \\n550/arXiv.2211.08411 \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n147'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"Karpukhin, V., Oğuz, B., Min , S., Lewis, P., Wu, L., \\nEdunov, S., Chen, D [Danqi], & Yih, W. (2020). Dense \\nPassage Retrieval for Open-Domain Question \\nAnswering. https://doi.org/10.48550/arXiv.2004.04906 \\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., \\nParikh, A., Alberti, C., Eps tein, D., Polosukhin, I., \\nDevlin, J., Lee, K., Toutanova, K., Jones, L., \\nKelcey, M., Chang, M.‑W., Dai, A. M., Uszkoreit, J., \\nLe, Q., & Petrov, S. (2019). Natural Questions: A \\nBenchmark for Question Answering Research. \\nTransactions of the Association for Computational \\nLinguistics, 7, 453–466. https://doi.org/10.116 \\n2/tacl_a_00276 \\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., \\nGoyal, N., Küttler, H., Lewis, M., Yih, W., \\nRocktäschel, T., Riedel , S., & Kiela, D. (2020). \\nRetrieval-Augmented Generation for Knowledge-\\nIntensive NLP Tasks. https://doi.org/10.4 \\n8550/arXiv.2005.11401 \\nMa, X., Gong, Y., He, P., Zhao, H., & Duan, N. (2023). \\nQuery Rewriting for Retrieval-Augmented Large \\nLanguage Models. https://doi.org/10.48550/arXiv. \\n2305.14283 \\nMallen, A., Asai, A., Zhong, V.,  Das, R., Khashabi, D., & \\nHajishirzi, H. (2022). When Not to Trust Language \\nModels: Investigating Effectiveness of Parametric and \\nNon-Parametric Memories. https://doi.org/10.48550/ \\narXiv.2212.10511 \\nMin, S., Krishna, K., Lyu, X., Lewis, M., Yih, W., Koh, P., \\nIyyer, M., Zettlemoyer, L.,  & Hajishirzi, H. (2023). \\nFActScore: Fine-grained Atomic Evaluation of Factual \\nPrecision in Long Form Text Generation. In H. \\nBouamor, J. Pino, & K. Bali (Eds.), Proceedings of the \\n2023 Conference on Empirical Methods in Natural \\nLanguage Processing (pp. 12076–12100). Association \\nfor Computational Linguistics. \\nhttps://doi.org/10.18653/v1/2023.emnlp-main.741 \\nOpenAI, Achiam, J., Adler, S. , Agarwal, S., Ahmad, L., \\nAkkaya, I., Aleman, F. L., Almeida, D., \\nAltenschmidt, J., Altman, S. , Anadkat, S., Avila, R., \\nBabuschkin, I., Balaji, S., Balcom, V., Baltescu, P., \\nBao, H., Bavarian, M., Belgum, J., . . . Zoph, B. \\n(2023). GPT-4 Technical Report. \\nhttps://doi.org/10.48550/arXiv.2303.08774 \\nParé, G., Tate, M., Johnstone, D., & Kitsiou, S. (2016). \\nContextualizing the twin concepts of systematicity and \\ntransparency in information systems literature reviews. \\nEuropean Journal of Information Systems, 25(6), 493–\\n508. https://doi.org/10.1057/s41303-016-0020-3 \\nRackauckas, Z., Câmara, A., & Zavrel, J. (2024). \\nEvaluating RAG-Fusion with RAGElo: an Automated \\nElo-based Framework. https://doi.org/10.48550/ \\narXiv.2406.14783 \\nRajpurkar, P., Jia, R., & Liang, P. (2018). Know What You \\nDon't Know: Unanswerable Questions for SQuAD. \\nhttps://doi.org/10.48550/arXiv.1806.03822 \\nRau, D., Déjean, H.,  Chirkova, N., Form al, T., Wa ng, S., \\nNikoulina, V., & Clinchant, S. (2024). BERGEN: A \\nBenchmarking Library fo r Retrieval-Augmented \\nGeneration. https://doi.org/10.48550/arXiv.2407. \\n01102 \\nRavi, S. S., Mielczarek, B., Kannappan, A., Kiela, D., & \\nQian, R. (2024). Lynx: An Open Source Hallucination \\nEvaluation Model. https://arxiv.org/abs/2407.08488  \\nSaad-Falcon, J., Khattab, O., Potts, C., & Zaharia, M. \\n(2023). ARES: An Automated Evaluation Framework \\nfor Retrieval-Augmented Generation Systems. \\nhttps://doi.org/10.48550/arXiv.2311.09476 \\nWang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., \\nJiayang, C., Yao, Y., Gao, W., Hu, X [Xuming], Qi, Z., \\nWang, Y [Yidong], Yang, L., Wang, J [Jindong], \\nXie, X., Zhang, Z [Zheng], & Zhang, Y [Yue]. (2023). \\nSurvey on Factuality in Large Language Models: \\nKnowledge, Retrieval and Domain-Specificity. \\nhttps://doi.org/10.48550/arXiv.2310.07521 \\nWebster, J., & Watson, R. T. (2002). Analyzing the Past to \\nPrepare for the Future: Writing a Literature Review. \\nMIS Quarterly, 26(2), xiii–xxiii. https://www.jstor.org/ \\nstable/4132319?seq=1#metadata_info_tab_contents \\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., \\nSalakhutdinov, R., & Manning, C. D. (2018). \\nHotpotQA: A Dataset for Diverse, Explainable Multi-\\nhop Question Answering. https://doi.org/10.48550/ \\narXiv.1809.09600 \\nYu, H., Gan, A., Zhang, K., Tong, S., Liu, Q., & Liu, Z. \\n(2024). Evaluation of Retr ieval-Augmented \\nGeneration: A Survey. https://doi.org/10.48550/ \\narXiv.2405.07437 \\nZhang, Z [Zihan], Fang, M., & Chen, L. (2024). \\nRetrievalQA: Assessing Adaptive Retrieval-Augmented \\nGeneration for Short-form Open-Domain Question \\nAnswering. https://doi.org/10.48550/arXiv.2402.16457 \\nZhang, Z [Zihan], Fang, M., Chen, L., Namazi-Rad, M.‑R., \\n& Wang, J [Jun]. (2023). How Do Large Language \\nModels Capture the Ever-changing World Knowledge? \\nA Review of Recent Advances. https://doi.org/10. \\n48550/arXiv.2310.07343      \\n \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n148\"),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented Generation (RAG) has\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents innovative opportunities for the conceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. I NTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, Advanced\\nRAG paradigm focuses on optimizing the retrieval phase,\\naiming to enhance retrieval efficiency and strengthen the\\nutilization of retrieved chunks. As shown in Figure 1 ,typical\\nstrategies involve pre-retrieval processing and post-retrieval\\nprocessing. For instance, query rewriting is used to make\\nthe queries more clear and specific, thereby increasing the\\naccuracy of retrieval [10], and the reranking of retrieval results\\nis employed to enhance the LLM’s ability to identify and\\nutilize key information [11].\\nDespite the improvements in the practicality of Advanced\\nRAG, there remains a gap between its capabilities and real-\\nworld application requirements. On one hand, as RAG tech-\\nnology advances, user expectations rise, demands continue to\\nevolve, and application settings become more complex. For\\ninstance, the integration of heterogeneous data and the new\\ndemands for system transparency, control, and maintainability.\\nOn the other hand, the growth in application demands has\\nfurther propelled the evolution of RAG technology.\\nAs shown in Figure 2, to achieve more accurate and efficient\\ntask execution, modern RAG systems are progressively inte-\\ngrating more sophisticated function, such as organizing more\\nrefined index base in the form of knowledge graphs, integrat-\\ning structured data through query construction methods, and\\nemploying fine-tuning techniques to enable encoders to better\\nadapt to domain-specific documents.\\nIn terms of process design, the current RAG system has\\nsurpassed the traditional linear retrieval-generation paradigm.\\nResearchers use iterative retrieval [12] to obtain richer con-\\ntext, recursive retrieval [13] to handle complex queries, and\\nadaptive retrieval [14] to provide overall autonomy and flex-\\nibility. This flexibility in the process significantly enhances\\narXiv:2407.21059v1  [cs.CL]  26 Jul 2024'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='2\\nFig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\\nquestions, both encounter limitations and struggle to provide satisfactory\\nanswers. Despite the fact that Advanced RAG improves retrieval accuracy\\nthrough hierarchical indexing, pre-retrieval, and post-retrieval processes, these\\nrelevant documents have not been used correctly.\\nthe expressive power and adaptability of RAG systems, en-\\nabling them to better adapt to various application scenarios.\\nHowever, this also makes the orchestration and scheduling of\\nworkflows more complex, posing greater challenges to system\\ndesign. Specifically, RAG currently faces the following new\\nchallenges:\\nComplex data sources integration. RAG are no longer\\nconfined to a single type of unstructured text data source but\\nhave expanded to include various data types, such as semi-\\nstructured data like tables and structured data like knowledge\\ngraphs [15]. Access to heterogeneous data from multiple\\nsources can provide the system with a richer knowledge\\nbackground, and more reliable knowledge verification capa-\\nbilities [16].\\nNew demands for system interpretability, controllability,\\nFig. 2. Case of current Modular RAG.The system integrates diverse data\\nand more functional components. The process is no longer confined to linear\\nbut is controlled by multiple control components for retrieval and generation,\\nmaking the entire system more flexible and complex.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='3\\nand maintainability. With the increasing complexity of sys-\\ntems, system maintenance and debugging have become more\\nchallenging. Additionally, when issues arise, it is essential to\\nquickly pinpoint the specific components that require opti-\\nmization.\\nComponent selection and optimization. More neural net-\\nworks are involved in the RAG system, necessitating the\\nselection of appropriate components to meet the needs of spe-\\ncific tasks and resource configurations. Moreover, additional\\ncomponents enhance the effectiveness of RAG but also bring\\nnew collaborative work requirements [17]. Ensuring that these\\nmodels perform as intended and work efficiently together to\\nenhance the overall system performance is crucial.\\nWorkflow orchestration and scheduling. Components\\nmay need to be executed in a specific order, processed in paral-\\nlel under certain conditions, or even judged by the LLM based\\non different outputs. Reasonable planning of the workflow is\\nessential for improving system efficiency and achieving the\\ndesired outcomes [18].\\nTo address the design, management, and maintenance chal-\\nlenges posed by the increasing complexity of RAG systems,\\nand to meet the ever-growing and diverse demands and ex-\\npectations, this paper proposes Modular RAG architecture.\\nIn modern computing systems, modularization is becoming\\na trend. It can enhance the system’s scalability and maintain-\\nability and achieve efficient task execution through process\\ncontrol.\\nThe Modular RAG system consists of multiple independent\\nyet tightly coordinated modules, each responsible for handling\\nspecific functions or tasks. This architecture is divided into\\nthree levels: the top level focuses on the critical stages of\\nRAG, where each stage is treated as an independent module.\\nThis level not only inherits the main processes from the\\nAdvanced RAG paradigm but also introduces an orchestration\\nmodule to control the coordination of RAG processes. The\\nmiddle level is composed of sub-modules within each module,\\nfurther refining and optimizing the functions. The bottom level\\nconsists of basic units of operation—operators. Within the\\nModular RAG framework, RAG systems can be represented\\nin the form of computational graphs, where nodes represent\\nspecific operators. The comparison of the three paradigms is\\nshown in the Figure 3. Modular RAG evolves based on the\\nprevious development of RAG. The relationships among these\\nthree paradigms are ones of inheritance and development.\\nAdvanced RAG is a special case of Modular RAG, while Naive\\nRAG is a special case of Advanced RAG.\\nThe advantages of Modular RAG are significant, as it\\nenhances the flexibility and scalability of RAG systems. Users\\ncan flexibly combine different modules and operators accord-\\ning to the requirements of data sources and task scenarios. In\\nsummary, the contributions of this paper are as follows:\\n• This paper proposes a new paradigm called modular\\nRAG, which employs a three-tier architectural design\\ncomprising modules, sub-modules, and operators to de-\\nfine the RAG system in a unified and structured manner.\\nThis design not only enhances the system’s flexibility and\\nscalability but also, through the independent design of\\noperators, strengthens the system’s maintainability and\\ncomprehensibility.\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which\\ncan flexibly express current RAG methods. This paper has\\nfurther summarized six typical flow patterns and specific\\nmethods have been analyzed to reveal the universality of\\nmodular RAG in practical scenarios.\\n• The Modular RAG framework offers exceptional flexi-\\nbility and extensibility. This paper delves into the new\\nopportunities brought by Modular RAG and provides a\\nthorough discussion on the adaptation and expansion of\\nnew methods in different application scenarios, offering\\nguidance for future research directions and practical ex-\\nploration.\\nII. R ELATED WORK\\nThe development of RAG technology can be summarized\\nin three stages. Initially, retrieval-augmented techniques were\\nintroduced to improve the performance of pre-trained lan-\\nguage models on knowledge-intensive tasks [19], [20]. In\\nspecific implementations, Retro [21] optimized pre-trained\\nautoregressive models through retrieval augmentation, while\\nAtlas [22] utilized a retrieval-augmented few-shot fine-tuning\\nmethod, enabling language models to adapt to diverse tasks.\\nIRCOT [23] further enriched the reasoning process during\\nthe inference phase by combining chain-of-thought and multi-\\nstep retrieval processes. Entering the second stage, as the\\nlanguage processing capabilities of LLMs significantly im-\\nproved, retrieval-augmented techniques began to serve as a\\nmeans of supplementing additional knowledge and providing\\nreferences, aiming to reduce the hallucination. For instance,\\nRRR [24] improved the rewriting phase, and LLMlingua [25]\\nremoved redundant tokens in retrieved document chunks.\\nWith the continuous progress of RAG technology, research\\nhas become more refined and focused, while also achieving\\ninnovative integration with other technologies such as graph\\nneural networks [26] and fine-tuning techniques [27]. The\\noverall pipeline has also become more flexible, such as using\\nLLMs to proactively determine the timing of retrieval and\\ngeneration [14], [28].\\nThe development of RAG technology has been acceler-\\nated by LLM technology and practical application needs.\\nResearchers are examining and organizing the RAG frame-\\nwork and development pathways from different perspectives.\\nBuilding upon the enhanced stages of RAG, Gao et al., [2] sub-\\ndivided RAG into enhancement during pre-training, inference,\\nand fine-tuning stages. Based on the main processes of RAG,\\nrelevant works on RAG were organized from the perspectives\\nof retrieval, generation, and augmentation methods. Huang\\net al., [29] categorize RAG methods into four main classes:\\npre-retrieval, retrieval, post-retrieval, generation, and provide\\na detailed discussion of the methods and techniques within\\neach class. Hu et al., [30] discuss Retrieval-Augmented Lan-\\nguage Models (RALMs) form three key components, including\\nretrievers, language models, augmentations, and how their\\ninteractions lead to different model structures and applications.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='4\\nFig. 3. Comparison between three RAG paradigms. Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG\\nsystems.\\nThey emphasize the importance of considering robustness,\\naccuracy, and relevance when evaluating RALMs and pro-\\npose several evaluation methods. Ding et al., [31] provide a\\ncomprehensive review from the perspectives of architecture,\\ntraining strategies, and applications. They specifically discuss\\nfour training methods of RALMs: training-free methods, in-\\ndependent training methods, sequence training methods, and\\njoint training methods, and compare their advantages and\\ndisadvantages. Zhao et al., [32]analyze the applications of\\nRAG technology in various fields such as text generation,\\ncode generation, image generation, and video generation from\\nthe perspective of augmented intelligence with generative\\ncapabilities.\\nThe current collation of RAG systems primarily focuses\\non methods with a fixed process, mainly concerned with\\noptimizing the retrieval and generation stages. However, it has\\nnot turned its attention to the new characteristics that RAG\\nresearch is continuously evolving, namely the characteristics\\nof process scheduling and functional componentization. There\\nis currently a lack of comprehensive analysis of the overall\\nRAG system, which has led to research on paradigms lagging\\nbehind the development of RAG technology.\\nIII. F RAMEWORK AND NOTATION\\nFor query Q = {qi}, a typical RAG system mainly consists\\nof three key components. 1) Indexing. Given documents D =\\n{d1, d2, . . . , dn} , where di represents the document chunk.\\nIndexing is the process of converting di into vectors through\\nan embedding model fe(·) , and then store vectors in vector\\ndatabase.\\nI = {e1, e2, . . . , en} and e i = fe(di) ∈ Rd (1)\\nNotation Description\\nq The original query\\ny The output of LLM\\nD A document retrieval repository composed of chunks di.\\nR(q, D) Retriever,find similar chunks from D based on q.\\nF RAG Flow\\nP RAG Flow pattern\\nfqe Query expansion function\\nfqc Query transform function\\nfcomp Chunk compression function\\nfsel Chunk selection function\\nfr Routing function\\nM Module in modular RAG\\nop The specific operators within the Module.\\nTABLE I\\nIMPORTANT NOTATION\\n2) Retrieval . Transform the query into a vector using the\\nsame encoding model, and then filter out the top k document\\nchunks that are most similar based on vector similarity.\\nR : topk\\ndi∈D\\nSim(q, di) → Dq (2)\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.\\nSim(q, di) = eq · edi or eq · edi\\n∥eq∥ · ∥edi∥ (3)\\n3) Generation . After getting the relevant documents. The\\nquery q and the retrieved document Dq chunks are inputted\\ntogether to the LLM to generate the final answer, where [·, ·]\\nstands for concatenation.\\ny = LLM([Dq, q]) (4)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='5\\nWith the evolution of RAG technology, more and more func-\\ntional components are being integrated into systems. Modular\\nRAG paradigm includes three levels, ranging from large to\\nsmall:\\nL1 Module (M = {Ms}). The core process in RAG\\nsystem.\\nL2 Sub-module (Ms = {Op}).The functional modules in\\nmodule.\\nL3 Operator (Op = {fθi}). The the specific functional\\nimplementation in a module or sub-module. As a result, a\\nModular RAG system can be represented as:\\nG = {q, D,M, {Ms}, {Op}} (5)\\nThe arrangement between modules and operators constitutes\\nthe RAG Flow F = ( Mϕ1 , . . . , Mϕn) where ϕ stands for\\nthe set of module parameters. A modular rag flow can be\\ndecomposed into a graph of sub-functions. In the simplest\\ncase,the graph is a linear chain.\\nNaiveRAG : q\\nR(q,D)\\n− − − − − − − − − − − →\\nText−Embedding\\nDq LLM([q,Dq])\\n− − − − − − − − − − − →\\nOpenAI/GPT −4\\ny\\n(6)\\nIV. M ODULE AND OPERATOR\\nThis chapter will specifically introduce modules and op-\\nerators under the Modular RAG framework. Based on the\\ncurrent stage of RAG development, we have established\\nsix main modules: Indexing, Pre-retrieval, Retrieval, Post-\\nretrieval, Generation, and Orchestration.\\nA. Indexing\\nIndexing is the process of split document into manageable\\nchunks and it is a key step in organizing a system. Indexing\\nfaces three main challenges. 1) Incomplete content represen-\\ntation.The semantic information of chunks is influenced by the\\nsegmentation method, resulting in the loss or submergence of\\nimportant information within longer contexts. 2) Inaccurate\\nchunk similarity search . As data volume increases, noise in\\nretrieval grows, leading to frequent matching with erroneous\\ndata, making the retrieval system fragile and unreliable. 3)\\nUnclear reference trajectory. The retrieved chunks may orig-\\ninate from any document, devoid of citation trails, potentially\\nresulting in the presence of chunks from multiple different\\ndocuments that, despite being semantically similar, contain\\ncontent on entirely different topics.\\n1) Chunk Optimization: The size of the chunks and the\\noverlap between the chunks play a crucial role in the overall\\neffectiveness of the RAG system. Given a chunk di, its chunk\\nsize is denoted as Li = |di|, and the overlap is denoted as\\nLo\\ni = |di ∩ di+1|. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise [17].\\nSliding Window using overlapping chunks in a sliding win-\\ndow enhances semantic transitions. However, it has limitations\\nsuch as imprecise context size control, potential truncation of\\nwords or sentences, and lacking semantic considerations.\\nMetadata Attachment. Chunks can be enriched with meta-\\ndata like page number, file name, author, timestamp, sum-\\nmary, or relevant questions. This metadata allows for filtered\\nretrieval, narrowing the search scope.\\nSmall-to-Big [33] separate the chunks used for retrieval\\nfrom those used for synthesis. Smaller chunks enhance re-\\ntrieval accuracy, while larger chunks provide more context.\\nOne approach is to retrieve smaller summarized chunks and\\nreference their parent larger chunks. Alternatively, individual\\nsentences could be retrieved along with their surrounding text.\\n2) Structure Organization: One effective method for en-\\nhancing information retrieval is to establish a hierarchical\\nstructure for the documents. By constructing chunks structure,\\nRAG system can expedite the retrieval and processing of\\npertinent data.\\nHierarchical Index . In the hierarchical structure of docu-\\nments, nodes are arranged in parent-child relationships, with\\nchunks linked to them. Data summaries are stored at each\\nnode, aiding in the swift traversal of data and assisting the\\nRAG system in determining which chunks to extract. This\\napproach can also mitigate the illusion caused by chunk\\nextraction issues. The methods for constructing a structured\\nindex primarily include: 1) Structural awareness based on\\nparagraph and sentence segmentation in docs. 2) Content\\nawareness based on inherent structure in PDF, HTML, and\\nLatex. 3) Semantic awareness based on semantic recognition\\nand segmentation of text.\\nKG Index [34]. Using Knowledge Graphs (KGs) to struc-\\nture documents helps maintain consistency by clarifying con-\\nnections between concepts and entities, reducing the risk of\\nmismatch errors. KGs also transform information retrieval\\ninto instructions intelligible to language models, improving re-\\ntrieval accuracy and enabling contextually coherent responses.\\nThis enhances the overall efficiency of the RAG system.\\nFor example, organizing a corpus in the format of graph\\nG = {V, E, X}, where node V = {vi}n\\ni=1 represent document\\nstructures (e.g.passage, pages, table) , edge E ⊂ V × Vrep-\\nresent semantic or lexical similarity and belonging relations,\\nand node features X = {Xi}n\\ni=1 represent text or markdown\\ncontent for passage.\\nB. Pre-retrieval\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nThe primary challenges in this module include: 1) Poorly\\nworded queries . The question itself is complex, and the\\nlanguage is not well-organized. 2) Language complexity and\\nambiguity. Language models often struggle when dealing\\nwith specialized vocabulary or ambiguous abbreviations with\\nmultiple meanings. For instance, they may not discern whether\\nLLM refers to Large Language Model or a Master of Laws in\\na legal context.\\n1) Query Expansion : Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='6\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nfqe(q) = {q1, q2, . . . , qn} ∀qi ∈ {q1, q2, . . . , qn}, qi /∈ Q\\n(7)\\nMulti-Query uses prompt engineering to expand queries\\nvia LLMs, allowing for parallel execution. These expansions\\nare meticulously designed to ensure diversity and coverage.\\nHowever, this approach can dilute the user’s original intent.\\nTo mitigate this, the model can be instructed to assign greater\\nweight to the original query.\\nSub-Query. By decomposing and planning for complex\\nproblems, multiple sub-problems are generated. Specifically,\\nleast-to-most prompting [35] can be employed to decom-\\npose the complex problem into a series of simpler sub-\\nproblems. Depending on the structure of the original problem,\\nthe generated sub-problems can be executed in parallel or\\nsequentially. Another approach involves the use of the Chain-\\nof-Verification (CoVe) [36]. The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing hallu-\\ncinations.\\n2) Query Transformation: Retrieve and generate based on\\na transformed query instead of the user’s original query.\\nfqt(q) = q′ (8)\\nRewrite. Original queries often fall short for retrieval in\\nreal-world scenarios. To address this, LLMs can be prompted\\nto rewrite. Specialized smaller models can also be employed\\nfor this purpose [24]. The implementation of the query rewrite\\nmethod in Taobao has significantly improved recall effective-\\nness for long-tail queries, leading to an increase in GMV [10].\\nHyDE [37]. In order to bridge the semantic gap between\\nquestions and answers, it constructs hypothetical documents\\n(assumed answers) when responding to queries instead of\\ndirectly searching the query. It focuses on embedding simi-\\nlarity from answer to answer rather than seeking embedding\\nsimilarity for the problem or query. In addition, it also in-\\ncludes reverse HyDE, which generate hypothetical query for\\neach chunks and focuses on retrieval from query to query.\\nStep-back Prompting [38]. The original query is abstracted\\ninto a high-level concept question (step-back question). In the\\nRAG system, both the step-back question and the original\\nquery are used for retrieval, and their results are combined\\nto generate the language model’s answer.\\n3) Query Construction: In addition to text data, an in-\\ncreasing amount of structured data, such as tables and graph\\ndata, is being integrated into RAG systems. To accommodate\\nvarious data types, it is necessary to restructure the user’s\\nquery. This involve converting the query into another query\\nlanguage to access alternative data sources, with common\\nmethods including Text-to-SQL or Text-to-Cypher . In many\\nscenarios, structured query languages (e.g., SQL, Cypher)\\nare often used in conjunction with semantic information and\\nmetadata to construct more complex queries.\\nfqc(q) = q∗, q∗ ∈ Q∗ = {SQL, Cypher, . . .} (9)\\nC. Retrieval\\nThe retrieval process is pivotal in RAG systems. By lever-\\naging powerful embedding models, queries and text can be\\nefficiently represented in latent spaces, which facilitates the\\nestablishment of semantic similarity between questions and\\ndocuments, thereby enhancing retrieval. Three main consider-\\nations that need to be addressed include retrieval efficiency,\\nquality, and the alignment of tasks, data and models.\\n1) Retriever Selection: With the widespread adoption of\\nRAG technology, the development of embedding models has\\nbeen in full swing. In addition to traditional models based\\non statistics and pre-trained models based on the encoder\\nstructure, embedding models fine-tuned on LLMs have also\\ndemonstrated powerful capabilities [39]. However, they often\\ncome with more parameters, leading to weaker inference\\nand retrieval efficiency. Therefore, it is crucial to select the\\nappropriate retriever based on different task scenarios.\\nSparse Retriever uses statistical methods to convert queries\\nand documents into sparse vectors. Its advantage lies in its\\nefficiency in handling large datasets, focusing only on non-zero\\nelements. However, it may be less effective than dense vectors\\nin capturing complex semantics. Common methods include\\nTF-IDF and BM25.\\nDense Retriever employs pre-trained language models\\n(PLMs) to provide dense representations of queries and doc-\\numents. Despite higher computational and storage costs, it\\noffers more complex semantic representations. Typical models\\ninclude BERT structure PLMs, like ColBERT, and multi-task\\nfine-tuned models like BGE [40] and GTE [41].\\nHybrid Retriever is to use both sparse and dense retrievers\\nsimultaneously. Two embedding techniques complement each\\nother to enhance retrieval effectiveness. Sparse retriever can\\nprovide initial screening results. Additionally, sparse models\\nenhance the zero-shot retrieval capabilities of dense models,\\nparticularly in handling queries with rare entities, thereby\\nincreasing system robustness.\\n2) Retriever Fine-tuning: In cases where the context may\\ndiverge from pre-trained corpus, particularly in highly special-\\nized fields like healthcare, law, and other domains abundant in\\nproprietary terminology. While this adjustment demands addi-\\ntional effort, it can substantially enhance retrieval efficiency\\nand domain alignment.\\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval\\nmodel based on labeled domain data is typically done using\\ncontrastive learning. This involves reducing the distance be-\\ntween positive samples while increasing the distance between\\nnegative samples. The commonly used loss calculation is\\nshown in the following:\\nL(DR) = − 1\\nT\\nTX\\ni=1\\nlog e(sim(qi,d+\\ni ))\\ne(sim(qi,d+\\ni )) + PN\\nj=1 e(sim(qi,d−\\ni ))\\n(10)\\nwhere d+\\ni is the positive sample document corresponding to\\nthe i-th query, d−\\ni is several negative sample, T is the total\\nnumber of queries, N is the number of negative samples, and\\nDR is the fine-tuning dataset.\\nLM-supervised Retriever (LSR) . In contrast to directly\\nconstructing a fine-tuning dataset from the dataset, LSR uti-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='7\\nlizes the LM-generated results as supervisory signals to fine-\\ntune the embedding model during the RAG process.\\nPLSR(d|q, y) = ePLM(y|d,q)/β\\nP\\nd′∈D ePLM(y|d,q)/β) (11)\\nPLM (y|d, q) is LM probability of the ground truth output y\\ngiven the input context d and query q, and β is a hyper-\\nparamter.\\nAdapter. At times, fine-tuning a large retriever can be\\ncostly, especially when dealing with retrievers based on LLMs\\nlike gte-Qwen. In such cases, it can mitigate this by incorpo-\\nrating an adapter module and conducting fine-tuning. Another\\nbenefit of adding an adapter is the ability to achieve better\\nalignment with specific downstream tasks [42].\\nD. Post-retrieval\\nFeeding all retrieved chunks directly into the LLM is not an\\noptimal choice. Post-processing the chunks can aid in better\\nleveraging the contextual information. The primary challenges\\ninclude: 1) Lost in the middle . Like humans, LLM tends\\nto remember only the beginning or the end of long texts,\\nwhile forgetting the middle portion [43]. 2) Noise/anti-fact\\nchunks. Retrieved noisy or factually contradictory documents\\ncan impact the final retrieval generation [44]. 3) Context\\nWindow. Despite retrieving a substantial amount of relevant\\ncontent, the limitation on the length of contextual information\\nin large models prevents the inclusion of all this content.\\n1) Rerank: Rerank the retrieved chunks without altering\\ntheir content or length, to enhance the visibility of the more\\ncrucial document chunks. Given the retrieved set Dq and a\\nre-ranking method frerank to obtain the re-ranked set:\\nDq\\nr = frerank(q, Dq) = {d′\\n1, d′\\n2, . . . , d′\\nk}\\nwheref(d′\\n1) ≥ f(d′\\n2) ≥ . . .≥ f(d′\\nk). (12)\\nRule-base rerank. Metrics are calculated to rerank chunks\\naccording to certain rules. Common metrics include: diversity,\\nrelevance and MRR (Maximal Marginal Relevance) [45]. The\\nidea is to reduce redundancy and increase result diversity.\\nMMR selects phrases for the final key phrase list based on a\\ncombined criterion of query relevance and information novelty.\\nModel-base rerank. Utilize a language model to reorder the\\ndocument chunks, commonly based on the relevance between\\nthe chunks and the query. Rerank models have become an\\nimportant component of RAG systems, and relevant model\\ntechnologies are also being iteratively upgraded. The scope\\nreordering has also been extended to multimodal data such as\\ntables and images [46].\\n2) Compression: A common misconception in the RAG\\nprocess is the belief that retrieving as many relevant docu-\\nments as possible and concatenating them to form a lengthy\\nretrieval prompt is beneficial. However, excessive context can\\nintroduce more noise, diminishing the LLM’s perception of\\nkey information. A common approach to address this is to\\ncompress and select the retrieved content.\\nDq\\nc = fcomp(q, Dq), where|dqc\\ni | < |dq\\ni | ∀dq\\ni ∈ Dq (13)\\n(Long)LLMLingua [47]. By utilizing aligned and trained\\nsmall language models, such as GPT-2 Small or LLaMA-\\n7B, the detection and removal of unimportant tokens from\\nthe prompt is achieved, transforming it into a form that is\\nchallenging for humans to comprehend but well understood by\\nLLMs. This approach presents a direct and practical method\\nfor prompt compression, eliminating the need for additional\\ntraining of LLMs while balancing language integrity and\\ncompression ratio.\\n3) Selection: Unlike compressing the content of document\\nchunks, Selection directly removes irrelevant chunks.\\nDq\\ns = fsel(Dq) = {di ∈ D | ¬P(di)} (14)\\nWhere fsel is the function for deletion operation and P(di) is\\na conditional predicate indicating that document ( di) satisfies\\na certain condition. If document ( di) satisfies ( P(di)), it will\\nbe deleted. Conversely, documents for which ( ¬P(di)) is true\\nwill be retained.\\nSelective Context. By identifying and removing redundant\\ncontent in the input context, the input is refined, thus improv-\\ning the language model’s reasoning efficiency. In practice, se-\\nlective context assesses the information content of lexical units\\nbased on the self-information computed by the base language\\nmodel. By retaining content with higher self-information, this\\nmethod offers a more concise and efficient textual representa-\\ntion, without compromising their performance across diverse\\napplications. However, it overlooks the interdependence be-\\ntween compressed content and the alignment between the\\ntargeted language model and the small language model utilized\\nfor prompting compression [48].\\nLLM-Critique. Another straightforward and effective ap-\\nproach involves having the LLM evaluate the retrieved content\\nbefore generating the final answer. This allows the LLM\\nto filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [49], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nE. Generation\\nUtilize the LLM to generate answers based on the user’s\\nquery and the retrieved contextual information. Select an\\nappropriate model based on the task requirements, considering\\nfactors such as the need for fine-tuning, inference efficiency,\\nand privacy protection.\\n1) Generator Fine-tuning: In addition to direct LLM usage,\\ntargeted fine-tuning based on the scenario and data character-\\nistics can yield better results. This is also one of the greatest\\nadvantages of using an on-premise setup LLMs.\\nInstruct-Tuning. When LLMs lack data in a specific do-\\nmain, additional knowledge can be provided to the LLM\\nthrough fine-tuning. General fine-tuning dataset can also be\\nused as an initial step. Another benefit of fine-tuning is the\\nability to adjust the model’s input and output. For example, it\\ncan enable LLM to adapt to specific data formats and generate\\nresponses in a particular style as instructed [50].\\nReinforcement learning. Aligning LLM outputs with hu-\\nman or retriever preferences through reinforcement learning is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='8\\na potential approach [51]. For instance, manually annotating\\nthe final generated answers and then providing feedback\\nthrough reinforcement learning. In addition to aligning with\\nhuman preferences, it is also possible to align with the\\npreferences of fine-tuned models and retrievers.\\nDual Fine-tuing Fine-tuning both generator and retriever\\nsimultaneously to align their preferences. A typical approach,\\nsuch as RA-DIT [27], aligns the scoring functions between\\nretriever and generator using KL divergence. Retrieval likeli-\\nhood of each retrieved document d is calculated as :\\nPR(d|q) = e(sim(d,q))/γP\\nd∈Dq e(sim(d,q)/γ (15)\\nPLM (y|d, q) is the LM probability of the ground truth output y\\ngiven the input context d, question q, and γ is a hyperparamter.\\nThe overall loss is calculated as:\\nL = 1\\n|T|\\nTX\\ni=1\\nKL(PR(d|q)||PLSR(d|q, y|)) (16)\\n2) Verification : Although RAG enhances the reliability\\nof LLM-generated answers, in many scenarios, it requires to\\nminimize the probability of hallucinations. Therefore, it can\\nfilter out responses that do not meet the required standards\\nthrough additional verification module. Common verification\\nmethods include knowledge-base and model-base .\\nyk = fverify (q, Dq, y) (17)\\nKnowledge-base verification refers to directly validating the\\nresponses generated by LLMs through external knowledge.\\nGenerally, it extracts specific statements or triplets from re-\\nsponse first. Then, relevant evidence is retrieved from verified\\nknowledge base such as Wikipedia or specific knowledge\\ngraphs. Finally, each statement is incrementally compared with\\nthe evidence to determine whether the statement is supported,\\nrefuted, or if there is insufficient information [52].\\nModel-based verification refers to using a small language\\nmodel to verify the responses generated by LLMs [53].\\nGiven the input question, the retrieved knowledge, and the\\ngenerated answer, a small language model is trained to de-\\ntermine whether the generated answer correctly reflects the\\nretrieved knowledge. This process is framed as a multiple-\\nchoice question, where the verifier needs to judge whether the\\nanswer reflects correct answer . If the generated answer does\\nnot correctly reflect the retrieved knowledge, the answer can\\nbe iteratively regenerated until the verifier confirms that the\\nanswer is correct.\\nF . Orchestration\\nOrchestration pertains to the control modules that govern the\\nRAG process. Unlike the traditional, rigid approach of a fixed\\nprocess, RAG now incorporates decision-making at pivotal\\njunctures and dynamically selects subsequent steps contingent\\nupon the previous outcomes. This adaptive and modular ca-\\npability is a hallmark of modular RAG, distinguishing it from\\nthe more simplistic Naive and Advance RAG paradigm.\\n1) Routing: In response to diverse queries, the RAG system\\nroutes to specific pipelines tailored for different scenario, a\\nfeature essential for a versatile RAG architecture designed\\nto handle a wide array of situations. A decision-making\\nmechanism is necessary to ascertain which modules will be\\nengaged, based on the input from the model or supplementary\\nmetadata. Different routes are employed for distinct prompts\\nor components. This routing mechanism is executed through\\na function, denoted as fr(·), which assigns a score αi to\\neach module. These scores dictate the selection of the active\\nsubset of modules. Mathematically, the routing function is\\nrepresented as:\\nfr : Q → F (18)\\nwhere fr(·) maps the identified query to its corresponding\\nRAG flow.\\nMetadata routing involves extracting key terms, or entities,\\nfrom the query, applying a filtration process that uses these\\nkeywords and associated metadata within the chunks to refine\\nthe routing parameters. For a specific RAG flow, denoted as\\nFi, the pre-defined routing keywords are represented as the\\nset Ki = {ki1, ki2, . . . , kin}. The keyword identified within\\nthe query qi is designated as K′\\ni. The matching process for\\nthe query q is quantified by the key score equation:\\nscorekey(qi, Fj) = 1\\n|K′\\nj||Ki ∩ K′\\nj| (19)\\nThis equation calculates the overlap between the pre-defined\\nkeywords and those identified in the query, normalized by the\\ncount of keywords in K′\\nj. The final step is to determine the\\nmost relevant flow for the query q:\\nFi(q) = argmaxFj∈Fscore(q, Fj) (20)\\nSemantic routing routes to different modules based on the\\nsemantic information of the query. Given a pre-defined intent\\nΘ = {θ1, θ2, . . . , θn}, the possibility of intent for query q is\\nPΘ(θ|q) = ePLM(θ|q)\\nP\\nθ∈Θ ePLM(θ|q)) . Routing to specific RAG flow is\\ndetermined by the semantic score:\\nsocresemantic(q, Fj) = argmaxθj∈ΘP(Θ) (21)\\nThe function δ(·) serves as a mapping function that assigns\\nan intent to a distinct RAG flow Fi = δ(θi)\\nHybrid Routing can be implemented to improve query\\nrouting by integrating both semantic analysis and metadata-\\nbased approaches, which can be defined as follows:\\nαi = a·scorekey(q, Fj)+(1−α)·maxθj∈Θsocresemantic(q, Fj)\\n(22)\\na is a weighting factor that balances the contribution of the\\nkey-based score and the semantic score.\\n2) Scheduling: The RAG system evolves in complexity\\nand adaptability, with the ability to manage processes through\\na sophisticated scheduling module. The scheduling module\\nplays a crucial role in the modular RAG , identifying critical\\njunctures that require external data retrieval, assessing the\\nadequacy of the responses, and deciding on the necessity for\\nfurther investigation. It is commonly utilized in scenarios that\\ninvolve recursive, iterative, and adaptive retrieval, ensuring'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='9\\nthat the system makes informed decisions on when to cease\\ngeneration or initiate a new retrieval loop.\\nRule judge. The subsequent steps are dictated by a set of\\nestablished rules. Typically, the system evaluates the quality of\\ngenerated answers through scoring mechanisms. The decision\\nto proceed or halt the process is contingent upon whether these\\nscores surpass certain predetermined thresholds, often related\\nto the confidence levels of individual tokens, which can be\\ndefined as follow:\\nyt =\\n(\\nˆst if all tokens of ˆst have probs ≥ τ\\nst = LM([Dqt, x, y<t]) otherwise\\nHere, ˆst represents the tentative answer, and st is the output\\nfrom the language model. The condition for accepting ˆst is that\\nall tokens within it must have associated probabilities greater\\nthan or equal to the threshold τ. If this condition is not met,\\nthe system reverts to generating a new answer.\\nLLM judge. The LLM independently determines the sub-\\nsequent course of action. Two primary approaches facilitate\\nthis capability. The first method leverages LLM ’s in-context\\nlearning capability, and make judgments through prompt\\nengineering. A significant advantage of this method is the\\nelimination of model fine-tuning. Nonetheless, the format of\\nthe judgment output is contingent upon the LLM’s adherence\\nto the provided instructions.\\nThe second approach involves the LLM generating specific\\ntokens that initiate targeted actions through fine-tuning. This\\ntechnique, with roots in the Toolformer [50], has been inte-\\ngrated into frameworks like Self-RAG [28]. This allows for a\\nmore direct control mechanism over the LLM’s actions, en-\\nhancing the system’s responsiveness to specific triggers within\\nthe conversational context. However, it requires generating a\\nlarge number of compliant instruction sets to fine-tune LLM.\\nKnowledge-guide scheduling. Beyond the confines of rule-\\nbased methods and the complete reliance on LLMs for process\\ncontrol, a more adaptable intermediate approach emerges with\\nknowledge-guided scheduling [26]. These methods harness\\nthe power of knowledge graphs, to steer the retrieval and\\ngeneration processes. Specifically, it involves extracting infor-\\nmation relevant to the question from a knowledge graph and\\nconstructing a reasoning chain. This reasoning chain consists\\nof a series of logically interconnected nodes, each containing\\ncritical information for the problem-solving process. Based\\non the information from the nodes in this reasoning chain,\\ninformation retrieval and content generation can be performed\\nseparately. By integrating this approach, it enhance not only\\nthe efficacy and precision of problem-solving but also the\\nclarity of the explanations provided.\\n3) Fusion: As RAG process has evolved beyond a linear\\npipeline, it frequently necessitates broadening the retrieval\\nscope or enhancing diversity by exploring multiple pipelines.\\nConsequently, after the expansion into various branches, the\\nfusion module effectively integrates the information, ensuring\\na comprehensive and coherent response. The fusion module’s\\nreliance is not just for merging answers but also for ensuring\\nthat the final output is both rich in content and reflective of\\nthe multifaceted nature of the inquiry.\\nLLM fusion .One of the most straightforward methods for\\nmulti-branch aggregation is to leverage the powerful capa-\\nbilities of LLMs to analyze and integrate information from\\ndifferent branches. However, this approach also faces some\\nchallenges, particularly when dealing with long answers that\\nexceeds the LLM’s context window limitation. To mitigate this\\nissue, it is common practice to first summarize each branch’s\\nanswer, extracting the key information before inputting it into\\nthe LLM, thus ensuring that the most important content is\\nretained even within length constraints.\\nWeighted ensemble is based on the weighted values of\\ndifferent tokens generated from multiple branches, leading to\\nthe comprehensive selection of the final output. This approach\\ncan be calculated as :\\np(y|q, Dq) =\\nX\\nd∈Dq\\np(y|d, q) · λ(d, q) (23)\\nThe weight λ(d, q) is determined by the similarity score\\nbetween the document d and the input query q. This weight is\\ncalculated using the softmax function, which ensures that the\\nweights are normalized and sum up to one.\\nλ(d, q) = es(d,q)\\nP\\nd∈Dq es(d,q) (24)\\nRRF (Reciprocal Rank Fusion) is an ensemble technique\\nthat synthesizes multiple retrieval result rankings into a co-\\nhesive, unified list [54]. It employs a tailored weighted aver-\\naging approach to enhance collective predictive performance\\nand ranking precision. The method’s strength is its dynamic\\nweight assignment, which is informed by the interplay among\\nbranches. RRF is especially potent in scenarios characterized\\nby model or source heterogeneity, where it can markedly\\namplify the accuracy of predictions.\\nV. RAG F LOW AND FLOW PATTERN\\nThe collaboration between operators forms the workflow\\nof the module, which we refer to as RAG flow F =\\n(Mϕ1 , . . . , Mϕn), where ϕ stands for the set of module param-\\neters. A modular rag flow can be decomposed into a graph of\\nsub-functions. Through control logic, the operators can execute\\nin a predetermined pipeline, while also performing conditional,\\nbranching or looping when necessary. In the simplest case. the\\ngraph is a linear chain.\\nAfter conducting an in-depth analysis of current RAG meth-\\nods, we have identified a set of common RAG flow patterns,\\ndenoted as P. These patterns transcend various application\\ndomains and demonstrate a high level of consistency and\\nreusability, revealing the prevalent structures and behaviors in\\nprocess design. A RAG flow pattern can be defined as P =\\n{Mϕ1 : {Op1} →Mϕ2 : {Op2} →. . .→ Mϕn : {Opn}}\\nA. Linear Pattern\\nThe modules in the modular RAG system are organized in\\na linear way, and can be described as Algorithm 1.\\nPlinear = {M1 → M2 → . . .→ Mn} (25)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='10\\nFig. 4. Linear RAG flow pattern. Each module is processed in a fixed\\nsequential order.\\nFig. 5. RRR [24] is a typical linear flow that introduces a learnable query\\nrewrite module before retrieval. This module employs reinforcement based on\\nthe output results of the LLM.\\nThe linear flow pattern is the simplest and most com-\\nmonly used pattern. As shown in Figure 4, the full linear\\nRAG flow pattern mainly includes pre-retrieval processing,\\nretrieval, post-retrieval processing, and generation modules.\\nPlinearfull = {Mindexing → Mpre-retrieval → Mretrieval →\\nMpost-retrieval → Mgenerate}. If there are no pre-retrieval and\\npost-retrieval modules, it follows the Naive RAG paradigm.\\nAlgorithm 1 Linear RAG Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM, pre-processing function fpre, post-\\nprocessing function fpost\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve documents related to the pre-\\nprocessed query\\n4: ˆDq′\\n← fpost(q′, Dq′\\n) // Post-process the retrieved docu-\\nments\\n5: ˆy ← LLM([q, ˆDq′\\n]) // Generate output using the lan-\\nguage model with the original query and post-processed\\ndocuments\\n6: return ˆy // Return the final output\\nCommon linear RAG flow involves a query transform\\nmodule (such as rewrite or HyDE operators) at the pre-retrieval\\nstage and utilize rerank at the post-retrieval stage. Rewrite-\\nRetrieve-Read (RRR) [24] is a typical linear structure. As\\nillustrated in Figure 5, the query rewrite module frewrite is a\\nsmaller trainable language model fine-tuned on T5-large, and\\nin the context of reinforcement learning, the optimization of\\nthe rewriter is formalized as a Markov decision process, with\\nthe final output of the LLM serving as the reward. The retriever\\nutilizes a sparse encoding model, BM25.\\nB. Conditional Pattern\\nThe RAG flow with conditional structure involves select-\\ning different RAG pipeline based on different conditions,\\nas illustrated in Figure 6. A detailed definition is shown in\\nAlgorithm 2. Typically, pipleline selection is accomplished\\nFig. 6. The conditional flow pattern. There is a routing module that controls\\nwhich RAG flow the query is directed to. Typically, different flows are used for\\nvarious configurations to meet the general requirements of the RAG system.\\nFig. 7. Pre-retrieval branching flow pattern.Each branch performs retrieval\\nand generation separately, and then they are aggregated at the end.\\nthrough a routing module that determines the next module\\nin the flow.\\nPconditional = {Mi\\nfr\\n− →Mj ∨ Mk} (26)\\nWhere\\nfr\\n− →represents that based on routing function fr(·), the\\nflow can go to module Mj or Mk.\\nAlgorithm 2 Conditional RAG Flow Pattern\\nRequire: original query q, documents D, language model\\nLM, retriever R, routing function fr\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← QueryTransform(q) // Pre-process the initial query\\nif needed\\n3: D′ ← R(q′, D) // Retrieve or update documents related\\nto the query\\n4: Mnext ← fr(q′, D′) // Determine the next module using\\nthe routing function\\n5: if Mnext = Mj then\\n6: ˆy ← Mj(q′, D′) // Execute module Mj\\n7: else if Mnext = Mk then\\n8: ˆy ← Mk(q′, D′) Mk\\n9: end if\\n10: return ˆy\\nPipeline selection is determined by the nature of the ques-\\ntion, directing different flows tailored to specific scenarios. For\\nexample, the tolerance for responses generated by LLMs varies\\nacross questions related to serious issues, political matters,\\nor entertainment topics. These routing flow often diverge in\\nterms of retrieval sources, retrieval processes, configurations,\\nmodels, and prompts.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='11\\nFig. 8. Post-retrieval branching flow pattern.Only one retrieval performed, and\\nthen generation is carried out separately for each retrieved document chunks,\\nfollowed by aggregation.\\nC. Branching\\nIn many cases, the RAG flow system may have multiple\\nparallel running branches , usually to increase the diver-\\nsity of generated results. Assuming multiple branches bi are\\ngenerated in module B = Msplit(·) = {b1, b2, . . . , bm}.\\nFor each branch bi ∈ B, the same or different RAG pro-\\ncesses can be executed, passing through multiple processing\\nmodules {M1, M2, . . . , Mk} to obtain branch output result\\npi = Mik(. . . Mi2(Mi1(bi)) . . .). The results of multiple\\nbranches are aggregated using an aggregation function to\\nobtain intermediate output results. ˆO = Mmerge({pi | bi ∈\\nB}). However, aggregation is not necessarily the end of the\\nRAG flow, as it can continue to connect to other modules,\\nMjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating\\nmultiple model responses, they can continue through a val-\\nidation module. Therefore, the entire branch flow pattern can\\nbe represented as:\\nPbranch =Mjn(. . . Mj1(Mmerge({Mik\\n(. . . Mi1(bi) . . .) | bi ∈ Msplit(q)})) . . .) (27)\\nAlgorithm 3 Pre-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, query expand mod-\\nule Mexpand, retriever Mretrieve, language model LLM,\\nmerge module Mmerge\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q′ ← Mexpand(q) // Expand the original query to multiple\\nsub-queries\\n3: for all q′\\ni ∈ Q′ do\\n4: D′\\ni ← Mretrieve(q′\\ni, D) // Retrieve documents for each\\nsub-query\\n5: Gi ← ∅// Initialize an empty set for generated results\\nof the sub-query\\n6: for all d′\\nij ∈ D′\\ni do\\n7: yij ← LLM([q′\\ni, d′\\nij]) // Generate results for each\\ndocument of the sub-query\\n8: Oi ← Oi ∪ {yij} // Add generated results to the set\\n9: end for\\n10: ˆy ← Mmerge(Oi) // Merge generated results of the sub-\\nquery into the final result\\n11: end for\\n12: return ˆy\\nThe RAG flow with a branching structure differs from\\nthe conditional approach in that it involves multiple parallel\\nbranches, as opposed to selecting one branch from multiple\\noptions in the conditional approach. Structurally, it can be\\ncategorized into two types, which are depicted in Figure 7\\nand Figure 8.\\nPre-Retrieval Branching (Multi-Query, Parallel Retrieval).\\nAs shown in Algorithm 3, the process involves initially taking\\na query q and expanding it through a module Mexpand to gen-\\nerate multiple sub-queries Q′. Each sub-query q′\\ni is then used\\nto retrieve relevant documents via Mretrieve, forming document\\nsets D′\\ni. These document sets, along with the corresponding\\nsub-queries, are fed into a generation module Mgenerate to\\nproduce a set of answers Gi. Ultimately, all these generated\\nanswers are combined using a merging module Mmerge to\\nform the final result y. This entire flow can be mathematically\\nrepresented as:\\nPbranchpre =Mmerge(q′\\ni∈Mexpand(q){Mgenerate(q′\\ni, d′\\nij) |\\nd′\\nij ∈ Mretrieve(q′\\ni)}) (28)\\nPost-Retrieval Branching (Single Query, Parallel Genera-\\ntion). As shown in Algorithm 4, in the post-retrieval branching\\npattern, the process starts with a single query q which is\\nused to retrieve multiple document chunks through a retrieval\\nmodule Mretrieve, resulting in a set of documents Dq. Each\\ndocument dq\\ni from this set is then independently processed by\\na generation module Mgenerate to produce a set of generated\\nresults G. These results are subsequently merged using a\\nmerge module Mmerge to form the final result y. The process\\ncan be succinctly represented as y = Mmerge(Oi), where Oi is\\nthe collection of all generated results from each document dq\\ni\\nin Dq. Therefore, the entire process can be represented as:\\nPbranchpost = Mmerge({Mgenerate(dq\\ni ) | dq\\ni ∈ Mretrieve(q)})\\n(29)\\nAlgorithm 4 Post-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM, merge module Mmerge\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve a set of documents based on\\nthe pre-processed query\\n4: G ← ∅// Initialize an empty set to store generated results\\n5: for all di ∈ Dq′\\ndo\\n6: yi ← LLM([q, di]) // Generate results independently\\nfor each document chunk using the language model\\n7: Oi ← Oi ∪ {yi} // Add the generated result to the set\\nof results\\n8: end for\\n9: ˆy ← Mmerge(Oi) // Merge all generated results using the\\nmerge function\\n10: return ˆy\\nREPLUG [55] embodies a classic post-retrieval branching\\nstructure, wherein the probability of each token is predicted\\nfor each branch. Through weighted possibility ensemble, the\\ndifferent branches are aggregated, and the final generation'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='12\\nFig. 9. The RAG flow in REPLUG [55], which follows a typical post-retrieval\\nbranching pattern. Each retrieved chunks undergoes parallel generation, and\\nthen they are aggregated using a weighted probability ensemble.\\nresult is used to fine-tune the retriever, known as Contriever,\\nthrough feedback.\\nD. Loop Pattern\\nThe RAG flow with a loop structure, as an important char-\\nacteristic of Modular RAG, involves interdependent retrieval\\nand generation steps. It typically includes a scheduling module\\nfor flow control. The modular RAG system can be abstracted\\nas a directed graph G = (V, E), where V is the set of vertices\\nrepresenting the various modules Mi in the system, and E is\\nthe set of edges representing the control flow or data flow be-\\ntween modules. If there is a vertex sequence Mi1 , Mi2 , ..., Min\\nsuch that Min can reach Mi1 (i.e., Min → Mi1 ), then this\\nRAG system forms a loop. If Mj is the successor module of\\nMi and Mi decides whether to return to Mj or a previous\\nmodule Mk through a Judge module, it can be represented\\nas: Mi\\nJudge\\n− − − →Mj or Mi\\nJudge\\n− − − →Mk where Mk is the\\npredecessor module of Mj. If Mi return to Mj, it can be\\nrepresented as: ∃Judge(Mi, Mj) s.t. (Mi, Mj) ∈ E and\\nJudge(Mi, Mj) = true. If the Judge module not to return\\nto any previous module, it can be represented as: ∀Mi ∈\\nV, Judge(Mi, Mj) = false for all Mj that are predecessors\\nof Mi. Loop pattern can be further categorized into iterative,\\nrecursive, and adaptive (active) retrieval approaches.\\nIterative retrieval At times, a single retrieval and genera-\\ntion may not effectively address complex questions requiring\\nextensive knowledge. Therefore, an iterative approach can be\\nused in RAG (see Algorithm 5), typically involving a fixed\\nnumber of iterations for retrieval. At step t, given the query\\nqt and the previous output sequence y<t = [ y0, . . . , yt−1] ,\\niterations proceed under the condition that t is less than the\\nmaximum allowed iterations T. In each loop, it retrieves a\\ndocument chunks Dt−1 using the last output yt−1 and the\\ncurrent query qt. Subsequently, a new output yt is generated.\\nThe continuation of the iteration is determined by a Judge\\nmodule, which makes its decision based on the yt, y<t, qt,\\nand the Dt−1.\\nAn exemplary case of iterative retrieval is ITER-\\nRETGEN [56] (Figure 11), which iterates retrieval-augmented\\ngeneration and generation-augmented retrieval. Retrieval-\\naugmented generation outputs a response to a task input based\\non all retrieved knowledge. In each iteration, ITER-RETGEN\\nleverages the model output from the previous iteration as a\\nspecific context to help retrieve more relevant knowledge.\\nFig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\\nof retrieval and generation. It can be categorized into three forms: iterative,\\nrecursive, and adaptive.\\nAlgorithm 5 Iterative RAG Flow Pattern\\nRequire: original query q, documents D, maximum iterative\\ntimes T, language model LLM, retriever R, initial output\\ny<1 = ∅\\nEnsure: final output ˆy\\n1: Initialize:\\n2: qt ← q // Initialize query for the first iteration\\n3: y<1 ← ∅// Initialize previous outputs as empty\\n4: t ← 1 // Initialize iteration step\\n5: while t ≤ T do\\n6: qt ← QueryTransform(y<t−1, qt−1) // Generate query\\nbased on previous output and original query\\n7: Dt ← R(yt−1||qt, D) // Retrieve or update documents\\nrelated to the current query\\n8: yt ← LLM([y<t−1, qt, Dt]) // Generate output using\\nthe language model\\n9: y<t ← [y<t−1, yt] // Update the list of previous outputs\\n10: if Judge(yt, q) = false then\\n11: break\\n12: end if\\n13: t ← t + 1 // Increment iteration step\\n14: end while\\n15: yfinal = synthesizeOutput(y≤t) // Synthesize final output\\nfrom the list of outputs\\n16: return ˆy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='13\\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\\nof retrieval and generation are performed within the limit of the maximum\\nnumber of iterations.\\nTermination of the loop is determined by a predefined number\\nof iterations.\\nRecursive retrieval The characteristic feature of recursive\\nretrieval (see Algorithm 6), as opposed to iterative retrieval, is\\nits clear dependency on the previous step and its continuous\\ndeepening of retrieval. Typically, it follows a tree-like structure\\nand there is a clear termination mechanism as an exit condition\\nfor recursive retrieval. In RAG systems, recursive retrieval usu-\\nally involves query transform, relying on the newly rewritten\\nquery for each retrieval.\\nAlgorithm 6 Recursive RAG Flow Pattern\\nRequire: initial query q, document D, retriever R, language\\nmodel LM, maximum recursive depth Kmax\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q ← {q}\\n3: k ← 0 // Initialize recursion depth\\n4: while Q ̸= ∅ and k < Kmax do\\n5: Q′ ← ∅// To store queries for the next recursion level\\n6: for all q ∈ Q do\\n7: Dq ← R(q, D) // Retrieve or update documents\\nrelated to the current query\\n8: Y ← LM([q, Dq]) // Generate outputs using the\\nlanguage model\\n9: Q′′ ← deriveNewQueries(q, Dq, Y) // Derive new\\nqueries from generated outputs\\n10: for all q′ ∈ Q′′ do\\n11: if q′ /∈ Q′ and q′ /∈ Q then\\n12: Q′ ← Q′ ∪ {q′}\\n13: end if\\n14: end for\\n15: end for\\n16: Q ← Q′ // Update the set of queries for the next\\nrecursion\\n17: k ← k + 1 // Increment recursion depth\\n18: end while\\n19: ˆy = synthesizeOutput(Y ) // Synthesize final output from\\ngenerated outputs\\n20: return ˆy\\nA typical implementation of recursive retrieval, such as\\nToC [13] (see Figure 12 ), involves recursively executing RAC\\n(Recursive Augmented Clarification) to gradually insert sub-\\nnodes into the clarification tree from the initial ambiguous\\nquestion (AQ). At each expansion step, paragraph re-ranking\\nis performed based on the current query to generate a disam-\\nFig. 12. RAG flow of ToC [13]. A typical characteristic of this process is\\nthat each recursive retrieval uses the new query generated from the previous\\nstep, thereby progressively deepening analysis of the original complex query.\\nbiguous Question (DQ). The exploration of the tree concludes\\nupon reaching the maximum number of valid nodes or the\\nmaximum depth. Once the clarification tree is constructed,\\nToC gathers all valid nodes and generates a comprehensive\\nlong-text answer to address AQ.\\nAdaptive (Active) retrieval With the advancement of RAG,\\nthere has been a gradual shift beyond passive retrieval to the\\nemergence of adaptive retrieval (see Algorithm 7) , also known\\nas active retrieval, which is partly attributed to the powerful\\ncapabilities of LLM. This shares a core concept with LLM\\nAgent [57]. RAG systems can actively determine the timing\\nof retrieval and decide when to conclude the entire process and\\nproduce the final result. Based on the criteria for judgment,\\nthis can be further categorized into Prompt-base and Tuning-\\nbase approaches.\\nAlgorithm 7 Active RAG Flow Pattern\\nRequire: original query Q, documents D, maximum iterative\\ntimes T, language model LLM, retriever R\\nEnsure: final output ˆy\\n1: Initialize:\\n2: t ← 1 // Initialize loop step\\n3: qt ← q // Initialize query for the first iteration\\n4: y<1 ← ∅// Initialize previous outputs as empty\\n5: while t ≤ T do\\n6: Qt ← QueryTransform(y<t−1, qt−1) // Derive new\\nquery from previous output and query\\n7: if Evaluate(Qt, y<t−1) then\\n8: Dt ← R(qt, D) // Retrieve documents based on the\\nnew query\\n9: yt ← LLM([qt, Dt]) // Generate output using the\\nlanguage model\\n10: else\\n11: yt ← ∅// Set output as empty if query evaluation is\\nfalse\\n12: end if\\n13: y<t ← [y<t−1, yt] // Update the list of previous outputs\\n14: if isOutputAcceptable(yt, y<t, qt) = false then\\n15: break // Break if the output is not acceptable\\n16: end if\\n17: t ← t + 1 // Increment iteration step\\n18: end while\\n19: ˆy = synthesizeOutput(y≤t) // Synthesize final output from\\nthe list of outputs\\n20: return ˆy\\nPrompt-base. The prompt-base approach involves control-\\nling the flow using Prompt Engineering to direct LLM. A'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='14\\nFig. 13. RAG flow of FLARE [14]. The generated provisional answer will\\nundergo confidence assessment. If it does not meet the required confidence\\nlevel, the process will return to the retrieval stage and generate anew. The\\nassessment criteria are implemented through prompt\\nFig. 14. RAG flow of SELF-RAG [28]. First, it prompt GPT-4 to obtain\\na suitable instruct fine-tuning dataset to fine-tune the deployed open-source\\nLLM. This allows the model to output four specific tokens during generation,\\nwhich are used to control the RAG process.\\ntypical implementation example is FLARE [14]. Its core\\nconcept is that LLMs should only retrieve when essential\\nknowledge is lacking, to avoid unnecessary or inappropriate\\nretrieval in an enhanced LM. FLARE iteratively generates the\\nnext provisional sentence and checks for the presence of low-\\nprobability tokens. If found, the system retrieves relevant docu-\\nments and regenerates the sentence. Tuning-base. The tuning-\\nbased approach involves fine-tuning LLM to generate special\\ntokens, thereby triggering retrieval or generation. This concept\\ncan be traced back to Toolformer [50], where the generation of\\nspecific content assists in invoking tools. In RAG systems, this\\napproach is used to control both retrieval and generation steps.\\nA typical case is Self-RAG [28](see Figure 14). Given an\\ninput prompt and the preceding generation result, first predict\\nwhether the special token Retrieve is helpful for enhancing\\nthe continued generation through retrieval. Then, if retrieval\\nis needed, the model generates a critique token to evaluate the\\nretrieved passage’s relevance. and a critique token to evaluate\\nif the information in the response is supported by the retrieved\\npassage. Finally, a critique token evaluates the overall utility of\\nthe response and selects the optimal result as the final output.\\nE. Tuning Pattern\\nRAG is continuously integrating with more LLM-related\\ntechnologies. In Modular RAG, many components are com-\\nposed of trainable language models. Through fine-tuning, the\\nperformance of the components and the compatibility with\\nthe overall flow can be further optimized. This section will\\nintroduce three main patterns of fine-tuning stages, namely\\nretriever fine-tuning, generator fine-tuning, and dual fine-\\ntuning.\\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.\\n1) Retriever FT: In the RAG flow, common methods for\\nfine-tuning the retriever is shown in Figure 15 ,which include:\\n• Direct supervised fine-tuning of the retriever. Construct-\\ning a specialized dataset for retrieval and fine-tuning the\\ndense retriever. For example, using open-source retrieval\\ndatasets or constructing one based on domain-specific\\ndata.\\n• Adding trainable adapter modules. Sometimes, direct\\nfine-tuning of the API-base embedding model (e.g., Ope-\\nnAI Ada-002 and Cohere) is not feasible. Incorporating\\nan adapter module can enhance the representation of\\nyour data. Additionally, the adapter module facilitates\\nbetter alignment with downstream tasks, whether for task-\\nspecific (e.g., PRCA [42]) or general purposes (e.g.,\\nAAR [58]).\\n• LM-supervised Retrieval (LSR). Fine-tuning the retriever\\nbased on the results generated by LLM.\\n• LLM Reward RL. Still using the LLM output results as\\nthe supervisory signal. Employing reinforcement learning\\nto align the retriever with the generator. The whole re-\\ntrieval process is disassembled in the form of a generative\\nMarkov chain.\\n2) Generator FT: The primary methods for fine-tuning a\\ngenerator in RAG flow is shown in Figure 16, which include:\\n• Direct supervised fine-tuning . Fine-tuning through an\\nexternal dataset can supplement the generator with ad-\\nditional knowledge. Another benefit is the ability to\\ncustomize input and output formats. By setting the Q&A\\nformat, LLM can understand specific data formats and\\noutput according to instructions.\\n• Distillation. When using on-premise deployment of open-\\nsource models, a simple and effective Optimization\\nmethod is to use GPT-4 to batch construct fine-tuning\\ndata to enhance the capabilities of the open-source model.\\n• RL from LLM/human feedback. Reinforcement learning\\nbased on feedback from the final generated answers. In\\naddition to using human evaluations, powerful LLMs can\\nalso serve as an evaluative judge.\\n3) Dual FT: In the RAG system, fine-tuning both the\\nretriever and the generator simultaneously is a unique feature\\nof the RAG system. It is important to note that the emphasis\\nof system fine-tuning is on the coordination between the\\nretriever and the generator. An exemplary implementation is\\nRA-DIT [27], which fine-tunes both the LLM and the retriever.\\nThe LM-ft component updates the LLM to maximize the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='15\\nFig. 16. Generator fine-tuning pattern, The main methods include SFT,\\ndistillation and RL from LLM/human feedback.\\nFig. 17. Dual fine-tuning pattern. In this mode, both the retriever and\\ngenerator participate in fine-tuning, and their preferences will be aligned.\\nlikelihood of the correct answer given the retrieval-augmented\\ninstructions while the R-ft component updates the retriever\\nto minimize the KL-Divergence between the retriever score\\ndistribution and the LLM preference.\\nVI. D ISCUSSION\\nIn this chapter, we explore the innovative horizons opened\\nby the modular RAG paradigm. We examine its compatibility\\nwith cutting-edge methodologies in the progression of RAG\\ntechnology, emphasizing its scalability. It not only fosters a\\nfertile ground for model innovation but also paves the way for\\nseamless adaptation to the dynamic requirements of various\\napplications.\\nA. Opportunities in Modular RAG\\nThe benefits of Modular RAG are evident, providing a\\nfresh and comprehensive perspective on existing RAG-related\\nwork. Through modular organization, relevant technologies\\nand methods are clearly summarized.\\nFrom a research perspective. Modular RAG is highly\\nscalable, it empowers researchers to introduce innovative mod-\\nules and operators, leveraging a deep understanding of RAG’s\\nevolving landscape. This flexibility enables the exploration of\\nnew theoretical and practical dimensions in the field.\\nFrom an application perspective . The modularity of RAG\\nsystems simplifies their design and implementation. Users can\\ntailor RAG flows to fit their specific data, use cases, and\\ndownstream tasks, enhancing the adaptability of the system\\nto diverse requirements. Developers can draw from existing\\nflow architectures and innovate by defining new flows and\\npatterns that are tailored to various application contexts and\\ndomains. This approach not only streamlines the development\\nprocess but also enriches the functionality and versatility of\\nRAG applications.\\nB. Compatibility with new methods\\nModular RAG paradigm demonstrates exceptional compati-\\nbility with new developments. To gain a deeper understanding\\nof this, we list three typical scalability cases, which clearly\\nshows that Modular RAG paradigm provides robust support\\nand flexibility for the innovation and development of RAG\\ntechnology.\\n1) Recombination of the current modules: In this scenario,\\nno new modules or operators are proposed; rather, specific\\nproblems are addressed through the combination of existing\\nmodules.DR-RAG [59] employs a two-stage retrieval strategy\\nand classifier selection mechanism, incorporating a branching\\nretrieval structure. In the first stage, retrieving chunks relevant\\nto the query. In the second stage, the query is combined\\nindividually with each chunk retrieved in the first stage, and a\\nparallel secondary retrieval is conducted. The retrieved content\\nis then input into a classifier to filter out the most relevant\\ndynamic documents. This ensures that the retrieved documents\\nare highly relevant to the query while reducing redundant\\ninformation. DR-RAG improved retrieval method significantly\\nenhances the accuracy and efficiency of answers, bolstering\\nRAG’s performance in multi-hop question-answering scenar-\\nios.\\n2) New flow without adding new operators.: This refers\\nto redesigning the processes for retrieval and generation to\\naddress more complex scenarios without proposing new mod-\\nules. The core idea of PlanRAG [18] lies in its introduction of\\na preliminary planning stage, a crucial step that occurs before\\nretrieval and generation. Initially, the system employs a judge\\nmodule to assess whether the current context necessitates the\\nformulation of a new plan or adjustments to an existing one.\\nWhen encountering a problem for the first time, the system\\ninitiates the planning process, while in subsequent interactions,\\nit decides whether to execute re-planning based on previous\\nplans and retrieved data.\\nNext, the system devises an execution plan tailored to the\\nquery, treating this process as a logical decomposition of\\ncomplex queries. Specifically, PlanRAG uses a query expan-\\nsion module to extend and refine the query. For each derived\\nsub-query, the system conducts targeted retrieval. Following\\nretrieval, another judge module evaluates the current results to\\ndecide whether further retrieval is required or if it should return\\nto the planning stage for re-planning. Through this strategy,\\nPlanRAG is able to handle complex decision-making problems\\nthat require multi-step data analysis more efficiently.\\n3) New flow derived from new operators.: New operators\\noften introduce novel flow design, exemplified by Multi-Head\\nRAG [60]. Existing RAG solutions do not focus on queries that\\nmay require retrieving multiple documents with significantly\\ndifferent content. Such queries are common but difficult to\\nhandle because embeddings of these documents may be far\\napart in the embedding space. Multi-Head RAG addresses this\\nby designing a new retriever that uses the activations of the\\nmulti-head attention layers of the Transformer, rather than the\\ndecoder layers, as keys for retrieving multifaceted documents.\\nDifferent attention heads can learn to capture different aspects\\nof the data. By using the corresponding activation results,\\nembeddings that represent different aspects of the data items\\nand the query can be generated, thereby enhancing the retrieval\\naccuracy for complex queries.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='16\\nVII. C ONCLUSION\\nRAG is emerging as a pivotal technology for LLM applica-\\ntions. As technological landscapes evolve and the intricacies of\\napplication requirements escalate, RAG systems are being en-\\nhanced by integrating a diverse suite of technologies, thereby\\nachieving a higher level of complexity and functionality. This\\npaper introduces the innovative paradigm of Modular RAG.\\nThis approach systematically disassembles the complex archi-\\ntecture of RAG systems into well-defined, discrete functional\\nmodules. Each module is meticulously characterized by its\\nspecific operational functions, ensuring clarity and precision.\\nTherefore, the entire system is composed of those modules\\nand operators, akin to Lego bricks. By conducting an in-\\ndepth analysis of numerous studies, the paper also distills\\ncommon RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for\\nthe design and application of RAG systems but also en-\\nables a scenario-based customization of these systems. The\\nmodularity inherent in this design facilitates ease of tracking\\nand debugging, significantly enhancing the maintainability and\\nscalability of RAG systems. Furthermore, Modular RAG opens\\nup new avenues for the future progression of RAG technology.\\nIt encourages the innovation of novel functional modules and\\nthe crafting of innovative workflows, thereby driving forward\\nthe frontiers of RAG systems.\\nREFERENCES\\n[1] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[2] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and\\nH. Wang, “Retrieval-augmented generation for large language models:\\nA survey,” arXiv preprint arXiv:2312.10997 , 2023.\\n[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\\nand Z. Li, “Retrieval-augmented generation with knowledge graphs\\nfor customer service question answering,” in Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2024, pp. 2905–2909.\\n[4] C. Zhang, S. Wu, H. Zhang, T. Xu, Y . Gao, Y . Hu, and E. Chen,\\n“Notellm: A retrievable large language model for note recommendation,”\\nin Companion Proceedings of the ACM on Web Conference 2024 , 2024,\\npp. 170–179.\\n[5] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[6] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[7] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[8] D. S. Asudani, N. K. Nagwani, and P. Singh, “Impact of word embedding\\nmodels on text analytics in deep learning environment: a review,”\\nArtificial intelligence review, vol. 56, no. 9, pp. 10 345–10 425, 2023.\\n[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[10] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\\n“Large language model based long-tail query rewriting in taobao search,”\\narXiv preprint arXiv:2311.03758 , 2023.\\n[11] Y . Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\\nY . Yu, “A bird’s-eye view of reranking: from list level to page level,”\\nin Proceedings of the Sixteenth ACM International Conference on Web\\nSearch and Data Mining , 2023, pp. 1075–1083.\\n[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.\\n[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,” arXiv\\npreprint arXiv:2305.06983, 2023.\\n[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\\nand J. Larson, “From local to global: A graph rag approach to query-\\nfocused summarization,” arXiv preprint arXiv:2404.16130 , 2024.\\n[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi, Z. Wang,\\nS. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\\ngeneration,” arXiv preprint arXiv:2407.01219 , 2024.\\n[18] M. Lee, S. An, and M.-S. Kim, “Planrag: A plan-then-retrieval aug-\\nmented generation for generative large language models as decision\\nmakers,” arXiv preprint arXiv:2406.12430 , 2024.\\n[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158 , 2023.\\n[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems , vol. 33, pp. 9459–9474, 2020.\\n[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\\n“Improving language models by retrieving from trillions of tokens,” in\\nInternational conference on machine learning. PMLR, 2022, pp. 2206–\\n2240.\\n[22] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509 , 2022.\\n[24] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” in Proceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track and\\nGovernment Track), J. Campbell, S. Larocca, J. Marciano, K. Savenkov,\\nand A. Yanishevsky, Eds. Orlando, USA: Association for Machine\\nTranslation in the Americas, Sep. 2022, pp. 202–209. [Online].\\nAvailable: https://aclanthology.org/2022.amta-upg.14\\n[26] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faith-\\nful and interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez,\\nJ. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\\ninstruction tuning,” arXiv preprint arXiv:2310.01352 , 2023.\\n[28] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning\\nto retrieve, generate, and critique through self-reflection,” arXiv preprint\\narXiv:2310.11511, 2023.\\n[29] Y . Huang and J. Huang, “A survey on retrieval-augmented text gen-\\neration for large language models,” arXiv preprint arXiv:2404.10981 ,\\n2024.\\n[30] Y . Hu and Y . Lu, “Rag and rau: A survey on retrieval-augmented\\nlanguage model in natural language processing,” arXiv preprint\\narXiv:2404.19543, 2024.\\n[31] Y . Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and\\nQ. Li, “A survey on rag meets llms: Towards retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2405.06211 , 2024.\\n[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,\\nand B. Cui, “Retrieval-augmented generation for ai-generated content:\\nA survey,” arXiv preprint arXiv:2402.19473 , 2024.\\n[33] S. Yang, “Advanced rag 01: Small-to-\\nbig retrieval,” https://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='17\\n[34] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730 , 2023.\\n[35] D. Zhou, N. Sch ¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al. , “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[37] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\\n[38] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.\\n[39] H. Cao, “Recent advances in text embedding: A comprehensive review\\nof top-performing methods on the mteb benchmark,” arXiv preprint\\narXiv:2406.01607, 2024.\\n[40] BAAI, “Flagembedding,” https://github.com/FlagOpen/FlagEmbedding,\\n2023.\\n[41] Z. Li, X. Zhang, Y . Zhang, D. Long, P. Xie, and M. Zhang, “Towards\\ngeneral text embeddings with multi-stage contrastive learning,” arXiv\\npreprint arXiv:2308.03281, 2023.\\n[42] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\\n“Prca: Fitting black-box large language models for retrieval question an-\\nswering via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172 , 2023.\\n[44] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[45] L. Xia, J. Xu, Y . Lan, J. Guo, and X. Cheng, “Learning maximal\\nmarginal relevance model via directly optimizing diversity evaluation\\nmeasures,” in Proceedings of the 38th international ACM SIGIR con-\\nference on research and development in information retrieval , 2015, pp.\\n113–122.\\n[46] Cohere, “Say goodbye to irrelevant search results: Cohere rerank is\\nhere,” https://txt.cohere.com/rerank/, 2023.\\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context sce-\\nnarios via prompt compression,” arXiv preprint arXiv:2310.06839, 2023.\\n[48] R. Litman, O. Anschel, S. Tsiper, R. Litman, S. Mazor, and R. Man-\\nmatha, “Scatter: selective context attentional scene text recognizer,” in\\nproceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, 2020, pp. 11 962–11 972.\\n[49] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092 , 2023.\\n[50] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in neural\\ninformation processing systems , vol. 35, pp. 27 730–27 744, 2022.\\n[52] S. J. Semnani, V . Z. Yao, H. C. Zhang, and M. S. Lam, “Wikichat:\\nStopping the hallucination of large language model chatbots by few-\\nshot grounding on wikipedia,” arXiv preprint arXiv:2305.14292 , 2023.\\n[53] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[54] G. V . Cormack, C. L. Clarke, and S. Buettcher, “Reciprocal rank\\nfusion outperforms condorcet and individual rank learning methods,”\\nin Proceedings of the 32nd international ACM SIGIR conference on\\nResearch and development in information retrieval , 2009, pp. 758–759.\\n[55] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box language\\nmodels,” arXiv preprint arXiv:2301.12652 , 2023.\\n[56] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[57] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou et al., “Metagpt: Meta programming for\\nmulti-agent collaborative framework,” arXiv preprint arXiv:2308.00352,\\n2023.\\n[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\\n“Dr-rag: Applying dynamic document relevance to retrieval-augmented\\ngeneration for question-answering,” arXiv preprint arXiv:2406.07348 ,\\n2024.\\n[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\\ndorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M ¨uller et al. , “Multi-\\nhead rag: Solving multi-aspect problems with llms,” arXiv preprint\\narXiv:2406.05085, 2024.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6992477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs=text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    if split_docs:\n",
    "        print(f\"Example chunk:\")\n",
    "        print(f\"{split_docs[0].page_content[200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094fa1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 63 documents into 320 chunks\n",
      "Example chunk:\n",
      "l...\n",
      "Metadata: {'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Finetune-RAG: Fine-Tuning Language Models to\\nResist Hallucination in Retrieval-Augmented\\nGeneration\\nZhan Peng Lee\\nPints AI Labs\\nzhanpeng.lee@pints.co\\nAndre Lin∗\\nPints AI Labs\\nandre_lin@u.nus.edu\\nandrelim444@gmail.com\\nCalvin Tan\\nPints AI Labs\\ncalvin@pints.co\\nAbstract\\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to\\nimprove factuality in large language models (LLMs) by grounding their outputs in\\nretrieved documents. However, ensuring perfect retrieval of relevant information\\nremains challenging, and when irrelevant content is passed downstream to an LLM,\\nit can lead to hallucinations. In this work, we propose Finetune-RAG, a simple\\nand effective fine-tuning approach that features the first-of-its-kind RAG training\\ndataset constructed to mimic real-world imperfections. Experimental results show\\nthat Finetune-RAG improves factual accuracy by 21.2% over the base model. We\\nalso propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='that Finetune-RAG improves factual accuracy by 21.2% over the base model. We\\nalso propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests\\nmodels under realistic imperfect retrieval scenarios. Our codebase2 and dataset3\\nare fully open sourced for community use.\\n1 Introduction\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of\\nnatural language processing tasks (Wang et al., 2023; Rozière et al., 2024; Cui et al., 2025; Yasunaga\\net al., 2022; Liu et al., 2024). However, their tendency to \"hallucinate\", that is, to produce fluent\\nbut factually incorrect information, remains a persistent challenge (Li et al., 2024a; Duan et al.,\\n2024; Zhang et al., 2023), particularly in high-stakes domains such as healthcare, law, and finance\\n(Agarwal et al., 2024; Dahl et al., 2024; Kang and Liu, 2023). To address this,Retrieval-Augmented\\nGeneration (RAG)has become a popular solution. Instead of relying solely on parametric memory,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='(Agarwal et al., 2024; Dahl et al., 2024; Kang and Liu, 2023). To address this,Retrieval-Augmented\\nGeneration (RAG)has become a popular solution. Instead of relying solely on parametric memory,\\nRAG systems retrieve external documents and condition the model’s response on this evidence.\\nIn practice, retrieval accuracy in RAG is far from flawless. Retrieved documents may be outdated,\\nmisleading, or topically adjacent but factually incorrect. These errors can propagate downstream,\\nleading models to blend inaccurate context into fluent but false answers. This is especially concerning\\nin domains such as law, compliance, financial reporting, or medicine, where mistakes can have\\nwide-ranging repercussions.\\nMost prior work has addressed this issue from the retrieval perspective, focusing on improving\\nretrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024; Dong et al.,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Most prior work has addressed this issue from the retrieval perspective, focusing on improving\\nretrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024; Dong et al.,\\n2024; Zhou and Chen, 2025). In contrast, relatively little attention has been given to improving the\\nmodel’s ability to resist using the incorrect information.\\nIn this paper, we introduceFinetune-RAG, a method that directly targets hallucination by fine-tuning\\nthe model with imperfect RAG samples that mimic real-world retrieval scenarios. We constructed a\\n∗Work was done during an internship at Pints AI\\n2https://github.com/Pints-AI/Finetune-Bench-RAG\\n3https://huggingface.co/datasets/pints-ai/Finetune-RAG\\narXiv:2505.10792v3  [cs.CL]  3 Dec 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='diverse dataset covering legal documents, scientific literature, books, and web data, each paired with\\na plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta’s\\nLlama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: aBaseline\\nformatand aStructured XMLvariant. This setup allows us to assess generalization and prompt\\nsensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing\\nRAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the\\nLLM’s ability to extract coherent answers from relevant chunks.\\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye,\\nsuch as financial reports from a similarly named company or outdated information based on dates\\nindicated by document metadata. Through fine-tuning models with a controlled mixture of true and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='such as financial reports from a similarly named company or outdated information based on dates\\nindicated by document metadata. Through fine-tuning models with a controlled mixture of true and\\nfalse context placed alongside, we teach them to ground their answers exclusively in the reliable\\ninformation provided.\\nWe evaluated the effectiveness of Finetune-RAG usingBench-RAG, a custom benchmarking suite\\nwe have created that leveragesGPT-4o(OpenAI, 2024) as an automated judge to assess the accu-\\nracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG\\nsubstantially improves factual correctness while maintaining output quality across other dimensions,\\ndemonstrating that generation-time defenses are a viable complement to improved retrieval.\\nOur contributions are as follows:\\n• Fine-tuning Approach.We propose a novel fine-tuning strategy for RAG systems that\\nteaches models to ignore misleading context and generate answers based solely on factual\\ninput.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Fine-tuning Approach.We propose a novel fine-tuning strategy for RAG systems that\\nteaches models to ignore misleading context and generate answers based solely on factual\\ninput.\\n• Training Dataset.We release a curated, multi-domain dataset designed for hallucination\\nresistance training, with both factual and fictitious content.\\n• Evaluation Setup.We benchmark the effectiveness of our approach using GPT-4o-based\\nevaluations and show significant gains in factual accuracy without compromising helpfulness\\nor relevance.\\n• Open-source release.We make our code, models, dataset, and evaluation framework\\npublicly available to facilitate further research. They can be accessed in our open-source\\nrepository4 and dataset5.\\nBy fine-tuning LLMs on RAG examples containing both factual and fictitious documents, we show\\nthat it is possible to build models that can reliably choose truth over noise. Our dataset reflects noisy,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='that it is possible to build models that can reliably choose truth over noise. Our dataset reflects noisy,\\ndomain-diverse retrieval as encountered in practice, making it a strong foundation for stress-testing\\nhallucination resistance in future RAG systems.\\n2 Background\\n2.1 Retrieval-Augmented Generation\\nRetrieval-Augmented Generation (RAG) augments large language models by incorporating external\\ndocuments into the generation process. Rather than relying solely on the model’s internal parameters,\\nRAG retrieves relevant passages from a knowledge base and feeds them, along with the user query,\\ninto the model to guide its response (Zhou et al., 2024).\\nA standard RAG system operates in two phases:\\n•Retrieval.A retriever model selects the top-kmost relevant documents for a given query.\\n• Generation.A language model generates a response conditioned on both the query and the\\nretrieved documents.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='•Retrieval.A retriever model selects the top-kmost relevant documents for a given query.\\n• Generation.A language model generates a response conditioned on both the query and the\\nretrieved documents.\\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information,\\nwhich is especially useful in fast-changing or specialized fields. However, it also introduces new\\nfailure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).\\n4https://github.com/Pints-AI/Finetune-Bench-RAG\\n5https://huggingface.co/datasets/pints-ai/Finetune-RAG\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='2.2 Hallucination in Language Models\\nHallucination refers to the phenomenon where language models produce outputs that are factually\\nincorrect or unsupported by the input, resulting in unfaithful outputs (Rawte et al., 2023). In RAG\\nsystems, hallucination can be especially problematic when the model is presented with a mixture of\\nrelevant and irrelevant (or even misleading) context. Even with carefully worded prompts, models\\ncan inadvertently \"trust\" incorrect sources and generate plausible but wrong answers (Yoran et al.,\\n2024).\\nDespite the presence of external context, most current models lack mechanisms to actively filter or\\nignore misleading information once it is included in the prompt (Shi et al., 2023). Finetune-RAG\\nspecifically targets this weakness by training models to develop this filtering capability.\\n3 Related Works\\n3.1 Mitigating Hallucination with Synthetic Prompt Tuning\\nSYNTRA (Jones et al., 2023) reduces hallucinations in large language models by modifying the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='3 Related Works\\n3.1 Mitigating Hallucination with Synthetic Prompt Tuning\\nSYNTRA (Jones et al., 2023) reduces hallucinations in large language models by modifying the\\nmodel’s instructions rather than adjusting its internal weights. SYNTRA does this by attaching a\\nsmall, trainable embedding vector to the system message, which acts as an additional instruction\\nprefix. This vector is optimized using a synthetic task where hallucinations are easy to measure. For\\nexample, the model is prompted to return names starting with a specific letter from a visible list, and\\nany incorrect or invented names are counted as hallucinations. By learning to avoid such mistakes in\\na controlled setting, the model can generalize to reduce hallucinations in downstream tasks. However,\\nbecause SYNTRA focuses on modifying prompts and not the model’s internal reasoning, it does not\\nenable the model to distinguish between factual and misleading content, failing to address real-world'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='because SYNTRA focuses on modifying prompts and not the model’s internal reasoning, it does not\\nenable the model to distinguish between factual and misleading content, failing to address real-world\\nRAG scenarios (Barnett et al., 2024)(Shi et al., 2023).\\n3.2 Refusal-Aware Fine-Tuning\\nZhang et al. (2024) propose a fine-tuning method, R-Tuning, that teaches language models to express\\nuncertainty and decline to answer when a question falls outside their pre-trained knowledge. This is\\nachieved by identifying questions the model answers incorrectly during training and appending an\\nuncertainty statement such as “I am unsure” to those responses. The result is a model that behaves\\nmore conservatively and with improved confidence calibration. However, R-Tuning is designed for\\nclosed-book settings, where the model relies only on its internal knowledge without a RAG system.\\n3.3 Constrained Reasoning with Decompose-and-Query (D&Q)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='closed-book settings, where the model relies only on its internal knowledge without a RAG system.\\n3.3 Constrained Reasoning with Decompose-and-Query (D&Q)\\nCao et al. (2023) propose the Decompose-and-Query (D&Q) framework, which extends retrieval-\\naugmented generation (RAG) by teaching language models to break down complex queries, retrieve\\nrelevant information using external tools, and generate answers based on a structured knowledge\\nsource. In particular, D&Q introduces a curated question–answer (QA) base, which is a collection of\\nverified QA pairs that the model consults during reasoning. This setup helps reduce hallucinations\\nby constraining the model to reliable content and allowing it to backtrack when inconsistencies are\\ndetected.\\nHowever, the effectiveness of D&Q depends strongly on the quality and coverage of its QA base. In\\npractical RAG applications, where retrieved content can be noisy, ambiguous, or incomplete (Shi'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='However, the effectiveness of D&Q depends strongly on the quality and coverage of its QA base. In\\npractical RAG applications, where retrieved content can be noisy, ambiguous, or incomplete (Shi\\net al., 2023), relying on a fixed and curated source may become a limitation. Since the framework\\nlacks mechanisms to dynamically assess the reliability of new information, it remains susceptible to\\nhallucinations caused by misleading or inaccurate context.\\n4 Methodology\\nWe introduceFinetune-RAG, a fine-tuning method designed to train large language models (LLMs)\\nto distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG)\\nsetup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase,\\nFinetune-RAG focuses on improving the model’s generation behavior when faced with imperfect or\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='misleading inputs. Our core idea is to fine-tune the model using examples where both correct and\\nincorrect information are explicitly presented to model, allowing it to learn the ability to sift out the\\ncorrect information to use for its response.\\n4.1 Problem Setup\\nIn a typical RAG system, the model is given a user query q and a set of retrieved documents\\n{d1, d2, ..., dk} (Zhou et al., 2024). When any of the documents is irrelevant or misleading, the model\\nmay generate incorrect responses (Yoran et al., 2024).\\nIn Finetune-RAG, we simulate this scenario during training by constructing prompts that include:\\n• One correct (factual) document chunkd correct\\n• One fictitious (misleading) document chunkd fictitious\\n• A corresponding questionq\\n• A reference answera, written using onlyd correct as the reference\\nThe model is then trained using supervised fine-tuning to produce the answer a despite having access\\nto bothd correct andd fictitious in the input.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='The model is then trained using supervised fine-tuning to produce the answer a despite having access\\nto bothd correct andd fictitious in the input.\\nIn Bayesian modeling, we can think of the task as a conditional generation problem where the goal is\\nto maximize the probability of generating a truthful answer a given a question q and a mixed set of\\ncontexts (some correctd correct, some fictitiousd fictitious).\\nWe aim to model:\\nP(a|q, d correct, dfictitious)(1)\\nHowever, this is the observed conditional probability, and what we want the model to learn is to\\nignore dfictitious and generate the answer as if conditioned only on dcorrect. So our training objective is\\nto align to the following idealized posterior:\\nP∗(a|q, dcorrect, dfictitious)→P(a|q, d correct)(2)\\nIn other words, even though the model receives both correct and fictitious information, it must assign\\nzero (or negligible) attention/mass tod fictitious during decoding.\\n4.2 Prompt Construction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='In other words, even though the model receives both correct and fictitious information, it must assign\\nzero (or negligible) attention/mass tod fictitious during decoding.\\n4.2 Prompt Construction\\nEach training example in Finetune-RAG is processed to include asystem messageand auser\\nmessage, following the standard instruction-tuning format (Ouyang et al., 2022) used in chat-style\\nlanguage models. The system message defines the behavior of the assistant, while the user message\\nprovides the question along with correct and fictitious information.\\n4.2.1 System Message\\nThe system message is consistent in all training examples. It instructs the assistant to rely solely on\\nthe provided context and discourages the use of prior knowledge or hallucination:\\n\"Some information is retrieved from the database as provided based on the\\nuser’s question. The assistant is to answer the question to the best of\\nhis/her ability, using only the information provided. The assistant must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='user’s question. The assistant is to answer the question to the best of\\nhis/her ability, using only the information provided. The assistant must\\nnot add his/her own knowledge.\"\\n4.2.2 User Message\\nTo help the model distinguish between factual and fictitious context more effectively, we explore the\\nuse of XML-like (Bray et al., 1998) structured input. We hypothesize that introducing a consistent\\nand explicit hierarchy, where document chunks are clearly labeled and separated, can make it easier\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='for the model to parse and evaluate different sources of information. This is especially important in\\nRAG settings, where hallucinations often result from the model blending or misattributing content\\nacross documents. Our approach aligns with findings from recent work such as StructRAG (Li et al.,\\n2024b) and SRAG (Lin et al., 2025), which demonstrates that task-specific structured representations\\nsuch as tables or graphs can significantly improve the performance of LLMs on knowledge-intensive\\nreasoning tasks. Our use of XML aims to impose syntactic clarity and boundary enforcement at the\\ninput level.\\nTo test this, we compare two user message formats: an unstructuredBaseline Formatand a structured\\nXML Format. Both present a question along with two document chunks, one factual and one\\nfictitious, but differ in how the information is presented. Refer to Section 6.4 for the exact prompt\\nstructure.\\n5 Experimental Setup\\n5.1 Model'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='fictitious, but differ in how the information is presented. Refer to Section 6.4 for the exact prompt\\nstructure.\\n5 Experimental Setup\\n5.1 Model\\nWe fine-tuned Meta’s Llama 3.1–8B-Instruct (Grattafiori et al., 2024), an instruct-tuned model that\\nsupports chat-style interaction and long context windows. We adapt the system and user message\\nformatting based on the chosen prompt structure described in Section 4.2.2.\\n5.2 Dataset and Preprocessing\\nOur dataset contains a total of 1,653 examples from diverse domains, such as legal documents,\\nscientific papers, news articles, and technical reports. For the complete structure of each example in\\nthe dataset, refer to Annex A.\\nEach example is formatted in both the baseline and XML structures. The dataset is then partitioned\\ninto training (80%), validation (10%), and test (10%) sets.\\n5.3 Hyperparameters\\nWe selected hyperparameter values that balance model performance with computational efficiency.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='into training (80%), validation (10%), and test (10%) sets.\\n5.3 Hyperparameters\\nWe selected hyperparameter values that balance model performance with computational efficiency.\\nRefer to Table 1 for the complete set of hyperparameters used.\\nTable 1: Fine-tuning hyperparameters used on Llama 3.1-8B-Instruct\\nParameter Value\\nSteps 20\\nBatch size 64\\nLearning rate 2e-5\\nWarmup ratio 0.1\\nLR Scheduler Cosine decay\\nOptimizer AdamW\\nβ1 0.9\\nβ2 0.95\\nWeight decay 0.1\\nMixed precision BF16\\n5.4 Checkpoints and Reproducibility\\nWe have released the model checkpoints fine-tuned with both Baseline6 and XML7 formats on Hug-\\ngingFace. Each prompt structure has two repositories, and each repository contains five checkpoints,\\ntotaling 10 checkpoints each.\\n6https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-2\\n7https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-2\\n7https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-2\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='6 Evaluation\\nWe evaluate Finetune-RAG’s ability to generate factually accurate answers when presented with both\\ncorrect and fictitious context. Our evaluation framework focuses on measuring whether the model\\nis able toselectively use only the correct information, and we assess output quality across four key\\ndimensions.\\n6.1 Bench-RAG\\nWe adopt a custom benchmarking pipeline, namelyBench-RAG, usingGPT-4omodel (OpenAI,\\n2024) in a LLM-as-a-judge inspired by prior work(Zheng et al., 2023; Gu et al., 2025; Li et al., 2025).\\nUsing structured prompts to elicit consistent evaluations for each model output, we measure:\\n• Accuracy: A binary metric indicating whether the generated answer is factually correct and\\nbased solely on the correct chunk. (True/False)\\n• Helpfulness: A score from 1 to 10 assessing how useful the answer is in addressing the\\nuser’s question.\\n•Relevance: A score from 1 to 10 measuring how relevant the content is to the query.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Helpfulness: A score from 1 to 10 assessing how useful the answer is in addressing the\\nuser’s question.\\n•Relevance: A score from 1 to 10 measuring how relevant the content is to the query.\\n•Depth: A score from 1 to 10 reflecting the level of detail or insight present in the answer.\\nEach generated output is rated using a structured prompt format, which requests scores across these\\ncategories and a brief justification. Refer to Appendix B for the full structure. This methodology\\ndraws from recent research demonstrating that LLMs can align closely with human preferences when\\nprompted properly, achieving high inter-rater agreement, i.e. multiple evaluators provide consistent\\nratings for the same outputs (Gu et al., 2025; Li et al., 2025).\\n6.2 Checkpoints Evaluated\\nFor each prompt structure, we evaluate all 10 model checkpoints saved during training (see Section\\n5.4). These checkpoints represent the model’s learning trajectory over the course of a single fine-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='For each prompt structure, we evaluate all 10 model checkpoints saved during training (see Section\\n5.4). These checkpoints represent the model’s learning trajectory over the course of a single fine-\\ntuning epoch. At each checkpoint, we generate answers to the test dataset questions using both the\\ncorrect context dcorrect and the fictitious context dfictitious. The generated answers are then submitted to\\nthe evaluator for scoring. Refer to Appendix B.1 and B.2 for the structure of the prompt used for\\nevaluation.\\n6.3 Results\\nWe report quantitative results from our fine-tuning experiments scored across 4 dimensions:factual\\naccuracy, helpfulness, relevance, and depth. Evaluation was performed using GPT-4o (OpenAI,\\n2024) as an LLM judge, as described in Section 6.1. We then aggregate the scores of each sequence\\nin the test dataset to derive the final evaluation result for each checkpoint:\\n¯Accuracy=\\n \\n1\\nntest\\nntestX\\ni=1\\n1[Accuracyi =T rue]\\n!\\n×100%(3)\\n¯Helpfulness= 1\\nntest\\nntestX\\ni=1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='in the test dataset to derive the final evaluation result for each checkpoint:\\n¯Accuracy=\\n \\n1\\nntest\\nntestX\\ni=1\\n1[Accuracyi =T rue]\\n!\\n×100%(3)\\n¯Helpfulness= 1\\nntest\\nntestX\\ni=1\\nHelpfulness i (4)\\n¯Relevance= 1\\nntest\\nntestX\\ni=1\\nRelevancei (5)\\n¯Depth= 1\\nntest\\nntestX\\ni=1\\nDepthi (6)\\nFigures 1 and 2 summarize performance trends across training steps. We observe consistent improve-\\nments in factual accuracy over time, particularly in the Baseline format. In most cases, gains in\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='accuracy are achieved without sacrificing helpfulness or relevance, and in later checkpoints, all four\\nmetrics reach strong levels of performance.\\nNotably, accuracy rises from 76.97% at step 0 to 98.18% at step 20 in the Baseline format, demon-\\nstrating the model’s increasing ability to ignore fictitious context. Helpfulness and depth also improve\\nsteadily, with a dip at the first generated checkpoint.\\nFigure 1: Evaluation results across training steps (Baseline format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. (%) Help Rel. Depth\\n0 76.97 8.81 9.55 8.32\\n2 67.88 7.08 7.48 6.76\\n4 91.52 8.08 8.47 7.15\\n6 93.94 9.58 9.83 8.81\\n8 96.36 9.38 9.61 8.55\\n10 97.58 9.33 9.62 8.51\\n12 96.36 9.52 9.78 8.80\\n14 96.97 9.73 9.91 9.01\\n16 97.589.789.959.06\\n18 97.58 9.77 9.95 9.05\\n2098.189.779.959.02 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='16 97.589.789.959.06\\n18 97.58 9.77 9.95 9.05\\n2098.189.779.959.02 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\nFigure 2: Evaluation results across training steps (XML format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. Help Rel Depth\\n0 78.79 8.81 9.56 8.19\\n2 52.73 5.79 6.16 5.24\\n4 87.88 6.56 7.09 5.47\\n6 95.76 9.46 9.73 8.75\\n8 94.55 9.09 9.35 8.21\\n10 94.55 8.93 9.32 8.01\\n12 95.76 8.95 9.33 8.05\\n14 95.76 9.28 9.59 8.52\\n16 97.58 9.35 9.61 8.61\\n1897.589.28 9.50 8.50\\n20 96.979.40 9.64 8.64 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='6.4 Ablation: Effect of Prompt Structure\\nTo assess the impact of prompt formatting on hallucination resistance, we perform an ablation study\\ncomparing two versions of Finetune-RAG: one trained using theBaseline formatand another using\\na more structuredXML format. Both models were fine-tuned on the same dataset with identical\\nhyperparameters and evaluated using the same GPT-4o-based benchmarking pipeline.\\nPrompt Format DifferencesThe Baseline format presents context in a flat, unstructured layout,\\nwhile the XML format uses nested tags to explicitly delineate retrieved content blocks (see Section\\n4.2.2). We hypothesized that structured formatting might help the model better separate and reason\\nabout distinct chunks.\\nBaseline FormatThis format presents the retrieved content in a plain and direct layout:\\nFilename: {filename1}\\nInformation:\\n{content1}\\nFilename: {filename2}\\nInformation:\\n{content2}\\nQuestion: {question}'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Filename: {filename1}\\nInformation:\\n{content1}\\nFilename: {filename2}\\nInformation:\\n{content2}\\nQuestion: {question}\\nXML FormatThis version wraps the content in an XML-like structure for clearer boundaries:\\n<Results>\\n<Result>\\n<Filename>{filename1}</Filename>\\n<Information>{content1}</Information>\\n</Result>\\n<Result>\\n<Filename>{filename2}</Filename>\\n<Information>{content2}</Information>\\n</Result>\\n</Results>\\nQuestion: {question}\\nResultsAs shown in Figures 1 and 2, both models demonstrate strong improvements over time.\\nHowever, the Baseline model consistently achieves higher accuracy and better overall scores in the\\nlater checkpoints:\\n• At step 20, the Baseline-tuned model achieves an accuracy of 98.18%, compared to 96.97%\\nfor the XML-tuned model.\\n• The Baseline-tuned model also maintains slightly higher scores for helpfulness (9.77 vs\\n9.40) and depth (9.02 vs 8.64).\\nInterpretationThese results suggest that while XML-style formatting introduces clear structural'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='9.40) and depth (9.02 vs 8.64).\\nInterpretationThese results suggest that while XML-style formatting introduces clear structural\\nboundaries that aid human readers, it did not consistently outperform the simpler Baseline prompt. We\\noffer two possible explanations: (1) the model may have developed inductive biases from pretraining\\nthat favor interpreting flat, plain-text layouts, such as those seen in summaries or abstracts, and (2)\\nfine-tuning datasets used in LLaMA or similar models may have predominantly featured unstructured\\nprompts, making the model more adept at handling them.\\nThis suggests that while prompt formatting is an important factor, training data design and supervision\\nsignal play a larger role in hallucination resistance.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='7 Discussion\\nOur results show that Finetune-RAG significantly improves a model’s ability to resist hallucinations\\nin a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning\\nwith dual-context examples leads to consistent improvements in factual accuracy, while preserving\\nhelpfulness, relevance, and depth.\\n7.1 Inductive Bias Emergence in Structure-Agnostic Learning\\nA significant and perhaps unexpected result in our study is that models trained on unstructured\\nprompts (Baseline format) performed better, especially in factual accuracy, compared to those trained\\nwith structured XML prompts. This challenges the common belief that clear structure always aids\\nreasoning. Instead, it suggests a deeper learning process, which involves the development of stronger\\nbuilt-in tendencies for selecting content when structure is absent. This raises a potential area that can\\nbe further researched upon.\\n7.2 Limitations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='built-in tendencies for selecting content when structure is absent. This raises a potential area that can\\nbe further researched upon.\\n7.2 Limitations\\nDespite promising results, several limitations remain:\\n• Synthetic dataset generation: The fictitious content is generated using GPT-4o (OpenAI,\\n2024), which may introduce distributional artifacts that differ from real-world retrieval\\nerrors. Additionally, the size of the dataset can be further increased for effective fine-tuning\\nin larger models.\\n• Binary supervision: We treat hallucination as a binary decision at the generation level.\\nHowever, hallucination is often more nuanced, involving partial truths, omissions, or subtle\\nphrasing, which our current framework may not sufficiently address.\\n• Controlled context pairing: During training, each example includes exactly one correct\\nand one incorrect document chunk. This creates a simplified binary contrast that may not'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Controlled context pairing: During training, each example includes exactly one correct\\nand one incorrect document chunk. This creates a simplified binary contrast that may not\\ngeneralize to real-world scenarios where multiple retrieved documents vary in quality. A\\nstronger training approach can be constructed using our existing dataset to create more\\nvaried and robust scenarios that the model can train on.\\n• Compute requirements: While our method is simpler and less resource-intensive than\\nalternatives such as full retraining or reinforcement learning, it still requires access to a\\nhigh-memory GPU (e.g., H100) to fine-tune long-context models with large batch sizes.\\nThis may limit accessibility for some users or institutions.\\n7.3 Future Work\\nThere are several promising extensions to Finetune-RAG that could further improve its robustness\\nand applicability:\\n• Training with more in-context RAG: Real-world retrieval often returns more than two'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='and applicability:\\n• Training with more in-context RAG: Real-world retrieval often returns more than two\\ndocuments, and the context window of LLMs are increasing rapidly. At the time of our\\nwork, we focused on relatively low context window of 8k, which would realistically be used\\nfor two to three RAG documents using up to 3k context window. With increasing context\\nwindow, future work can explore training with more RAG chunks to optimize LLMs RAG\\nperformance even at high level of stresses caused by more retrieved chunks. To support this,\\nwe future-proofed our dataset by including two additional relevant chunks per example to\\nsupport generating more complex multi-document training scenarios.\\n• Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the\\ngeneration component, combining it with learned retrieval mechanisms such as reranker-\\naware retrievers or contrastively trained retrievers could lead to further improvements in'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='generation component, combining it with learned retrieval mechanisms such as reranker-\\naware retrievers or contrastively trained retrievers could lead to further improvements in\\nfactual accuracy and context filtering.\\n• Multimodal extensions: Hallucination is not limited to text-based models. Ex-\\ntending Finetune-RAG to multimodal settings, such as image-caption retrieval or\\ncode+documentation generation, may help build more robust grounded systems in other\\ndomains.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucina-\\ntion settings, future work should assess Finetune-RAG’s impact on end-to-end performance\\nin downstream RAG applications such as open-domain question answering, legal document\\nsummarization, and domain-specific information retrieval.\\n8 Conclusion\\nIn this work, we presentFinetune-RAG, a simple yet effective method for reducing hallucination in\\nRetrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on\\nretrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while\\nignoring misleading information, with no architectural changes required.\\nWe constructed a diverse training set and evaluate usingBench-RAG, a technique that leverages\\nGPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving\\nhelpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts\\nrobustness, with less structured formats sometimes aiding discrimination.\\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully\\nimprove hallucination resistance in noisy retrieval environments. We release our code, dataset, and\\ncheckpoints to support further research in this direction, and highlight future extensions including\\nmulti-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='References\\nAgarwal, V ., Jin, Y ., Chandra, M., Choudhury, M. D., Kumar, S., and Sastry, N. (2024). Medhalu:\\nHallucinations in responses to healthcare queries by large language models.\\nBarnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., and Abdelrazek, M. (2024). Seven failure\\npoints when engineering a retrieval augmented generation system.\\nBray, T., Paoli, J., Sperberg-McQueen, C. M., Maler, E., and Yergeau, F. (1998). Extensible markup\\nlanguage (xml) 1.0.https://www.w3.org/TR/REC-xml/. W3C Recommendation.\\nCao, H., An, Z., Feng, J., Xu, K., Chen, L., and Zhao, D. (2023). A step closer to comprehensive\\nanswers: Constrained multi-stage question decomposition with large language models.\\nCui, M., Gao, P., Liu, W., Luan, J., and Wang, B. (2025). Multilingual machine translation with open\\nlarge language models at practical scale: An empirical study.\\nDahl, M., Magesh, V ., Suzgun, M., and Ho, D. E. (2024). Large legal fictions: Profiling legal'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='large language models at practical scale: An empirical study.\\nDahl, M., Magesh, V ., Suzgun, M., and Ho, D. E. (2024). Large legal fictions: Profiling legal\\nhallucinations in large language models.Journal of Legal Analysis, 16(1):64–93.\\nDong, J., Fatemi, B., Perozzi, B., Yang, L. F., and Tsitsulin, A. (2024). Don’t forget to connect!\\nimproving rag with graph-based reranking.\\nDuan, H., Yang, Y ., and Tam, K. Y . (2024). Do llms know about hallucination? an empirical\\ninvestigation of llm’s hidden states.\\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., and et al.\\n(2024). The llama 3 herd of models.\\nGu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y ., Ma, S., Liu, H., Wang, S., Zhang,\\nK., Wang, Y ., Gao, W., Ni, L., and Guo, J. (2025). A survey on llm-as-a-judge.\\nJones, E., Palangi, H., Simões, C., Chandrasekaran, V ., Mukherjee, S., Mitra, A., Awadallah, A., and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='K., Wang, Y ., Gao, W., Ni, L., and Guo, J. (2025). A survey on llm-as-a-judge.\\nJones, E., Palangi, H., Simões, C., Chandrasekaran, V ., Mukherjee, S., Mitra, A., Awadallah, A., and\\nKamar, E. (2023). Teaching language models to hallucinate less with synthetic tasks.\\nKang, H. and Liu, X.-Y . (2023). Deficiency of large language models in finance: An empirical\\nexamination of hallucination.\\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z., Bhattacharjee, A., Jiang, Y ., Chen, C.,\\nWu, T., Shu, K., Cheng, L., and Liu, H. (2025). From generation to judgment: Opportunities and\\nchallenges of llm-as-a-judge.\\nLi, J., Chen, J., Ren, R., Cheng, X., Zhao, W. X., Nie, J.-Y ., and Wen, J.-R. (2024a). The dawn after\\nthe dark: An empirical study on factuality hallucination in large language models.\\nLi, Z., Chen, X., Yu, H., Lin, H., Lu, Y ., Tang, Q., Huang, F., Han, X., Sun, L., and Li, Y . (2024b).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='the dark: An empirical study on factuality hallucination in large language models.\\nLi, Z., Chen, X., Yu, H., Lin, H., Lu, Y ., Tang, Q., Huang, F., Han, X., Sun, L., and Li, Y . (2024b).\\nStructrag: Boosting knowledge intensive reasoning of llms via inference-time hybrid information\\nstructurization.\\nLin, T., Zhu, Y ., Luo, Y ., and Tang, N. (2025). Srag: Structured retrieval-augmented generation for\\nmulti-entity question answering over wikipedia graph.\\nLiu, Y ., Shi, K., He, K., Ye, L., Fabbri, A., Liu, P., Radev, D., and Cohan, A. (2024). On learning to\\nsummarize with large language models as references. In Duh, K., Gomez, H., and Bethard, S.,\\neditors,Proceedings of the 2024 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\\n8647–8664, Mexico City, Mexico. Association for Computational Linguistics.\\nOpenAI (2024). Gpt-4o system card.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\\n8647–8664, Mexico City, Mexico. Association for Computational Linguistics.\\nOpenAI (2024). Gpt-4o system card.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\\nWelinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow\\ninstructions with human feedback.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Rawte, V ., Chakraborty, S., Pathak, A., Sarkar, A., Tonmoy, S. M. T. I., Chadha, A., Sheth, A. P., and\\nDas, A. (2023). The troubling emergence of hallucination in large language models – an extensive\\ndefinition, quantification, and prescriptive remediations.\\nRozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Sauvestre, R.,\\nRemez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori,\\nA., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T.,\\nand Synnaeve, G. (2024). Code llama: Open foundation models for code.\\nSawarkar, K., Mangal, A., and Solanki, S. R. (2024). Blended rag: Improving rag (retriever-\\naugmented generation) accuracy with semantic search and hybrid query-based retrievers. In2024\\nIEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),\\nvolume 24, page 155–161. IEEE.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),\\nvolume 24, page 155–161. IEEE.\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., Schärli, N., and Zhou, D. (2023). Large\\nlanguage models can be easily distracted by irrelevant context.\\nWang, Y ., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and Hoi, S. C. H. (2023). Codet5+: Open\\ncode large language models for code understanding and generation.\\nYasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J. (2022). Qa-gnn: Reasoning with\\nlanguage models and knowledge graphs for question answering.\\nYoran, O., Wolfson, T., Ram, O., and Berant, J. (2024). Making retrieval-augmented language models\\nrobust to irrelevant context.\\nZhang, H., Diao, S., Lin, Y ., Fung, Y . R., Lian, Q., Wang, X., Chen, Y ., Ji, H., and Zhang, T. (2024).\\nR-tuning: Instructing large language models to say ‘i don’t know’.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Zhang, H., Diao, S., Lin, Y ., Fung, Y . R., Lian, Q., Wang, X., Chen, Y ., Ji, H., and Zhang, T. (2024).\\nR-tuning: Instructing large language models to say ‘i don’t know’.\\nZhang, Y ., Li, Y ., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y ., Chen, Y ., Wang,\\nL., Luu, A. T., Bi, W., Shi, F., and Shi, S. (2023). Siren’s song in the ai ocean: A survey on\\nhallucination in large language models.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\\nE. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and\\nchatbot arena.\\nZhou, J. and Chen, L. (2025). Openrag: Optimizing rag end-to-end via in-context retrieval learning.\\nZhou, Y ., Liu, Y ., Li, X., Jin, J., Qian, H., Liu, Z., Li, C., Dou, Z., Ho, T.-Y ., and Yu, P. S. (2024).\\nTrustworthiness in retrieval-augmented generation systems: A survey.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='A Dataset Example Format\\nAligning with Section 4.1, each example in the dataset is structured as follows:\\n{\\n\"content\": <factual chunk>,\\n\"filename\": <original document filename>,\\n\"fictitious_content\": <misleading chunk>,\\n\"fictitious_filename\": <filename of misleading chunk>,\\n\"question\": <user query>,\\n\"answer\": <GPT-4o generated answer based only on correct content>,\\n}\\nB Bench-RAG Prompt Structure\\nGiven both the correct and fictitious document chunks, the fine-tuned model checkpoints are used\\nto generate answers for questions on the test dataset. The outputs are stored in a structured jsonl\\nformat, with each entry containing the following fields:\\n{\\n\"filename\": <original document filename>,\\n\"content\": <factual chunk>,\\n\"question\": <user query>,\\n\"response\": <model’s generated answer>\\n}\\nWith these output, we curate a prompt for the four measurements derived from our evaluation.\\nB.1 System Message for Evaluation\\nAccuracy\\n\"Please act as an impartial judge and evaluate the quality of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='B.1 System Message for Evaluation\\nAccuracy\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the accuracy of the\\nresponse.\\nYou will check whether the response contains extra details not found\\nin the piece of information provided. If extra details are found,\\naccuracy is false. Otherwise, accuracy is true. Take note that if the\\nresponse partially addresses the question, but did not provide extra\\ndetails not found in the piece of information provided, the response\\nwill still be considered accurate (hence accuracy = true).\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the accuracy\\nwith true or false by strictly following this JSON format:\\n{\\n\"accuracy_explanation\":'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='as possible. After providing your explanation, you must rate the accuracy\\nwith true or false by strictly following this JSON format:\\n{\\n\"accuracy_explanation\":\\n<provide an explanation on accuracy, whether extra details\\noutside the content were found.>,\\n\"accuracy\": <true/false>\\n}\"\\nHelpfulness\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='below, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the helpfulness of the\\nresponse.\\nYou will check whether the AI assistant is helpful in answering the\\nquestion based on the response.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nhelpfulness on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"helpfulness_explanation\": <provide an explanation on helpfulness>,\\n\"helpfulness\": <score>\\n}\"\\nRelevance\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the relevance of the\\nresponse.\\nYou will check the relevance of the response by evaluating whether the\\nresponse fully addresses the question.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='provided below. Your evaluation should consider the relevance of the\\nresponse.\\nYou will check the relevance of the response by evaluating whether the\\nresponse fully addresses the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nrelevance on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"relevance_explanation\": <provide an explanation on relevance>,\\n\"relevance\": <score>\\n}\"\\nDepth\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the depth of the\\nresponse.\\nYou will check the depth of the response by evaluating the level of\\ndetail of the response in answering the question.\\nBegin your evaluation by providing a short explanation. Be as objective'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='response.\\nYou will check the depth of the response by evaluating the level of\\ndetail of the response in answering the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\ndepth on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"depth_explanation\": <provide an explanation on depth>,\\n\"depth\": <score>\\n}\"\\nB.2 User Message for Evaluation\\nAll measurements utilizes the same user message structure for evaluation. Note that the content used\\nis the correct content, rather than the fictitious one:\\n[The Start of Provided Information Extracted from a File]\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Filename: {filename}\\nInformation: {content}\\n[The End of Provided Information]\\n[Question]\\n{question}\\n[The Start of Assistant’s Response]\\n{response}\\n[The End of Assistant’s Response]\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Rethinking the Role of Demonstrations:\\nWhat Makes In-Context Learning Work?\\nSewon Min1,2 Xinxi Lyu1 Ari Holtzman1 Mikel Artetxe2\\nMike Lewis2 Hannaneh Hajishirzi1,3 Luke Zettlemoyer1,2\\n1University of Washington 2Meta AI 3Allen Institute for AI\\n{sewon,alrope,ahai,hannaneh,lsz}@cs.washington.edu\\n{artetxe,mikelewis}@meta.com\\nAbstract\\nLarge language models (LMs) are able to in-\\ncontext learn—perform a new task via infer-\\nence alone by conditioning on a few input-\\nlabel pairs (demonstrations) and making pre-\\ndictions for new inputs. However, there has\\nbeen little understanding of how the model\\nlearns and which aspects of the demonstra-\\ntions contribute to end task performance. In\\nthis paper, we show that ground truth demon-\\nstrations are in fact not required—randomly\\nreplacing labels in the demonstrations barely\\nhurts performance on a range of classiﬁcation\\nand multi-choce tasks, consistently over 12 dif-\\nferent models including GPT-3. Instead, we\\nﬁnd that other aspects of the demonstrations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='hurts performance on a range of classiﬁcation\\nand multi-choce tasks, consistently over 12 dif-\\nferent models including GPT-3. Instead, we\\nﬁnd that other aspects of the demonstrations\\nare the key drivers of end task performance, in-\\ncluding the fact that they provide a few exam-\\nples of (1) the label space, (2) the distribution\\nof the input text, and (3) the overall format of\\nthe sequence. Together, our analysis provides\\na new way of understanding how and why\\nin-context learning works, while opening up\\nnew questions about how much can be learned\\nfrom large language models through inference\\nalone.\\n1 Introduction\\nLarge language models (LMs) have shown impres-\\nsive performance on downstream tasks by simply\\nconditioning on a few input-label pairs (demonstra-\\ntions); this type of inference has been referred to as\\nin-context learning (Brown et al., 2020). Despite in-\\ncontext learning consistently outperforming zero-\\nshot inference on a wide range of tasks (Zhao et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='in-context learning (Brown et al., 2020). Despite in-\\ncontext learning consistently outperforming zero-\\nshot inference on a wide range of tasks (Zhao et al.,\\n2021; Liu et al., 2021), there is little understanding\\nof how it works and which aspects of the demon-\\nstrations contribute to end task performance.\\nIn this paper, we show that ground truth demon-\\nstrations are in fact not required for effective in-\\ncontext learning (Section 4). Speciﬁcally, replac-\\ning the labels in demonstrations with random labels\\nbarely hurts performance in a range of classiﬁca-\\ntion and multi-choice tasks (Figure 1). The result\\nMetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nMetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 1: Results in classiﬁcation (top) and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 1: Results in classiﬁcation (top) and multi-\\nchoice tasks (bottom), using three LMs with varying\\nsize. Reported on six datasets on which GPT-3 is eval-\\nuated; the channel method is used. See Section 4 for\\nthe full results. In-context learning performance drops\\nonly marginally when labels in the demonstrations are\\nreplaced by random labels.\\nis consistent over 12 different models including the\\nGPT-3 family (Radford et al., 2019; Min et al.,\\n2021b; Wang and Komatsuzaki, 2021; Artetxe\\net al., 2021; Brown et al., 2020). This strongly\\nsuggests, counter-intuitively, that the model does\\nnot rely on the input-label mapping in the demon-\\nstrations to perform the task.\\nFurther analysis investigates which parts of\\ndemonstrations actually do contribute to the perfor-\\nmance. We identify possible aspects of demonstra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='strations to perform the task.\\nFurther analysis investigates which parts of\\ndemonstrations actually do contribute to the perfor-\\nmance. We identify possible aspects of demonstra-\\ntions (e.g., the label space and the distribution of\\nthe input text) and evaluate a series of variants of\\nthe demonstrations to quantify the impact of each\\n(Section 5). We ﬁnd that: (1) the label space and\\nthe distribution of the input text speciﬁed by the\\ndemonstrations are both key to in-context learn-\\ning (regardless of whether the labels are correct\\nfor individual inputs); (2) specifying the overall\\nformat is also crucial, e.g., when the label space\\nis unknown, using random English words as la-\\nbels is signiﬁcantly better than using no labels; and\\narXiv:2202.12837v2  [cs.CL]  20 Oct 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='(3) meta-training with an in-context learning objec-\\ntive (Min et al., 2021b) magniﬁes these effects—the\\nmodels almost exclusively exploit simpler aspects\\nof the demonstrations like the format rather than\\nthe input-label mapping.\\nIn summary, our analysis provides a new way\\nof understanding the role of the demonstrations in\\nin-context learning. We empirically show that the\\nmodel (1) counter-intuitively does not rely on the\\nground truth input-label mapping provided in the\\ndemonstrations as much as we thought (Section 4),\\nand (2) nonetheless still beneﬁts from knowing the\\nlabel space and the distribution of inputs speciﬁed\\nby the demonstrations (Section 5). We also include\\na discussion of broader implications, e.g., what we\\ncan say about the model learning at test time, and\\navenues for future work (Section 6).\\n2 Related Work\\nLarge language models have been key to strong per-\\nformance in a wide range of downstream tasks (De-\\nvlin et al., 2019; Radford et al., 2019; Liu et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='2 Related Work\\nLarge language models have been key to strong per-\\nformance in a wide range of downstream tasks (De-\\nvlin et al., 2019; Radford et al., 2019; Liu et al.,\\n2019; Raffel et al., 2020; Lewis et al., 2020). While\\nﬁnetuning has been a popular approach to transfer\\nto new tasks (Devlin et al., 2019), it is often imprac-\\ntical to ﬁnetune a very large model (e.g. ≥10B pa-\\nrameters). Brown et al. (2020) propose in-context\\nlearning as an alternative way to learn a new task.\\nAs depicted in Figure 2, the LM learns a new task\\nvia inference alone by conditioning on a concatena-\\ntion of the training data as demonstrations, without\\nany gradient updates.\\nIn-context learning has been the focus of signif-\\nicant study since its introduction. Prior work pro-\\nposes better ways of formulating the problem (Zhao\\net al., 2021; Holtzman et al., 2021; Min et al.,\\n2021a), better ways of choosing labeled exam-\\nples for the demonstrations (Liu et al., 2021; Lu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='poses better ways of formulating the problem (Zhao\\net al., 2021; Holtzman et al., 2021; Min et al.,\\n2021a), better ways of choosing labeled exam-\\nples for the demonstrations (Liu et al., 2021; Lu\\net al., 2021; Rubin et al., 2021), meta-training\\nwith an explicit in-context learning objective (Chen\\net al., 2021; Min et al., 2021b), and learning to\\nfollow instructions as a variant of in-context learn-\\ning (Mishra et al., 2021b; Efrat and Levy, 2020;\\nWei et al., 2022a; Sanh et al., 2022). At the\\nsame time, some work reports brittleness and over-\\nsensitivity for in-context learning (Lu et al., 2021;\\nZhao et al., 2021; Mishra et al., 2021a).\\nRelatively less work has been done to understand\\nwhy in-context learning works. Xie et al. (2022)\\nprovide theoretical analysis that in-context learn-\\ning can be formalized as Bayesian inference that\\nCirculation revenue has increased by 5% in Finland.         \\\\n    Positive \\nPanostaja did not disclose the purchase price.                  \\\\n    Neutral'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Circulation revenue has increased by 5% in Finland.         \\\\n    Positive \\nPanostaja did not disclose the purchase price.                  \\\\n    Neutral \\nPaying off the national debt will be extremely painful.      \\\\n    Negative \\nThe acquisition will have an immediate positive impact.  \\\\n    ________\\n =\\nDemonstrations\\nLM\\nPositive\\nTest input\\nPrediction\\nFigure 2: An overview of in-context learning. The\\ndemonstrations consist of k input-label pairs from the\\ntraining data (k = 3in the ﬁgure).\\nModel # Params Public Meta-trained\\nGPT-2 Large 774M \\x13 \\x17\\nMetaICL 774M \\x13 \\x13\\nGPT-J 6B \\x13 \\x17\\nfairseq 6.7B† 6.7B \\x13 \\x17\\nfairseq 13B† 13B \\x13 \\x17\\nGPT-3 175B ‡ \\x17 \\x17\\nTable 1: A list of LMs used in the experiments:\\nGPT-2 (Radford et al., 2019), MetaICL (Min et al.,\\n2021b), GPT-J (Wang and Komatsuzaki, 2021), fairseq\\nLMs (Artetxe et al., 2021) and GPT-3 (Brown et al.,\\n2020). ‘Public’ indicates whether the model weights\\nare public; ‘Meta-trained’ indicates whether the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='LMs (Artetxe et al., 2021) and GPT-3 (Brown et al.,\\n2020). ‘Public’ indicates whether the model weights\\nare public; ‘Meta-trained’ indicates whether the model\\nis meta-trained with an in-context learning objective.\\n†We use dense models in Artetxe et al. (2021) and re-\\nfer them as fairseq LMs for convenience. ‡We use the\\nDavinci API (the base version, not the instruct version)\\nand assume it to be 175B, following Gao et al. (2021)\\nand Artetxe et al. (2021).\\nuses the demonstrations to recover latent concepts.\\nRazeghi et al. (2022) show that in-context learn-\\ning performance is highly correlated with term fre-\\nquencies in the pretraining data. To the best of our\\nknowledge, this paper is the ﬁrst that provides an\\nempirical analysis that investigates why in-context\\nlearning achieves performance gains over zero-shot\\ninference. We ﬁnd that the ground truth input-label\\nmapping in the demonstrations has only a marginal\\neffect, and measure the impact of ﬁner-grained as-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='inference. We ﬁnd that the ground truth input-label\\nmapping in the demonstrations has only a marginal\\neffect, and measure the impact of ﬁner-grained as-\\npects of the demonstrations.\\n3 Experimental Setup\\nWe describe the experimental setup used in our\\nanalysis (Section 4 and 5).\\nModels. We experiment with 12 models in to-\\ntal. We include 6 language models (Table 1), all\\nof which are decoder-only, dense LMs. We use\\neach LM with two inference methods, direct and\\nchannel, following Min et al. (2021a). The sizes\\nof LMs vary from 774M to 175B. We include the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nDirect\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 3: Results when using no-demonstrations, demonstrations with gold labels, and demonstrations with ran-\\ndom labels in classiﬁcation (top) and multi-choice tasks (bottom). The ﬁrst eight models are evaluated on 16\\nclassiﬁcation and 10 multi-choice datasets, and the last four models are evaluated on 3 classiﬁcation and 3 multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='classiﬁcation and 10 multi-choice datasets, and the last four models are evaluated on 3 classiﬁcation and 3 multi-\\nchoice datasets. See Figure 11 for numbers comparable across all models. Model performance with random\\nlabels is very close to performance with gold labels (more discussion in Section 4.1).\\nlargest dense LM (GPT-3) and the largest publicly\\nreleased dense LM (fairseq 13B) at the time of con-\\nducting experiments. We also include MetaICL,\\nwhich is initialized from GPT-2 Large and then\\nmeta-trained on a collection of supervised datasets\\nwith an in-context learning objective, and ensure\\nthat our evaluation datasets do not overlap with\\nthose used at meta-training time.\\nEvaluation Data. We evaluate on 26 datasets,\\nincluding sentiment analysis, paraphrase detection,\\nnatural language inference, hate speech detection,\\nquestion answering, and sentence completion (full\\nlist and references provided in Appendix A).1 All\\ndatasets are classiﬁcation and multi-choice tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='question answering, and sentence completion (full\\nlist and references provided in Appendix A).1 All\\ndatasets are classiﬁcation and multi-choice tasks.\\nWe use these datasets because they (1) are true\\nlow-resource datasets with less than 10K train-\\ning examples, (2) include well-studied bench-\\nmarks from GLUE (Wang et al., 2018) and Super-\\nGLUE (Wang et al., 2019a), and (3) cover diverse\\ndomains including science, social media, ﬁnance,\\nand more.\\nOther Details. We use k = 16 examples as\\ndemonstrations by default for all experiments in\\nthe paper, unless otherwise speciﬁed. Examples\\nare sampled at uniform from the training data.\\nWe choose a set of k training examples using\\n5 different random seeds and run experiments 5\\ntimes. For fairseq 13B and GPT-3, due to lim-\\nited resources, we experiment with a subset of 6\\n1For convenience, we use ‘labels’ to refer to the output for\\nthe task, though our datasets include non-classiﬁcation tasks.\\ndatasets2 and 3 random seeds. We report Macro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='1For convenience, we use ‘labels’ to refer to the output for\\nthe task, though our datasets include non-classiﬁcation tasks.\\ndatasets2 and 3 random seeds. We report Macro-\\nF13 for classiﬁcation tasks and Accuracy for multi-\\nchoice tasks. We compute per-dataset average over\\nseeds, and then report macro-average over datasets.\\nWe use the minimal templates in forming an in-\\nput sequence from an example. We refer to Ap-\\npendix B for more details. All experiments are\\nreproducible from github.com/Alrope123/\\nrethinking-demonstrations.\\n4 Ground Truth Matters Little\\n4.1 Gold labels vs. random labels\\nTo see the impact of correctly-paired inputs and\\nlabels in the demonstrations—which we call the\\nground truth input-label mapping—we compare the\\nfollowing three methods.4\\nNo demonstrations is a typical zero-shot method\\nthat does not use any labeled data. A prediction\\nis made via argmaxy∈CP(y|x), where x is the test\\ninput and Cis a small discrete set of possible labels.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='that does not use any labeled data. A prediction\\nis made via argmaxy∈CP(y|x), where x is the test\\ninput and Cis a small discrete set of possible labels.\\nDemonstrations w/ gold labels are used in a typi-\\ncal in-context learning method with k labeled ex-\\namples (x1, y1)...(xk, yk). A concatenation of k\\ninput-label pairs is used to make a prediction via\\nargmaxy∈CP(y|x1, y1...xk, yk, x).\\n2Three classiﬁcation and three multi-choice: MRPC, RTE,\\nTweet_eval-hate, OpenbookQA, CommonsenseQA, COPA.\\n3Known to be better for imbalanced classes.\\n4Without loss of generality, all methods in Section 4 and 5\\nare described based on the direct method, but can be trivially\\nconverted to the channel method by ﬂipping x and y.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (Classification) GPT-J (Classification) MetaICL (Multi-choice) GPT-J (Multi-choice)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Accuracy (%)\\n100% correct 75% correct 50% correct 25% correct 0% correct No Demos\\nFigure 4: Results with varying number of correct labels in the demonstrations. Channel and Direct used for\\nclassiﬁcation and multi-choice, respectively. Performance with no demonstrations (blue) is reported as a reference.\\nDemonstrations w/ random labels are formed\\nwith random labels, instead of gold labels from\\nthe labeled data. Each xi (1 ≤ i ≤\\nk) is paired with ˜yi that is randomly sam-\\npled at uniform from C. A concatenation of\\n(x1, ˜y1)...(xk, ˜yk) is then used to make a predic-\\ntion via argmaxy∈CP(y|x1, ˜y1...xk, ˜yk, x).\\nResults are reported in Figure 3. First, using the\\ndemonstrations with gold labels signiﬁcantly im-\\nproves the performance over no demonstrations,5\\nas it has been consistently found in much of prior\\nwork (Brown et al., 2020; Zhao et al., 2021; Liu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='demonstrations with gold labels signiﬁcantly im-\\nproves the performance over no demonstrations,5\\nas it has been consistently found in much of prior\\nwork (Brown et al., 2020; Zhao et al., 2021; Liu\\net al., 2021). We then ﬁnd that replacing gold la-\\nbels with random labels only marginally hurts\\nperformance. The trend is consistent over nearly\\nall models: models see performance drop in the\\nrange of 0–5% absolute. There is less impact in\\nreplacing labels in multi-choice tasks (1.7% on av-\\nerage) than in classiﬁcation tasks (2.6% absolute).\\nThis result indicates that the ground truth input-\\nlabel pairs are not necessary to achieve perfor-\\nmance gains. This is counter-intuitive, given that\\ncorrectly paired training data is critical in typical\\nsupervised training—it informs the model of the ex-\\npected input-label correspondence required to per-\\nform the downstream task. Nonetheless, the mod-\\nels do achieve non-trivial performance on the down-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='pected input-label correspondence required to per-\\nform the downstream task. Nonetheless, the mod-\\nels do achieve non-trivial performance on the down-\\nstream tasks. This strongly suggests that the mod-\\nels are capable of recovering the expected input-\\nlabel correspondence for the task; however, it isnot\\ndirectly from the pairings in the demonstrations.\\nIt is also worth noting that there is particularly\\nlittle performance drop in MetaICL: 0.1–0.9% ab-\\nsolute. This suggests that meta-training with an\\nexplicit in-context learning objective actually en-\\ncourages the model to essentially ignore the input-\\n5There are some exceptions, e.g., in the classiﬁcation tasks,\\nDirect GPT-2, Direct GPT-J and Direct fairseq 6.7B models\\nare not signiﬁcantly better than random guessing on many\\ndatasets; Channel fairseq 13B has signiﬁcantly better no-\\ndemonstrations performance compared to demonstrations with\\ngold labels. We thus discuss the results from these models less'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='datasets; Channel fairseq 13B has signiﬁcantly better no-\\ndemonstrations performance compared to demonstrations with\\ngold labels. We thus discuss the results from these models less\\nsigniﬁcantly for the rest of analysis.\\n0 4 8 16 32\\nk\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDemos w/ gold\\nDemos w/ random\\n0 4 8 16 32\\nk\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\nDemos w/ gold\\nDemos w/ random\\nFigure 5: Ablations on varying numbers of examples\\nin the demonstrations (k). Models that are the best un-\\nder 13B in each task category (Channel MetaICL and\\nDirect GPT-J, respectively) are used.\\nlabel mapping and exploit other components of the\\ndemonstrations (more discussion in Section 5.4).\\nIn Appendix C.2, we provide additional results\\nshowing that (1) selecting random labels from a\\ntrue distribution of labels (instead of a uniform\\ndistribution) reduces the gap even further, and (2)\\nthe trends may depend on the dataset, although the\\noverall trend is consistent over most datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='distribution) reduces the gap even further, and (2)\\nthe trends may depend on the dataset, although the\\noverall trend is consistent over most datasets.\\n4.2 Ablations\\nFor additional ablations, we experiment with 5 clas-\\nsiﬁcation and 4 multi-choice datasets.6\\nDoes the number of correct labels matter? To\\nfurther examine the impact of correctness of la-\\nbels in the demonstrations, we conduct an ablation\\nstudy by varying the number of correct labels in the\\ndemonstrations. We evaluate “Demonstrations w/\\na% correct labels” (0 ≤a ≤100) which consist\\nof k ×a/100 correct pairs and k ×(1 −a/100)\\nincorrect pairs (see Algorithm 1 in Appendix B).\\nHere, a = 100 is the same as typical in-context\\nlearning, i.e., demonstrations w/ gold labels.\\nResults are reported in Figure 4. Model perfor-\\nmance is fairly insensitive to the number of correct\\nlabels in the demonstrations. In fact, always us-\\ning incorrect labels signiﬁcantly outperforms no-\\n6Classiﬁcation includes: MRPC, RTE, Tweet_eval-hate,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='labels in the demonstrations. In fact, always us-\\ning incorrect labels signiﬁcantly outperforms no-\\n6Classiﬁcation includes: MRPC, RTE, Tweet_eval-hate,\\nSICK, poem-sentiment; Multi-choice includes OpenbookQA,\\nCommonsenseQA, COPA and ARC.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (Classification) GPT-J (Classification) MetaICL (Multi-choice) GPT-J (Multi-choice)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Accuracy (%)\\nNo demos Gold labels Random labels No demos + T Gold labels + T Random labels + T\\nFigure 6: Results with minimal templates and manual templates. ‘+T’ indicates that manual templates are used.\\nChannel and Direct used for classiﬁcation and multi-choice, respectively.\\ndemonstrations, e.g., preserving 92%, 100% and\\n97% of improvements from using the demonstra-\\ntions with MetaICL in classiﬁcation, MetaICL in\\nmulti-choice, and GPT-J in multi-choice, respec-\\ntively. In contrast, GPT-J in classiﬁcation sees\\nrelatively signiﬁcant performance drop with more\\nincorrect labels, e.g., nearly 10% drop in perfor-\\nmance when always using incorrect labels. Still,\\nalways using incorrect labels is signiﬁcantly better\\nthan no demonstrations.\\nIs the result consistent with varying k? We\\nstudy the impact of the number of input-label pairs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='always using incorrect labels is signiﬁcantly better\\nthan no demonstrations.\\nIs the result consistent with varying k? We\\nstudy the impact of the number of input-label pairs\\n(k) in the demonstrations. Results are reported in\\nFigure 5. First, using the demonstrations signiﬁ-\\ncantly outperforms the no demonstrations method\\neven with small k (k = 4), and performance drop\\nfrom using gold labels to using random labels is\\nconsistently small across varying k, in the range of\\n0.8–1.6%.7 Interestingly, model performance does\\nnot increase much as k increases when k ≥8, both\\nwith gold labels and with random labels. This is\\nin contrast with typical supervised training where\\nmodel performance rapidly increases ask increases,\\nespecially when k is small. We hypothesize that\\nlarger labeled data is beneﬁcial mainly for super-\\nvising the input-label correspondence, and other\\ncomponents of the data like the example inputs,\\nexample labels and the data format are easier to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='larger labeled data is beneﬁcial mainly for super-\\nvising the input-label correspondence, and other\\ncomponents of the data like the example inputs,\\nexample labels and the data format are easier to\\nrecover from the small data, which is potentially a\\nreason for minimal performance gains from larger\\nk (more discussion in Section 5).\\nIs the result consistent with better templates?\\nWhile we use minimal templates by default, we\\nalso explore manual templates, i.e., templates that\\nare manually written in a dataset-speciﬁc manner,\\ntaken from prior work (details in Appendix B). Fig-\\nure 6 shows that the trend—replacing gold labels\\nwith random labels barely hurting performance—\\nholds with manual templates. It is worth noting\\n7With an exception of 4.4% in classiﬁcation with k = 4,\\nlikely due to a high variance with a very small value of k.\\nCirculation revenue has increased by 5% in Finland.         \\\\n         Positive\\nFormat \\n(The use \\nof pairs)\\n =\\nDistribution of inputs Label spaceDemonstrations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Circulation revenue has increased by 5% in Finland.         \\\\n         Positive\\nFormat \\n(The use \\nof pairs)\\n =\\nDistribution of inputs Label spaceDemonstrations\\nTest example Input-label mapping\\nPanostaja did not disclose the purchase price.                  \\\\n         Neutral\\nPaying off the national debt will be extremely painful.      \\\\n         Negative\\nThe acquisition will have an immediate positive impact.  \\\\n         ?\\nFigure 7: Four different aspects in the demonstrations:\\nthe input-label mapping, the distribution of the input\\ntext, the label space, and the use of input-label pairing\\nas the format of the demonstrations.\\nthat using manual templates does not always out-\\nperform using minimal templates.\\n5 Why does In-Context Learning work?\\nSection 4 shows that the ground truth input-label\\nmapping in the demonstrations has little impact to\\nperformance gains from in-context learning. This\\nsection further examines what other aspects of the\\ndemonstrations lead to good performance of in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='mapping in the demonstrations has little impact to\\nperformance gains from in-context learning. This\\nsection further examines what other aspects of the\\ndemonstrations lead to good performance of in-\\ncontext learning.\\nWe identify four aspects of the demonstrations\\n(x1, y1)...(xk, yk) that potentially provide learning\\nsignal (depicted in Figure 7).\\n1. The input-label mapping, i.e., whether each\\ninput xi is paired with a correct label yi.\\n2. The distribution of the input text , i.e., the\\nunderlying distribution that x1...xk are from.\\n3. The label space , i.e., the space covered by\\ny1...yk.\\n4. The format—speciﬁcally, the use of input-\\nlabel pairing as the format.\\nAs Section 4 does for the input-label mapping,\\nwe design a series of variants of the demonstrations\\nthat quantify the impact of each aspect in isolation\\n(Section 5.1–5.3). We then additionally discuss the\\ntrend of the models meta-trained with an in-context\\nlearning objective (Section 5.4). For all experi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='(Section 5.1–5.3). We then additionally discuss the\\ntrend of the models meta-trained with an in-context\\nlearning objective (Section 5.4). For all experi-\\nments, models are evaluated on ﬁve classiﬁcation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nFigure 8: Impact of the distribution of the inputs. Evaluated in classiﬁcation (top) and multi-choice (bottom). The\\nimpact of the distribution of the input text can be measured by comparing■ and ■. The gap is substantial, with an\\nexception in Direct MetaICL (discussion in Section 5.1).\\nand four multi-choice datasets as in Section 4.2.\\nSee Appendix B and Table 4 for implementation\\ndetails and example demonstrations, respectively.\\n5.1 Impact of the distribution of the input\\ntext\\nWe experiment with OOD demonstrations which\\ninclude out-of-distribution (OOD) text instead of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='details and example demonstrations, respectively.\\n5.1 Impact of the distribution of the input\\ntext\\nWe experiment with OOD demonstrations which\\ninclude out-of-distribution (OOD) text instead of\\nthe inputs from unlabeled training data. Specif-\\nically, a set of k sentences {xi,rand}k\\ni=1 are ran-\\ndomly sampled from an external corpus, and re-\\nplace x1...xk in the demonstrations. This variant\\nassesses the impact of the distribution of the input\\ntext, while keeping the label space and the format\\nof the demonstrations.\\nResults. Figure 8 shows that using out-of-\\ndistribution inputs instead of the inputs from the\\ntraining data signiﬁcantly drops the performance\\nwhen Channel MetaICL, Direct GPT-J or Channel\\nGPT-J are used, both in classiﬁcation and multi-\\nchoice, by 3–16% in absolute. In the case of Di-\\nrect GPT-J in multi-choice, it is even signiﬁcantly\\nworse than no demonstrations. Direct MetaICL\\nis an exception, which we think is the effect of\\nmeta-training (discussion in Section 5.4).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='rect GPT-J in multi-choice, it is even signiﬁcantly\\nworse than no demonstrations. Direct MetaICL\\nis an exception, which we think is the effect of\\nmeta-training (discussion in Section 5.4).\\nThis suggests that in-distribution inputs in the\\ndemonstrations substantially contribute to perfor-\\nmance gains. This is likely because conditioning on\\nthe in-distribution text makes the task closer to lan-\\nguage modeling, since the LM always conditioned\\non the in-distribution text during training.\\n5.2 Impact of the label space\\nWe also experiment with demonstrations w/ ran-\\ndom English words that use random English\\nwords as labels for all k pairs. Speciﬁcally, we\\nsample a random subset of English words Crand\\nwhere |Crand|= |C|, and randomly pair ˜yi ∈Crand\\nwith xi. This variant assesses the impact of the\\nlabel space, while keeping the distribution of the\\ninput text and the format of the demonstrations.\\nResults. Based on Figure 9, direct models and\\nchannel models exhibit different patterns. With di-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='label space, while keeping the distribution of the\\ninput text and the format of the demonstrations.\\nResults. Based on Figure 9, direct models and\\nchannel models exhibit different patterns. With di-\\nrect models, the performance gap between using\\nrandom labels within the label space and using ran-\\ndom English words is signiﬁcant, ranging between\\n5–16% absolute. This indicates that conditioning\\non the label space signiﬁcantly contributes to per-\\nformance gains. This is true even for multi-choice\\ntasks where there is no ﬁxed set of labels—we\\nhypothesize that multi-choice tasks still do have\\na particular distribution of the choices (e.g., ob-\\njects like “Bolts” or “Screws” in the OpenBookQA\\ndataset) that the model uses.\\nOn the other hand, removing the output space\\ndoes not lead to signiﬁcant drop in the channel\\nmodels: there is 0–2% drop in absolute, or some-\\ntimes even an increase. We hypothesize that this is\\nbecause the channel models only condition on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='does not lead to signiﬁcant drop in the channel\\nmodels: there is 0–2% drop in absolute, or some-\\ntimes even an increase. We hypothesize that this is\\nbecause the channel models only condition on the\\nlabels, and thus are not beneﬁting from knowing\\nthe label space. This is in contrast to direct models\\nwhich must generate the correct labels.\\n5.3 Impact of input-label pairing\\nSection 5.1 and 5.2 focus on variants which keep\\nthe format of the demonstrations as much as possi-\\nble. This section explores variants that change the\\nformat. While there are many aspects of the format,\\nwe make minimal modiﬁcations to remove the pair-\\nings of inputs to labels. Speciﬁcally, we evaluate\\ndemonstrations with no labels where the LM is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ Random English words\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nFigure 9: Impact of the label space. Evaluated in classiﬁcation (top) and multi-choice (bottom). The impact of\\nthe label space can be measured by comparing ■ and ■. The gap is signiﬁcant in the direct models but not in the\\nchannel models (discussion in Section 5.2).\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ Random labels only\\n■ Random English words\\n■ No labels'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ Random labels only\\n■ Random English words\\n■ No labels\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x13\\n\\x17\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\n\\x13\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nFigure 10: Impact of the format, i.e., the use of the input-label pairs. Evaluated in classiﬁcation (top) and multi-\\nchoice (bottom). Variants of demonstrations without keeping the format ( ■ and ■) are overall not better than no\\ndemonstrations (■). Keeping the format is especially signiﬁcant when it is possible to achieve substantial gains\\nwith the label space but without the inputs (■ vs. ■ in Direct MetaICL), or with the input distribution but without\\nthe labels (■ vs. ■ in Channel MetaICL and Channel GPT-J). More discussion in Section 5.3.\\nconditioned on the concatenation of x1...xk, and\\ndemonstrations with labels onlywhere the LM is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='the labels (■ vs. ■ in Channel MetaICL and Channel GPT-J). More discussion in Section 5.3.\\nconditioned on the concatenation of x1...xk, and\\ndemonstrations with labels onlywhere the LM is\\nconditioned on the concatenation of y1...yk. These\\nablations provide the no-format counterparts of the\\n‘demonstrations with random English words’ and\\n‘demonstrations with OOD inputs’, respectively.\\nResults. Based on Figure 10, removing the for-\\nmat is close to or worse than no demonstrations,\\nindicating the importance of the format. This is\\nlikely because conditioning on a sequence of input-\\nlabel pairs triggers the model to mimic the overall\\nformat and complete the new example as expected\\nwhen the test input is given.\\nMore interestingly, keeping the format plays a\\nsigniﬁcant role in retaining a large portion of per-\\nformance gains by only using the inputs or only\\nusing the labels. For instance, with Direct MetaICL,\\nit is possible to retain 95% and 82% of improve-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='formance gains by only using the inputs or only\\nusing the labels. For instance, with Direct MetaICL,\\nit is possible to retain 95% and 82% of improve-\\nments from in-context learning (demonstrations\\nwith gold labels) by simply sampling random sen-\\ntences from a corpus and randomly pairing them\\nwith the label set (■ in Figure 10) in classiﬁcation\\nand multi-choice, respectively. Similarly, with the\\nchannel models, it is possible to retain 82%, 87%,\\n86% and 75% of improvements from in-context\\nlearning by simply pairing each input from the un-\\nlabeled training data with a random English word\\n(■ in Figure 10) in MetaICL classiﬁcation, GPT-\\nJ classiﬁcation, MetaICL multi-choice and GPT-J\\nmulti-choice, respectively. For all of these cases,\\nremoving inputs instead of using OOD inputs, or\\nremoving labels instead of using random English\\nwords is signiﬁcantly worse, indicating that keep-\\ning the format of the input-label pairs is key.\\n5.4 Impact of meta-training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='removing labels instead of using random English\\nwords is signiﬁcantly worse, indicating that keep-\\ning the format of the input-label pairs is key.\\n5.4 Impact of meta-training\\nDifferent from other models, MetaICL is trained\\nwith an in-context learning objective, in line with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='recent work that uses multi-task training on a\\nlarge collection of supervised datasets (called meta-\\ntraining) for generalization to new tasks (Agha-\\njanyan et al., 2021; Khashabi et al., 2020; Wei\\net al., 2022a; Sanh et al., 2022). We aim to better\\nunderstand the role of this meta-training in relation\\nwith our ﬁndings by closely examining the result of\\nMetaICL. In particular, we observe that the patterns\\nwe see so far are signiﬁcantly more evident with\\nMetaICL than with other models. For instance, the\\nground truth input-label mapping matters even less,\\nand keeping the format of the demonstrations mat-\\nters even more. There is nearly zero inﬂuence of\\nthe input-label mapping and the input distribution\\nin Direct MetaICL, and the input-label mapping\\nand the output space in Channel MetaICL.\\nBased on this observation, we hypothesize that\\nmeta-training encourages the model to exclu-\\nsively exploit simpler aspects of the demonstra-\\ntions and to ignore others. This is based on our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Based on this observation, we hypothesize that\\nmeta-training encourages the model to exclu-\\nsively exploit simpler aspects of the demonstra-\\ntions and to ignore others. This is based on our\\nintuition that (1) the input-label mapping is likely\\nharder to exploit, (2) the format is likely easier to\\nexploit, and (3) the space of the text that the model\\nis trained to generate is likely easier to exploit than\\nthe space of the text that the model conditions on.8\\n6 Discussion & Conclusion\\nIn this paper, we study the role of the demonstra-\\ntions with respect to the success of in-context learn-\\ning. We ﬁnd that the ground truth input-label map-\\nping in the demonstrations matters signiﬁcantly\\nless than one might think—replacing gold labels\\nwith random labels in the demonstrations only\\nmarginally lowers the performance. We then iden-\\ntify a series of aspects in the demonstrations and\\nexamine which aspect actually contributes to per-\\nformance gains. Results reveal that (1) gains are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='marginally lowers the performance. We then iden-\\ntify a series of aspects in the demonstrations and\\nexamine which aspect actually contributes to per-\\nformance gains. Results reveal that (1) gains are\\nmainly coming from independent speciﬁcation of\\nthe input space and the label space, (2) the models\\ncan still retain up to 95% of performance gains by\\nusing either the inputs only or the label set only if\\nthe right format is used, and (3) meta-training with\\nan in-context learning objective magniﬁes these\\ntrends. Together, our ﬁndings lead to a set of\\nbroader indications about in-context learning, as\\nwell as avenues for future work.\\nDoes the model learn at test time? If we take\\na strict deﬁnition of learning: capturing the input-\\n8That is, the direct model exploits the label space better\\nthan the input distribution, and the channel model exploits the\\ninput distribution better than the label space.\\nlabel correspondence given in the training data,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='than the input distribution, and the channel model exploits the\\ninput distribution better than the label space.\\nlabel correspondence given in the training data,\\nthen our ﬁndings suggest that LMs do not learn\\nnew tasks at test time. Our analysis shows that the\\nmodel may ignore the task deﬁned by the demon-\\nstrations and instead use prior from pretraining.\\nHowever, learning a new task can be interpreted\\nmore broadly: it may include adapting to speciﬁc\\ninput and label distributions and the format sug-\\ngested by the demonstrations, and ultimately get-\\nting to make a prediction more accurately. With\\nthis deﬁnition of learning, the model does learn\\nthe task from the demonstrations. Our experiments\\nindicate that the model does make use of aspects of\\nthe demonstrations and achieve performance gains.\\nCapacity of LMs. The model performs a down-\\nstream task without relying on the input-label corre-\\nspondence from the demonstrations. This suggests'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='the demonstrations and achieve performance gains.\\nCapacity of LMs. The model performs a down-\\nstream task without relying on the input-label corre-\\nspondence from the demonstrations. This suggests\\nthat the model has learned the (implicit notion of)\\ninput-label correspondence from the language mod-\\neling objective alone, e.g., associating a positive\\nreview with the word ‘positive’. This is in line\\nwith Reynolds and McDonell (2021) who claim\\nthat the demonstrations are for task location and\\nthe intrinsic ability to perform the task is obtained\\nat pretraining time.9\\nOn one hand, this suggests that the language\\nmodeling objective has led to great zero-shot ca-\\npacity, even if it is not always evident from the\\nnaive zero-shot accuracy. On the other hand, this\\nsuggests that in-context learning may not work on\\na task whose input-label correspondence is not al-\\nready captured in the LM. This leads to the research\\nquestion of how to make progress in NLP problems'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='a task whose input-label correspondence is not al-\\nready captured in the LM. This leads to the research\\nquestion of how to make progress in NLP problems\\nthat in-context learning does not solve: whether\\nwe need a better way of extracting the input-label\\nmappings that are already stored in the LM, a bet-\\nter variant of the LM objective that learns a wider\\nrange of task semantics, or explicit supervision\\nthrough ﬁne-tuning on the labeled data.\\nConnection to instruction-following models.\\nPrior work has found it promising to train the model\\nthat reads the natural language description of the\\ntask (called instructions) and performs a new task\\nat inference (Mishra et al., 2021b; Efrat and Levy,\\n2020; Wei et al., 2022a; Sanh et al., 2022). We\\nthink the demonstrations and instructions largely\\nhave the same role to LMs, and hypothesize that our\\n9However, while Reynolds and McDonell (2021) claims\\nthat the demonstrations are thus unnecessary, we think using'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='have the same role to LMs, and hypothesize that our\\n9However, while Reynolds and McDonell (2021) claims\\nthat the demonstrations are thus unnecessary, we think using\\nthe demonstrations is actually the most unambiguous and the\\neasiest way to prompt the model to perform a task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='ﬁndings hold for instruction-following models: the\\ninstructions prompt the model to recover the capac-\\nity it already has, but do not supervise the model to\\nlearn novel task semantics. This has been partially\\nveriﬁed by Webson and Pavlick (2022) who showed\\nthat the model performance does not degrade much\\nwith irrelevant or misleading instructions. We leave\\nmore analysis on instruction-following models for\\nfuture work.\\nSigniﬁcantly improved zero-shot performance.\\nOne of our key ﬁndings is that it is possible to\\nachieve nearly k-shot performance without using\\nany labeled data, by simply pairing each unlabeled\\ninput with a random label and using it as the demon-\\nstrations. This means our zero-shot baseline level\\nis signiﬁcantly higher than previously thought. 10\\nFuture work can further improve the zero-shot per-\\nformance with relaxed assumptions in access to the\\nunlabeled training data.\\nLimitation\\nEffect of types of tasks and datasets. This pa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Future work can further improve the zero-shot per-\\nformance with relaxed assumptions in access to the\\nunlabeled training data.\\nLimitation\\nEffect of types of tasks and datasets. This pa-\\nper focuses on the tasks from established NLP\\nbenchmarks that have real natural language inputs.\\nSynthetic tasks with more limited inputs may actu-\\nally use the ground truth labels more, as observed\\nby Rong (2021).\\nWe report macro-level analysis by examining the\\naverage performance over multiple NLP datasets,\\nbut different datasets may behave differently. Ap-\\npendix C.2 discusses this aspect, including ﬁnd-\\nings that there are larger gaps between using the\\nground truth labels and using the random labels\\nin some dataset-model pairs (e.g., in the most\\nextreme case, nearly 14% absolute on the ﬁnan-\\ncial_phrasebank dataset with GPT-J). Since the ﬁrst\\nversion of our paper, Kim et al. (2022) showed\\nthat using negated labels substantially lowers the\\nperformance in classiﬁcation. 11 We believe it is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='cial_phrasebank dataset with GPT-J). Since the ﬁrst\\nversion of our paper, Kim et al. (2022) showed\\nthat using negated labels substantially lowers the\\nperformance in classiﬁcation. 11 We believe it is\\nimportant to understand to what extend the model\\nneeds the ground truth labels to successfully per-\\nform in-context learning.\\nExtensions to generation. Our experiments are\\nlimited to classiﬁcation and multi-choice tasks. We\\nhypothesize that ground truth output may not be\\n10We take the perspective that using the unlabeled training\\ndata is permitted (Kodirov et al., 2015; Wang et al., 2019b;\\nSchick and Schütze, 2021).\\n11Note that Kim et al. (2022) estimate the random label per-\\nformance by interpolating with the performance using negated\\nlabels, while our paper samples the random labels at uniform.\\nnecessary for in-context learning in the open-set\\ntasks such as generation, but leave this to future\\nwork. Extending of our experiments to such tasks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='necessary for in-context learning in the open-set\\ntasks such as generation, but leave this to future\\nwork. Extending of our experiments to such tasks\\nis not trivial, because it requires a variation of the\\noutput which has incorrect input-output correspon-\\ndence while keeping the correct output distribution\\n(which is important based on our analysis in Sec-\\ntion 5).\\nSince the ﬁrst version of our paper, Madaan and\\nYazdanbakhsh (2022) conducted a similar analy-\\nsis with the chain of thought prompting (Wei et al.,\\n2022b) which generates a rationale to perform com-\\nplex tasks such as math problems. Madaan and\\nYazdanbakhsh (2022) show that, while simply us-\\ning a random rationale in the demonstrations (e.g.,\\npairing with a rationale from a different example)\\nsigniﬁcantly degrades the performance, other types\\nof counterfactual rationales (e.g., wrong equations)\\ndo not degrade the performance as much as we\\nthought. We refer to Madaan and Yazdanbakhsh'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='signiﬁcantly degrades the performance, other types\\nof counterfactual rationales (e.g., wrong equations)\\ndo not degrade the performance as much as we\\nthought. We refer to Madaan and Yazdanbakhsh\\n(2022) for more discussions on what aspects of the\\nrationale matter or do not matter.\\nAcknowledgements\\nWe thank Gabriel Ilharco, Julian Michael, Oﬁr\\nPress, UW NLP members and anonymous review-\\ners for their comments in the paper. This research\\nwas supported by NSF IIS-2044660, ONR N00014-\\n18-1-2826, a Sloan fellowship and gifts from AI2.\\nReferences\\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivas-\\ntava, Xilun Chen, Luke Zettlemoyer, and Sonal\\nGupta. 2021. Muppet: Massive multi-task rep-\\nresentations with pre-ﬁnetuning. arXiv preprint\\narXiv:2101.11038.\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\\net al. 2021. Efﬁcient large scale language mod-\\neling with mixtures of experts. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\\net al. 2021. Efﬁcient large scale language mod-\\neling with mixtures of experts. arXiv preprint\\narXiv:2112.10684.\\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\\nDanilo Giampiccolo, Bernardo Magnini, and Idan\\nSzpektor. 2006. The second pascal recognising tex-\\ntual entailment challenge. In Proceedings of the sec-\\nond PASCAL challenges workshop on recognising\\ntextual entailment.\\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\\npinosa Anke, and Leonardo Neves. 2020. TweetE-\\nval: Uniﬁed benchmark and comparative evaluation\\nfor tweet classiﬁcation. In Findings of EMNLP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\\nGiampiccolo. 2009. The ﬁfth pascal recognizing tex-\\ntual entailment challenge. In TAC.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\\nV oss, Gretchen Krueger, Tom Henighan, Rewon\\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\\nClemens Winter, Chris Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In\\nNeurIPS.\\nMichael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-\\nnandez, and Doug Downey. 2019. CODAH: An\\nadversarially-authored question answering dataset\\nfor common sense. In Proceedings of the 3rd Work-\\nshop on Evaluating Vector Space Representations\\nfor NLP.\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='for common sense. In Proceedings of the 3rd Work-\\nshop on Evaluating Vector Space Representations\\nfor NLP.\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,\\nand He He. 2021. Meta-learning via language model\\nin-context tuning. arXiv preprint arXiv:2110.07814.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\\nAshish Sabharwal, Carissa Schoenick, and Oyvind\\nTafjord. 2018. Think you have solved question an-\\nswering? try arc, the ai2 reasoning challenge. ArXiv.\\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\\n2005. The pascal recognising textual entailment\\nchallenge. In Machine Learning Challenges Work-\\nshop.\\nOna de Gibert, Naiara Perez, Aitor García-Pablos, and\\nMontse Cuadros. 2018. Hate Speech Dataset from a\\nWhite Supremacy Forum. In Proceedings of the 2nd\\nWorkshop on Abusive Language Online (ALW2).\\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\\ndith Tonhauser. 2019. The commitmentbank: Inves-\\ntigating projection in naturally occurring discourse.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Workshop on Abusive Language Online (ALW2).\\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\\ndith Tonhauser. 2019. The commitmentbank: Inves-\\ntigating projection in naturally occurring discourse.\\nProceedings of Sinn und Bedeutung.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In NAACL.\\nT. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leippold.\\n2020. Climate-fever: A dataset for veriﬁcation of\\nreal-world climate claims. ArXiv.\\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\nAvia Efrat and Omer Levy. 2020. The turking test: Can\\nlanguage models understand instructions? arXiv\\npreprint arXiv:2010.11982.\\nL Gao, S Biderman, S Black, L Golding, T Hoppe,\\nC Foster, J Phang, H He, A Thite, N Nabeshima,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='language models understand instructions? arXiv\\npreprint arXiv:2010.11982.\\nL Gao, S Biderman, S Black, L Golding, T Hoppe,\\nC Foster, J Phang, H He, A Thite, N Nabeshima,\\net al. 2021. The pile: an 800gb dataset of diverse\\ntext for language modeling 2020. arXiv preprint\\narXiv:2101.00027.\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\\nand Bill Dolan. 2007. The third pascal recognizing\\ntextual entailment challenge. In Proceedings of the\\nACL-PASCAL workshop on textual entailment and\\nparaphrasing.\\nAndrew Gordon, Zornitsa Kozareva, and Melissa\\nRoemmele. 2012. SemEval-2012 task 7: Choice\\nof plausible alternatives: An evaluation of common-\\nsense causal reasoning. In The First Joint Confer-\\nence on Lexical and Computational Semantics (Se-\\nmEval).\\nAri Holtzman, Peter West, Vered Schwartz, Yejin Choi,\\nand Luke Zettlemoyer. 2021. Surface form compe-\\ntition: Why the highest probability answer isn’t al-\\nways right. In EMNLP.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='and Luke Zettlemoyer. 2021. Surface form compe-\\ntition: Why the highest probability answer isn’t al-\\nways right. In EMNLP.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\\nnaneh Hajishirzi. 2020. UniﬁedQA: Crossing for-\\nmat boundaries with a single qa system. In Findings\\nof EMNLP.\\nTushar Khot, Peter Clark, Michal Guerquin, Peter\\nJansen, and Ashish Sabharwal. 2020. Qasc: A\\ndataset for question answering via sentence compo-\\nsition. In AAAI.\\nJunyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho,\\nHwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\\nKang Min Yoo, and Taeuk Kim. 2022. Ground-truth\\nlabels matter: A deeper look into input-label demon-\\nstrations. arXiv preprint arXiv:2205.12685.\\nElyor Kodirov, Tao Xiang, Zhenyong Fu, and Shao-\\ngang Gong. 2015. Unsupervised domain adaptation\\nfor zero-shot learning. In Proceedings of the IEEE\\ninternational conference on computer vision.\\nHector J. Levesque, Ernest Davis, and Leora Morgen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='gang Gong. 2015. Unsupervised domain adaptation\\nfor zero-shot learning. In Proceedings of the IEEE\\ninternational conference on computer vision.\\nHector J. Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2012. The winograd schema challenge. In\\nProceedings of the Thirteenth International Confer-\\nence on Principles of Knowledge Representation\\nand Reasoning.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In ACL.\\nQuentin Lhoest, Albert Villanova del Moral, Yacine\\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\\nPatry, Angelina McMillan-Major, Philipp Schmid,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Sylvain Gugger, Clément Delangue, Théo Matus-\\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\\ntac, Thibault Goehringer, Victor Mustar, François\\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\\nDatasets: A community library for natural language\\nprocessing. In EMNLP: System Demonstrations.\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\\nLawrence Carin, and Weizhu Chen. 2021. What\\nmakes good in-context examples for gpt- 3? arXiv\\npreprint arXiv:2101.06804.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\nRobert L Logan IV , Ivana Balaževic, Eric Wallace,\\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\\n2021. Cutting down on prompts and parameters:\\nSimple few-shot learning with language models.\\narXiv preprint arXiv:2106.13353.\\nYao Lu, Max Bartolo, Alastair Moore, Sebastian'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='2021. Cutting down on prompts and parameters:\\nSimple few-shot learning with language models.\\narXiv preprint arXiv:2106.13353.\\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\\nRiedel, and Pontus Stenetorp. 2021. Fantastically\\nordered prompts and where to ﬁnd them: Overcom-\\ning few-shot prompt order sensitivity.arXiv preprint\\narXiv:2104.08786.\\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\\nand patterns: For effective chain of thought, it takes\\ntwo to tango. arXiv preprint arXiv:2209.07686.\\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-\\nlenius, and Pyry Takala. 2014. Good debt or bad\\ndebt: Detecting semantic orientations in economic\\ntexts. J. Assoc. Inf. Sci. Technol.\\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\\nelli. 2014. A SICK cure for the evaluation of com-\\npositional distributional semantic models. In LREC.\\nClara H. McCreery, Namit Katariya, Anitha Kannan,\\nManish Chablani, and Xavier Amatriain. 2020. Ef-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='positional distributional semantic models. In LREC.\\nClara H. McCreery, Namit Katariya, Anitha Kannan,\\nManish Chablani, and Xavier Amatriain. 2020. Ef-\\nfective transfer learning for identifying similar ques-\\ntions: Matching user questions to covid-19 faqs.\\nIn Proceedings of the 26th ACM SIGKDD Interna-\\ntional Conference on Knowledge Discovery & Data\\nMining.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\\nSabharwal. 2018. Can a suit of armor conduct elec-\\ntricity? a new dataset for open book question answer-\\ning. In EMNLP.\\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2021a. Noisy channel language\\nmodel prompting for few-shot text classiﬁcation.\\narXiv preprint arXiv:2108.04106.\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2021b. MetaICL: Learning to\\nlearn in context. arXiv preprint.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\\nChoi, and Hannaneh Hajishirzi. 2021a. Refram-\\ning instructional prompts to gptk’s language. arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='learn in context. arXiv preprint.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\\nChoi, and Hannaneh Hajishirzi. 2021a. Refram-\\ning instructional prompts to gptk’s language. arXiv\\npreprint arXiv:2109.07830.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\\nHannaneh Hajishirzi. 2021b. Cross-task generaliza-\\ntion via natural language crowdsourcing instructions.\\narXiv preprint arXiv:2104.08773.\\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\\nand Grigorios Tsoumakas. 2020. Ethos: an online\\nhate speech detection dataset. ArXiv.\\nSebastian Nagel. 2016. CC-News. http:\\n//web.archive.org/save/http:\\n//commoncrawl.org/2016/10/\\nnews-dataset-available.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nblog.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='blog.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. Journal of Machine Learning Research.\\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\\nand Sameer Singh. 2022. Impact of pretraining term\\nfrequencies on few-shot reasoning. arXiv preprint\\narXiv:2202.07206.\\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\\ngramming for large language models: Beyond the\\nfew-shot paradigm. In Extended Abstracts of the\\n2021 CHI Conference on Human Factors in Com-\\nputing Systems.\\nFrieda Rong. 2021. Extrapolating to unnatu-\\nral language processing with gpt-3’s in-context\\nlearning: The good, the bad, and the myste-\\nrious. https://ai.stanford.edu/blog/\\nin-context-learning.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='in-context-learning.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun\\nRaja, Manan Dey, M Saiful Bari, Canwen Xu, Ur-\\nmish Thakker, Shanya Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\\nJiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\\nGao, Tali Bers, Thomas Wolf, and Alexander M.\\nRush. 2022. Multitask prompted training enables\\nzero-shot task generalization. In ICLR.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Timo Schick and Hinrich Schütze. 2021. It’s not just\\nsize that matters: Small language models are also\\nfew-shot learners. In NAACL-HLT.\\nEmily Sheng and David Uthus. 2020. Investigating so-\\ncietal biases in a poetry composition system. In Pro-\\nceedings of the Second Workshop on Gender Bias in\\nNatural Language Processing.\\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\\nand Claire Cardie. 2019. DREAM: A challenge data\\nset and models for dialogue-based reading compre-\\nhension. TACL.\\nOyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau\\nYih, and Ashish Sabharwal. 2019a. Quarel: A\\ndataset and models for answering questions about\\nqualitative relationships. In AAAI.\\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter\\nClark. 2019b. QuaRTz: An open-domain dataset of\\nqualitative relationship questions. In EMNLP.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A ques-\\ntion answering challenge targeting commonsense\\nknowledge. In NAACL-HLT.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Alon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A ques-\\ntion answering challenge targeting commonsense\\nknowledge. In NAACL-HLT.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman. 2019a. Superglue: A\\nstickier benchmark for general-purpose language un-\\nderstanding systems. In NeurIPS.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R Bowman. 2018.\\nGlue: A multi-task benchmark and analysis plat-\\nform for natural language understanding. In Black-\\nboxNLP Workshop: Analyzing and Interpreting Neu-\\nral Networks for NLP.\\nBen Wang and Aran Komatsuzaki. 2021. GPT-\\nJ-6B: A 6 Billion Parameter Autoregressive\\nLanguage Model. https://github.com/\\nkingoflolz/mesh-transformer-jax.\\nWei Wang, Vincent W Zheng, Han Yu, and Chunyan\\nMiao. 2019b. A survey of zero-shot learning: Set-\\ntings, methods, and applications. ACM Transactions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='kingoflolz/mesh-transformer-jax.\\nWei Wang, Vincent W Zheng, Han Yu, and Chunyan\\nMiao. 2019b. A survey of zero-shot learning: Set-\\ntings, methods, and applications. ACM Transactions\\non Intelligent Systems and Technology (TIST).\\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\\nbased models really understand the meaning of their\\nprompts? In NAACL-HLT.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\\ndrew M Dai, and Quoc V Le. 2022a. Finetuned lan-\\nguage models are zero-shot learners. In ICLR.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. arXiv preprint arXiv:2201.11903.\\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\\nand Tengyu Ma. 2022. An explanation of in-context\\nlearning as implicit bayesian inference. In ICLR.\\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\\nCrossﬁt: A few-shot learning challenge for cross-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='and Tengyu Ma. 2022. An explanation of in-context\\nlearning as implicit bayesian inference. In ICLR.\\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\\nCrossﬁt: A few-shot learning challenge for cross-\\ntask generalization in nlp. In EMNLP.\\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\\nSameer Singh. 2021. Calibrate before use: Improv-\\ning few-shot performance of language models. In\\nICML.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='A Full Datasets\\nWe include 26 datasets as follows: ﬁ-\\nnancial_phrasebank (Malo et al., 2014),\\npoem_sentiment (Sheng and Uthus, 2020),\\nmedical_questions_pairs (McCreery et al., 2020),\\nglue-mrpc (Dolan and Brockett, 2005), glue-\\nwnli (Levesque et al., 2012), climate_fever (Diggel-\\nmann et al., 2020), glue-rte (Dagan et al., 2005;\\nBar-Haim et al., 2006; Giampiccolo et al.,\\n2007; Bentivogli et al., 2009), superglue-\\ncb (de Marneffe et al., 2019), sick (Marelli et al.,\\n2014) , hate_speech18 (de Gibert et al., 2018),\\nethos-national_origin (Mollas et al., 2020), ethos-\\nrace (Mollas et al., 2020), ethos-religion (Mollas\\net al., 2020), tweet_eval-hate (Barbieri et al., 2020),\\ntweet_eval-stance_atheism (Barbieri et al., 2020),\\ntweet_eval-stance_feminist (Barbieri et al., 2020),\\nquarel (Tafjord et al., 2019a), openbookqa (Mi-\\nhaylov et al., 2018), qasc (Khot et al., 2020), com-\\nmonsense_qa (Talmor et al., 2019), ai2_arc (Clark\\net al., 2018), codah (Chen et al., 2019), superglue-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='haylov et al., 2018), qasc (Khot et al., 2020), com-\\nmonsense_qa (Talmor et al., 2019), ai2_arc (Clark\\net al., 2018), codah (Chen et al., 2019), superglue-\\ncopa (Gordon et al., 2012), dream (Sun et al.,\\n2019), quartz-with_knowledge (Tafjord et al.,\\n2019b), quartz-no_knowledge (Tafjord et al.,\\n2019b). The choice of datasets is made following\\nlow-resource datasets in Min et al. (2021b), with\\nthe exact same set of k-shot train data using 5\\nrandom seeds. We use the HuggingFace version\\nof the data (Lhoest et al., 2021) and use the\\ndevelopment data for evaluation, following Ye\\net al. (2021). See Table 2 for statistics.\\nB Experimental Details\\nExample template We follow Ye et al. (2021);\\nMin et al. (2021b); Logan IV et al. (2021) in us-\\ning the minimal format to transform the input to a\\nsequence (e.g. a concatenation of multiple inputs)\\nand using the label words from each dataset as it is.\\nWe also explore manual templates taken from prior\\nwork (Holtzman et al., 2021; Zhao et al., 2021) as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='and using the label words from each dataset as it is.\\nWe also explore manual templates taken from prior\\nwork (Holtzman et al., 2021; Zhao et al., 2021) as\\nreported in Section 4.2, although we ﬁnd that using\\nthese templates is not consistently better than using\\nminimal templates. We thus run main experiments\\nwith minimal templates. Example templates are\\nprovided in Table 3.\\nFormat of the demonstrations We follow the\\nstandard of each model for formatting the demon-\\nstrations, either from exploration in prior work or\\nthe example code provided in the ofﬁcial tutorial.\\nFor GPT-2, we separate the input and the label,\\nDataset # Train # Eval\\nTask category: Sentiment analysis\\nﬁnancial_phrasebank 1,811 453\\npoem_sentiment 892 105\\nTask category: Paraphrase detection\\nmedical_questions_pairs 2,438 610\\nglue-mrpc 3,668 408\\nTask category: Natural language inference\\nglue-wnli 635 71\\nclimate_fever 1,228 307\\nglue-rte 2,490 277\\nsuperglue-cb 250 56\\nsick 4,439 495\\nTask category: Hate speech detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='glue-mrpc 3,668 408\\nTask category: Natural language inference\\nglue-wnli 635 71\\nclimate_fever 1,228 307\\nglue-rte 2,490 277\\nsuperglue-cb 250 56\\nsick 4,439 495\\nTask category: Hate speech detection\\nhate_speech18 8,562 2,141\\nethos-national_origin 346 87\\nethos-race 346 87\\nethos-religion 346 87\\ntweet_eval-hate 8,993 999\\ntweet_eval-stance_atheism 461 52\\ntweet_eval-stance_feminist 597 67\\nTask category: Question answering\\nquarel 1,941 278\\nopenbookqa 4,957 500\\nqasc 8,134 926\\ncommonsense_qa 9,741 1,221\\nai2_arc 1,119 299\\nTask category: Sentence completion\\ncodah 1665 556\\nsuperglue-copa 400 100\\ndream 6116 2040\\nquartz-with_knowledge 2696 384\\nquartz-no_knowledge 2696 384\\nTable 2: 26 datasets used for experiments, classiﬁed\\ninto 6 task categories. # Train and # Test indicate the\\nnumber of training and test examples of the dataset.\\nNote that # train is based on the original training dataset\\nbut we use k random samples for k-shot evaluation.\\nand each demonstration example with a space. For'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Note that # train is based on the original training dataset\\nbut we use k random samples for k-shot evaluation.\\nand each demonstration example with a space. For\\nMetaICL, GPT-J and GPT-3, we separate the input\\nand the label with a newline (\\\\n), and each demon-\\nstration example with three newlines. For fairseq\\nmodels, we use a newline to separate the input and\\nthe label as well as each demonstration example.\\nDetails in variants of the demonstrations For\\n“demonstrations w/ a% accurate labels” ( 0 ≤\\na ≤ 100), we use k ×a/100 correct pairs and\\nk ×(1 −a/100) incorrect pairs in a random order,\\nas described in Algorithm 1. For “OOD demon-\\nstrations”, we use CC-News (Nagel, 2016) as an\\nexternal corpus. We consider the length of the text\\nduring sampling, so that sampled sentences have\\nsimilar length to the test input. For “demonstrations\\nwith random English words”, we use pypi.org/\\nproject/english-words for the set of En-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nDirect\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 11: Results of No-demonstration, Gold demonstration and Random demonstration on 3 classiﬁcation\\ndatasets (top) and 3 multi-choice datasets (bottom). Details in Section 4.1. This ﬁgure is for providing numbers\\nthat are comparable across models—full results with more datasets are reported in Figure 3.\\nAlgorithm 1 Forming the demonstrations with an\\naccuracy of a%.\\n1: procedure FORM DEMONS ({(xi, yi)}k'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='that are comparable across models—full results with more datasets are reported in Figure 3.\\nAlgorithm 1 Forming the demonstrations with an\\naccuracy of a%.\\n1: procedure FORM DEMONS ({(xi, yi)}k\\ni=1, a)\\n2: D ←[] // demonstration to be formed\\n3: n ←k ×a/100 // number of correct pairs\\n4: G← Sample(Range(1, k), n)\\n5: for i ∈Range(1, k) do\\n6: if i ∈G then // add correct pair\\n7: D.append((xi, yi))\\n8: else // add incorrect pair\\n9: D.append((xi, Sample(C−yi)))\\n10: return D\\nglish words, which consists of 61,569 words.\\nTable 4 provides a list of example demonstra-\\ntions for each method used in Section 5.\\nC More Experimental Results\\nC.1 Gold labels vs. random labels\\nFigure 11 shares the same interface as Figure 3, but\\nall models are evaluated on 3 classiﬁcation and 3\\nmulti-choice datasets and are thus comparable to\\neach other.\\nC.2 Random labels from true distribution of\\nlabels & Task breakdown\\nIn Section 4, random labels are sampled from the\\nlabel space from a uniform distribution. We ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='each other.\\nC.2 Random labels from true distribution of\\nlabels & Task breakdown\\nIn Section 4, random labels are sampled from the\\nlabel space from a uniform distribution. We ex-\\nperiment with another variant of demonstrations in\\nthe classiﬁcation tasks, where labels are randomly\\nsampled from the true distribution of labels on the\\ntraining data. This may have large impact if labels\\nare far from uniform on the training data. Results\\nindicate that performance drop from using gold\\nlabels is further reduced compared to using uni-\\nformly random labels: with Channel MetaICL, the\\ngap is reduced from 1.9% to 1.3% absolute, and\\nwith Channel GPT-J, the gap is reduced from 5.0%\\nto 3.5% absolute.\\nFigure 12 shows performance gap between using\\ngold labels and using random labels per dataset. We\\nﬁnd that the trend that the gap is smaller than pre-\\nviously thought is consistant across most datasets.\\nNonetheless, there are a few outlier datasets where\\nperformance gap is non-negligible, such as ﬁnan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='viously thought is consistant across most datasets.\\nNonetheless, there are a few outlier datasets where\\nperformance gap is non-negligible, such as ﬁnan-\\ncial_phrasebank and a few hate speech detection\\ndatasets. Future work may investigate on which\\ntasks the model makes more use of the correctly\\npaired training data.\\nC.3 More variants of the demonstrations\\nWe explored demonstrations with a con-\\nstant label where all labels in the demon-\\nstrations are replaced with a constant text,\\n“answer”. Speciﬁcally, a prediction is made via\\nargmaxy∈CP(y|x1, answer...xk, answer, x).\\nThis can be viewed as another way to remove the\\nimpact of the label space while keeping the impact\\nof the distribution of the input text. However,\\nresults are consistently worse than the results\\nof demonstrations with random English labels.\\nWe think this is because constant labels actually\\nchange the format of the demonstrations, since\\nthey can be viewed as part of a separator between\\ndifferent demonstration examples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='We think this is because constant labels actually\\nchange the format of the demonstrations, since\\nthey can be viewed as part of a separator between\\ndifferent demonstration examples.\\nWe also exploreddemonstrations with the test\\ninput where all inputs in the demonstrations are\\nreplaced with the test input, each paired with a ran-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='dom label. Speciﬁcally, a prediction is made via\\nargmaxy∈CP(y|x, ˜y1...x, ˜yk, x), where ˜yi (1 ≤\\ni ≤k) is randomly sampled at uniform from C.\\nThis variant is seemingly a reasonable choice given\\nthat it satisﬁes the condition that the inputs in the\\ndemonstrations come from the same distribution\\nas the test input (since they are identical), and us-\\ning random labels is as good as using gold labels.\\nNonetheless, we ﬁnd that this variant is signiﬁ-\\ncantly worse than most other methods with demon-\\nstrations. We think this is because using the con-\\nstant input for all demonstration example signiﬁ-\\ncantly changes the format of the sequence, since the\\ninput can be viewed as part of a separator between\\ndifferent demonstration examples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='financial_phrasebank\\ntweet_eval-stance_feminist\\npoem_sentiment-new\\nsuperglue-cb\\nethos-national_origin\\ntweet_eval-stance_atheism\\nsick\\nhate_speech18\\nglue-wnli\\nethos-religion\\nethos-race\\ncodah quarel\\ncommonsense_qa\\nmedical_questions_pairs\\nopenbookqa\\ntweet_eval-hate\\nquartz-with_knowledge\\nsuperglue-copa-new\\nclimate_fever\\nqasc dream ai2_arc\\nglue-mrpc\\nquartz-no_knowledge\\nglue-rte\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nPerformance drop (Channel MetaICL, uniform)\\nfinancial_phrasebank\\nsuperglue-cb\\npoem_sentiment-new\\nethos-race\\nsick\\ntweet_eval-stance_feminist\\ntweet_eval-hatehate_speech18ethos-religion\\nglue-rte\\nclimate_fever\\nethos-national_origin\\nglue-wnli\\nquartz-with_knowledge\\ncommonsense_qa\\nqasc\\nquartz-no_knowledge\\nopenbookqa\\ndream\\ntweet_eval-stance_atheism\\nai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nglue-mrpc\\nquarel codah\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nPerformance drop (Channel GPT -J, uniform)\\ntweet_eval-stance_feminist\\npoem_sentiment-newfinancial_phrasebank\\ntweet_eval-stance_atheism'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='glue-mrpc\\nquarel codah\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nPerformance drop (Channel GPT -J, uniform)\\ntweet_eval-stance_feminist\\npoem_sentiment-newfinancial_phrasebank\\ntweet_eval-stance_atheism\\nethos-national_origin\\nsuperglue-cbethos-religion\\nglue-wnli\\nsick codah\\nclimate_fever\\nquarel\\ntweet_eval-hate\\nethos-race\\ncommonsense_qa\\nglue-mrpc\\nhate_speech18\\nmedical_questions_pairs\\nopenbookqa\\nquartz-with_knowledge\\nsuperglue-copa-new\\nqasc dream ai2_arc\\nquartz-no_knowledge\\nglue-rte\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\nPerformance drop (Channel MetaICL, true distribution)\\nsick\\npoem_sentiment-new\\nethos-race\\ntweet_eval-hate\\ntweet_eval-stance_feminist\\nfinancial_phrasebank\\nhate_speech18\\ntweet_eval-stance_atheism\\nclimate_fever\\nethos-national_origin\\nglue-rte\\nethos-religion\\nglue-wnli\\nquartz-with_knowledge\\ncommonsense_qa\\nqasc\\nglue-mrpc\\nquartz-no_knowledge\\nopenbookqa\\ndream ai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nquarel codah\\nsuperglue-cb\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='quartz-with_knowledge\\ncommonsense_qa\\nqasc\\nglue-mrpc\\nquartz-no_knowledge\\nopenbookqa\\ndream ai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nquarel codah\\nsuperglue-cb\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nPerformance drop (Channel GPT -J, true distribution)\\nFigure 12: Performance gap from using the demonstrations with gold labels to using the demonstrations with\\nrandom labels. Datasets are sorted in descending order. The top two ﬁgures use random labels that are sampled\\nat uniform, with Channel MetaICL and Channel GPT-J, respectively. The bottom two ﬁgures use random labels\\nthat are sampled from a true distribution of labels on the training data, with Channel MetaICL and Channel GPT-J,\\nrespectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Dataset Type Example\\nMRPC Minimal sentence 1: Cisco pared spending to compensate for sluggish sales . [SEP] sentence 2: In\\nresponse to sluggish sales , Cisco pared spending . \\\\n {equivalent|not_equivalent}\\nManual Cisco pared spending to compensate for sluggish sales . \\\\n The question is: In response to\\nsluggish sales , Cisco pared spending . True or False? \\\\n The answer is:{True|False}\\nRTE Minimal sentence 1: The girl was found in Drummondville. [SEP] sentence 2: Drummondville\\ncontains the girl. \\\\n {entailment|not_entailment}\\nManual The girl was found in Drummondville. \\\\n The question is: Drummondville contains the\\ngirl. True or False? \\\\n The answer is:{True|False}\\nTweet_eval-hate Minimal The Truth about #Immigration \\\\n {hate |non-hate}\\nManual Tweet: The Truth about #Immigration \\\\n Sentiment: {against |favor}\\nSICK Minimal sentence 1: A man is screaming. [SEP] sentence 2: A man is scared. \\\\n\\n{contradiction|entailment|neutral}'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Manual Tweet: The Truth about #Immigration \\\\n Sentiment: {against |favor}\\nSICK Minimal sentence 1: A man is screaming. [SEP] sentence 2: A man is scared. \\\\n\\n{contradiction|entailment|neutral}\\nManual A man is screaming. \\\\n The question is: A man is scared. True or False? \\\\n The answer is:\\n{False|True|Not sure}\\npoem-sentiment Minimal willis sneered: \\\\n {negative |no_impact|positive}\\nManual willis sneered: \\\\n The sentiment is: {negative |no_impact|positive}\\nOpenbookQA Minimal What creates a valley? \\\\n {feet |rock|water|sand}\\nManual The question is: What creates a valley? \\\\n The answer is: {feet |rock|water|sand}\\nCommonsenseQA Minimal What blocks sunshine? \\\\n {summer |park|desktop|sea|moon}\\nManual The question is: What blocks sunshine? \\\\n The answer is: {summer |park|desktop|sea|moon}\\nCOPA Minimal Effect: I coughed. \\\\n {Cause: I inhaled smoke. |Cause: I lowered my voice.}\\nManual I coughed because {I inhaled smoke. |I lowered my voice.}'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='COPA Minimal Effect: I coughed. \\\\n {Cause: I inhaled smoke. |Cause: I lowered my voice.}\\nManual I coughed because {I inhaled smoke. |I lowered my voice.}\\nARC Minimal Which biome has the most vegetation? \\\\n {desert |forest|grassland|tundra}\\nManual The question is: Which biome has the most vegetation? \\\\n The answer is: {desert |forest|\\ngrassland|tundra}\\nTable 3: A list of minimal templates taken from Ye et al. (2021); Min et al. (2021b) and manual templates taken\\nfrom Holtzman et al. (2021); Zhao et al. (2021). Details provided in Appendix B. See Figure 6 for discussion in\\nempirical results. The input and the label are in the red text and in the blue text, respectively. Note that |is used to\\nseparate different options for the labels.\\nDemos\\nw/ gold labels\\n(Format \\x13 Input distribution \\x13 Label space \\x13 Input-label mapping \\x13)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n positive\\nPanostaja did not disclose the purchase price. \\\\n neutral\\nDemos'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Circulation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n positive\\nPanostaja did not disclose the purchase price. \\\\n neutral\\nDemos\\nw/ random labels\\n(Format \\x13 Input distribution \\x13 Label space \\x13 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n neutral\\nPanostaja did not disclose the purchase price. \\\\n negative\\nOOD Demos\\nw/ random labels\\n(Format \\x13 Input distribution \\x17 Label space \\x13 Input-label mapping \\x17)\\nColour-printed lithograph. Very good condition. Image size: 15 x 23 1/2 inches. \\\\n neutral\\nMany accompanying marketing claims of cannabis products are often well-meaning. \\\\n negative\\nDemos\\nw/ random English words\\n(Format \\x13 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n unanimity\\nPanostaja did not disclose the purchase price. \\\\n wave\\nDemos\\nw/o labels\\n(Format \\x17 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Panostaja did not disclose the purchase price. \\\\n wave\\nDemos\\nw/o labels\\n(Format \\x17 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008.\\nPanostaja did not disclose the purchase price.\\nDemos\\nlabels only\\n(Format \\x17 Input distribution \\x17 Label space \\x13 Input-label mapping \\x17)\\npositive\\nneutral\\nTable 4: Example demonstrations when using methods in Section 5. The ﬁnancial_phrasebank dataset with C=\\n{“positive”, “neutral”, “negative”}is used. Red text indicates the text is sampled from an external corpus; blue\\ntext indicates the labels are randomly sampled from the label set; purple text indicates a random English word.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Rethinking the Role of Demonstrations:\\nWhat Makes In-Context Learning Work?\\nAnonymous ACL submission\\nWe are glad that all reviewers ﬁnd that the001\\npaper is novel (8jk5, LQ6N, 92YB, 7E5P), of002\\ninterest to the broader NLP community (LQ6N,003\\n92YB, 7E5P), supported by solid experiments004\\n(8jk5, LQ6N, 92YB, 7E5P), and well-written (8jk5,005\\nLQ6N, 92YB). Reviewers gave comments on more006\\ndiscussion, limitations and avenues for future work.007\\nWe will incorporate them in the next version of the008\\npaper.1009\\nAdding discussion on robustness of LMs (8jk5,010\\n7E5P): We think the fact that LMs do not use011\\nthe input-label correspondence does not necessar-012\\nily mean that LMs are robust to other aspects of013\\nthe demonstration. Nonetheless, it will be an inter-014\\nesting avenue for future work, given that LMs are015\\nhighly sensitive to small changes in the demonstra-016\\ntions (??).017\\nAdding discussion with respect to the model018\\nsize (8jk5): Absolute differences are similar019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='highly sensitive to small changes in the demonstra-016\\ntions (??).017\\nAdding discussion with respect to the model018\\nsize (8jk5): Absolute differences are similar019\\nacross different model sizes (Figure 3), but since020\\nthe large models have higher absolute performance,021\\nthe relative differences are larger with larger mod-022\\nels.023\\nWhen our ﬁndings hold or do not hold (8jk5):024\\nWe believe that the ﬁndings will hold only when025\\nsome notion of input-label correspondences are026\\nalready captured during pre-training—for tasks027\\nwhose input-label correspondences during pretrain-028\\ning is sparse, the demonstrations with random la-029\\nbels are highly unlikely to work. This has been030\\nshown by ? in a synthetic setup, and future work031\\ncan revisit this with more natural data.032\\nRisk in applying in-context learning, Rec-033\\nommendation to practitioners (8jk5, LQ6N):034\\nSince the models do not capture the correspondence035\\nfrom the demonstrations, it is possible that models036'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='ommendation to practitioners (8jk5, LQ6N):034\\nSince the models do not capture the correspondence035\\nfrom the demonstrations, it is possible that models036\\nare simply relying on some notation of input-label037\\ncorrespondence during pre-training. Thus, apply-038\\ning in-context learning for problems that were not039\\n1We answered reviewers’ questions on the OpenReview\\npage, and address higher-level comments here.\\nin the pretraining data would be potentially risky, 040\\nand practitioners may want to collect the labeled 041\\ndata and ﬁne-tune the model. 042\\nWhere do the gains from demonstrations come 043\\nfrom? (92YB): We think gains from demonstra- 044\\ntions are mainly due to the speciﬁcation of the in- 045\\nput distribution and the label space rather than the 046\\ninput-label correspondence, as Section 5 indicates. 047\\nWe will clarify this in the next version of the paper. 048\\nConcrete answer to “why” using random labels 049\\nkeeps reasonable performance (LQ6N): We 050'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='We will clarify this in the next version of the paper. 048\\nConcrete answer to “why” using random labels 049\\nkeeps reasonable performance (LQ6N): We 050\\nthink it is highly likely to be because the model 051\\nhas been exposed to some notion of input-label cor- 052\\nrespondence during pre-training, so that the demon- 053\\nstrations play a role of triggering them at inference. 054\\nConsideration when training large LMs 055\\n(LQ6N): Due to compute limitations, we were 056\\nnot be able to provide recommendations that 057\\nare empirically supported. Future work may 058\\ninvestigate aspects of language model pretraining 059\\nthat affect the ﬁndings, including the pretraining 060\\ndata and the objective. 061\\nWord ordering matters? (7E5P): We think it 062\\nis an important avenue for future work. For this 063\\npaper, we did not include it in our scope due to 064\\nrequiring multiple times more experiments. 065\\nMore task types, e.g., generation (7E5P): Ex- 066'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='paper, we did not include it in our scope due to 064\\nrequiring multiple times more experiments. 065\\nMore task types, e.g., generation (7E5P): Ex- 066\\ntending this work to generation tasks is not very 067\\ntrivial because designing the demonstrations that do 068\\nnot keep the input-output correspondence but keep 069\\nthe output distribution is difﬁcult. For instance, if 070\\nwe simply replace the output with a random out- 071\\nput as we did in the classiﬁcation tasks, it will 072\\ndestroy both the input-output correspondence and 073\\nthe output distribution. We hope future work can 074\\ninvestigate more in this direction. 075\\nStronger link with instruction-following models 076\\n(7E5P): We will add discussion on ? who ﬁnd 077\\nthat instructions that are irrelevant or even mislead- 078\\ning lead to performance gains as much as “good” 079\\n1\\narXiv:2202.12837v2  [cs.CL]  20 Oct 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='instructions do.080\\nLimitation081\\nThis paper focuses on the tasks from established082\\nNLP benchmarks that have real natural language083\\ninputs. Synthetic tasks with more limited inputs084\\nmay actually use the ground truth labels more, as085\\nobserved by ?. Our paper mainly includes macro-086\\nlevel analysis by examining the average perfor-087\\nmance over multiple NLP datasets, but different088\\ndatasets may behave differently. Appendix dis-089\\ncusses this aspect, including ﬁndings that there are090\\nlarger gaps between using the ground truth labels091\\nand using the random labels in some dataset-model092\\npairs (e.g., in the most extreme case, nearly 14%093\\nabsolute on the ﬁnancial_phrasebank dataset with094\\nGPT-J). We believe it is important to understand in095\\nwhich cases the ground truth labels matter or not,096\\nwhich we leave for future work. Furthermore, our097\\nwork is limited to classiﬁcation and multi-choice098\\ntasks. Extension to the open-set tasks such as gener-099'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='which we leave for future work. Furthermore, our097\\nwork is limited to classiﬁcation and multi-choice098\\ntasks. Extension to the open-set tasks such as gener-099\\nation is not trivial, since it is unclear how to remove100\\nthe input-output correspondence while keeping the101\\ncorrect output distribution. We leave the extension102\\nfor future work.103\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Benchmarking of Retrieval Augmented Generation: A \\nComprehensive Systematic Literature Review on Evaluation \\nDimensions, Evaluation Metrics and Datasets \\nSimon Knollmeyer1,* a , Oğuz Caymazer2,* b , Leonid Koval1c , Muhammad Uzair Akmal1d ,  \\nSaara Asif1e , Selvine G. Mathias1f  and Daniel Großmann1g  \\n1Technische Hochschule Ingolstadt, AImotion Bavaria, Esplanade 10, Ingolstadt, Germany \\n2University of Münster, Department of Information Systems, Münster, Germany \\n{Simon.Knollmeyer, Leonid.Koval, MuhammadUzair.Akmal, Saara.Asif, SelvineGeorge.Mathias, \\n \\nKeywords: Large Language Model,  Retrieval Augmented Generation,  Evaluation Dimensions, Evaluation Metrics, \\nDatasets, Systematic Literature Review. \\nAbstract: Despite the rapid advancements in the field of Large Language Models (LLM), traditional benchmarks have \\nproven to be inadequate for asse ssing the performance of Retrie val Augmented Generation (RAG) systems.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='proven to be inadequate for asse ssing the performance of Retrie val Augmented Generation (RAG) systems. \\nTherefore, this paper presents a comprehensive systematic literature review of evaluation dimensions, metrics, \\nand datasets for RAG systems. Thi s review identifies key evalua tion dimensions such as context relevance, \\nfaithfulness, answer relevance,  correctness, and citation quali ty. For each evaluation dimension, several \\nmetrics and evaluators are propo sed on how to assess them. This  paper synthesizes the findings from 12 \\nrelevant papers and presents a concept matrix that categorizes each evaluation approach. The results provide \\na foundation for the development of robust evaluation frameworks and suitable datasets that are essential for \\nthe effective implementation and deployment of RAG systems in real-world applications. \\n1 INTRODUCTION \\nThe rapid evolution of Artificial Intelligence (AI) \\nespecially in the field of Large Language Models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='1 INTRODUCTION \\nThe rapid evolution of Artificial Intelligence (AI) \\nespecially in the field of Large Language Models \\n(LLMs) attracts widespread attention due to their \\ngroundbreaking achievements in solving complex \\nproblems even surpassing the performance of humans \\nin certain fields (Benbya et al., 2024; Bubeck et al., \\n2023; OpenAI et al., 2023). The speed of AI \\ndevelopment in the area of LLMs outpaced methods \\nto assess their performance and accuracy, leading to \\nmajor flaws in existing traditional benchmarks to \\nevaluate LLMs output through reliable metrics  \\nimpeding their adoption (Hammond, 2024).  \\n \\na  https://orcid.org/0009-0002-1429-6992 \\nb  https://orcid.org/0009-0003-4096-3784 \\nc  https://orcid.org/0000-0003-4845-6579 \\nd  https://orcid.org/0009-0007-3961-1174 \\ne  https://orcid.org/0009-0006-1284-5635 \\nf  https://orcid.org/0000-0002-6549-0763 \\ng  https://orcid.org/0000-0002-7388-5757 \\n* co-authors of the paper, contributed equally'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='e  https://orcid.org/0009-0006-1284-5635 \\nf  https://orcid.org/0000-0002-6549-0763 \\ng  https://orcid.org/0000-0002-7388-5757 \\n* co-authors of the paper, contributed equally \\nDespite their potential, LLMs face substantial \\nchallenges, particularly in fully grasping contextual \\nfactors such as unique technical requirements within \\na specific industries, yet understanding these factors \\nis essential for effective decision-making (Benbya et \\nal., 2024). Even the most powerful models such as \\nGPT-4 struggle with hallucinations, lack of the ability \\nto update itself, and limited context (Bubeck et al., \\n2023; OpenAI et al., 2023). Several researchers point \\nout that these LLMs seem to rather memorize \\nfrequently occurring information encountered during \\ntheir pre-training and struggle with infrequent \\ninformation, i.e., which would typically occur in a \\nspecific industry (Kandpal et al., 2022; Mallen et al., \\n2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='their pre-training and struggle with infrequent \\ninformation, i.e., which would typically occur in a \\nspecific industry (Kandpal et al., 2022; Mallen et al., \\n2022). \\nKnollmeyer, S., Caymazer, O., Koval, L., Akmal, M., Asif, S., Mathias, S. and Großmann, D.\\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation Metrics and Datasets.\\nDOI: 10.5220/0013065700003838\\nPaper published under CC license (CC BY -NC-ND 4.0)\\nIn Proceedings of the 16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K 2024) - Volume 3: KMIS, pages 137-148\\nISBN: 978-989-758-716-0; ISSN: 2184-3228\\nProceedings Copyright© 2024 by SCITEPRESS – Science and Technology Publications, Lda.\\n137'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='M o s t  p r o m i s i n g  a n d  c o m m o n  t o  s o l v e  t h i s  \\nproblem is to augment LLM with non-parametric less \\ncommon knowledge by providing them retrieved text \\nchunks from an external database (Asai et al., 2024; \\nY. Gao et al., 2023; Mallen et al., 2022; Wang et al., \\n2023; Zhang et al., 2023). This approach is known as \\nRetrieval Augmented Generation (RAG), and there \\nare several different RAG paradigms, ranging from \\nso-called naïve RAG to more advanced ones (Asai et \\nal., 2023; Y. Gao et al., 2023; Ma et al., 2023). \\nResearch results suggest that LLM RAGs outperform \\nLLMs, particularly in long-t ail knowledge questions \\n(Asai et al., 2023; Izacard et al., 2022; Ma et al., 2023; \\nMallen et al., 2022; Wang et al., 2023). \\nHowever, there is still uncertainty about the \\naccuracy of such approaches due to the lack of \\ncomprehensive evaluation frameworks to provide \\nevaluation dimensions and metrics to assess RAG \\nLLMs (Y. Gao et al., 2023; Wang et al., 2023). Thus,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='accuracy of such approaches due to the lack of \\ncomprehensive evaluation frameworks to provide \\nevaluation dimensions and metrics to assess RAG \\nLLMs (Y. Gao et al., 2023; Wang et al., 2023). Thus, \\nwe address the following research questions (RQ): \\n \\n• R Q 1 :  H o w  t o  e v a l u a t e  a  R A G - e n h a n c e d  \\nLLM comprehensively across different \\ndimensions and metrics? \\n• RQ2: What type of datasets are available for \\napplying the dimensions and metrics?  \\n \\n Therefore, the research contribution of this paper \\nl i e s  i n  a d d r e s s i n g  t h e  c u r r e n t  r e s e a r c h  g a p  b y  \\nproviding a systematic overview of how to evaluate \\nRAG pipelines comprehensively, offering insights \\ninto the development of robust evaluation \\ndimensions, metrics and possible datasets (Y. Gao et \\nal., 2023; Hammond, 2024; Wang et al., 2023). \\nThe paper is structured as follows: The next \\nsection introduces RAG. Section three explains the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='dimensions, metrics and possible datasets (Y. Gao et \\nal., 2023; Hammond, 2024; Wang et al., 2023). \\nThe paper is structured as follows: The next \\nsection introduces RAG. Section three explains the \\nchosen research method. Section four presents the \\nevaluation framework. Finally, the last section \\nprovides the conclusions. \\n2 RETRIEV AL AUGMENTED \\nGENERATION \\nTraditional pre-trained LLMs such as GPT and BERT \\nencode knowledge within their parameters, but \\nstruggle with tasks requi ring specific factual \\nknowledge which is not present in their parameters \\n(Y. Gao et al., 2023; Lewis et al., 2020). This problem \\nis evident in the fact that even the most powerful \\nmodels such as GPT-4 struggle with made-up facts \\nknown as hallucinations, a lack of ability to self-\\nupdate and limited context (Bubeck et al., 2023; \\nOpenAI et al., 2023). Even further increasing the size \\nof their parameters, i.e., the training dataset, in which \\nthe knowledge appears to be stored to include more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='OpenAI et al., 2023). Even further increasing the size \\nof their parameters, i.e., the training dataset, in which \\nthe knowledge appears to be stored to include more \\ninformation will be likely insufficient to address the \\nissue of long-tail knowledge (Kandpal et al., 2022; \\nMallen et al., 2022). \\nTherefore, Lewis et al. (2020) proposed RAG by \\ncombining non-parametric memory with parametric \\nmemory that uses a dense vector index of external \\ndocuments such as Wikipedia articles that can be \\ndynamically accessed using a retriever. Research \\nresults comparing the performance of RAG LLM \\nwith standalone LLM, suggest for the former superior \\nperformance (Asai et al., 2023; Izacard et al., 2022; \\nLewis et al., 2020; Ma et al., 2023; Mallen et al., \\n2022; Wang et al., 2023). \\nA naïve RAG pipeline involves three main steps. \\nFirstly, the documents containing specific \\ninformation are indexed (L ewis et al., 2020). The \\nmost common method is to split the documents into'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='A naïve RAG pipeline involves three main steps. \\nFirstly, the documents containing specific \\ninformation are indexed (L ewis et al., 2020). The \\nmost common method is to split the documents into \\nsmaller sections so-called chunks and store their \\nembedding in a vector database (Y. Gao et al., 2023). \\nI n  t h e  s e c o n d  s t e p ,  a  g i v e n  i n p u t  q u e r y  i s  l i k e w i s e  \\nembedded and then compared with the passages in the \\nvector database by calcu lating the similarity, \\nreturning a set of top-ranked chunks that are most \\nrelevant for the query (Y. Gao et al., 2023; Karpukhin \\net al., 2020). In the final step, the retrieved content \\nand the query are combined and prompted into an \\nLLM so that it can provide a coherent answer (Y. Gao \\net al., 2023; Lewis et al., 2020). \\nThis naïve setup can be modified by applying \\ndifferent advanced methods relating to pre- or post-\\nretrieval (Asai et al., 2023; Y. Gao et al., 2023; Ma et'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='et al., 2023; Lewis et al., 2020). \\nThis naïve setup can be modified by applying \\ndifferent advanced methods relating to pre- or post-\\nretrieval (Asai et al., 2023; Y. Gao et al., 2023; Ma et \\nal., 2023). For instance, Ma et al. (2023) propose \\nquery rewriting as an advanced pre-retrieval method \\nand report performance improvements.  \\nDespite the use of advanced methods, the RAG \\napproach can still be divided into the outlined steps of \\na naïve RAG for evaluation. However, there is a lack \\nof evaluation dimensions and metrics on how to \\nanalyse and assess such systems, e.g., which evaluation \\ndimensions to consider and what kind of metrics to \\ncalculate for which step (Y. Gao et al., 2023). \\n3 RESEARCH METHOD \\nThe Systematic Literature Review (SLR) is a well-\\nknown and established research method within \\nInformation Systems (IS)  research for reviewing \\nscientific articles based on a search process (Bell et \\nal., 2019; Paré et al., 2016). The term \"systematic\"'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Information Systems (IS)  research for reviewing \\nscientific articles based on a search process (Bell et \\nal., 2019; Paré et al., 2016). The term \"systematic\" \\nmeans that the research steps should be \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n138'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='understandable, reproducible, and grounded in a \\nstructured process that minimizes potential researcher \\nbias by providing a clear audit trail for decisions and \\nconclusions (Bell et al., 2019). SLR is especially \\nvaluable when summarizing and comparing \\nfragmented knowledge on a certain topic (Bell et al., \\n2019). Existing literature reveals a significant gap in \\ncomprehensive evaluation frameworks for assessing \\nRAG systems (Y. Gao et al., 2023; Wang et al., 2023). \\nTherefore, given the emerging and unexplored \\nresearch on evaluation dimensions for RAG, the SLR \\nis particularly appropriate. \\nThis paper ensures rigorous documentation by \\nusing the literature search process proposed by \\nBrocke et al. (2009), extended with the \\nrecommendations of Paré et al. (2016). It also follows \\nthe recommendation of Webster and Watson (2002) \\nto use a concept matrix for structuring and comparing \\nthe results. The complete adopted literature search \\nprocess is illustrated in Figure 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='the recommendation of Webster and Watson (2002) \\nto use a concept matrix for structuring and comparing \\nthe results. The complete adopted literature search \\nprocess is illustrated in Figure 1. \\n \\n \\nFigure 1: Systematic Literature Review process. \\nThe search process incl uded conducting a pre-\\nstudy on Google Scholar by a detailed screening of \\ntitles, abstracts and full texts to identify papers \\nspecifically addressing evaluation metrics for RAG \\nsystems (Y. Gao et al., 2023; Wang et al., 2023). \\nThese insights resulted in the following search string: \\n \\n• TITLE-ABS-KEY (“Retrieval Augmented \\nGeneration” AND “Evaluation Metric”) \\n \\nThe following inclusion and exclusion criteria \\nwere applied to filter relevant papers from the search \\nresults in Google Scholar, Scopus and the IS \\nconferences ICIS and ECIS: \\nInclusion Criteria: \\n• Papers from academic journals, conferences, \\nor gray literature.  \\n• Published in English. \\n \\nExclusion Criteria: \\n• Duplicates across databases.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='conferences ICIS and ECIS: \\nInclusion Criteria: \\n• Papers from academic journals, conferences, \\nor gray literature.  \\n• Published in English. \\n \\nExclusion Criteria: \\n• Duplicates across databases. \\n• Minimal relevance to evaluation metrics, or \\nlack of focus on RAG systems. \\n• No guidelines on metric implementation or \\napplication at the different RAG steps. \\n \\nThe search process commen ced with an abstract \\nscreening of the initial results, followed by a full-text \\nreview of the selected papers. Forward and backward \\ncitation searches were subsequently performed on \\nrelevant studies to identify additional literature. This \\ncomprehensive approach yielded a final sample of 12 \\npapers, as illustrated in Figure 1. \\nIn the final step, overarching categories from the \\nfinal sample were synthesized into a concept matrix \\n(Webster & Watson, 2002). Relevant concepts on \\nevaluation dimensions were identified and mapped to \\nthe RAG steps (cf. Section 2) to provide an accurate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(Webster & Watson, 2002). Relevant concepts on \\nevaluation dimensions were identified and mapped to \\nthe RAG steps (cf. Section 2) to provide an accurate \\noverview on how to evaluate each RAG phase. \\nThe concept matrix is shown in Table 1. It \\ncategorizes the sampled papers based on predictive \\nevaluation criteria and dataset characteristics. The \\nformer includes the columns \"Retrieval\" and \\n\"Generation\" relating to the RAG steps. The \\n\"Evaluator\" column indi cates whether lexical \\nmatching, semantic similarity or LLM as a judge is \\nused for evaluation. The dataset characteristics \\ninclude single-hop and multi-hop reasoning tasks, \\nsynthetic datasets (triples), and open-domain question \\nanswering (QA). Each paper is marked (\"X\") to show \\nthe criteria it addresses, p roviding a comprehensive \\noverview of their focus areas. In addition, the \\nfrequency of occurrence in the literature can be used \\nto determine how widespread or accepted a metric is.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='overview of their focus areas. In addition, the \\nfrequency of occurrence in the literature can be used \\nto determine how widespread or accepted a metric is. \\nEach proposed metric for the evaluation dimensions \\ndepending on the evaluator and the requirements for \\nthe data to calculate it are summarized in Table 2. \\n4  RESULTS \\nThis section starts with examining the first column of \\nthe concept matrix: predictive evaluation , which \\ninvolves assessing the performance of the RAG \\nsystem in retrieving accurat e context and effectively \\nutilizing it to generate responses (Guinet et al., 2024).  \\n \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n139'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 1: Concept Matrix with Evaluation Dimensions and Datasets. \\n \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n140'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 2: Summary of proposed Evaluation Dimensions, the corresponding Metrics and Dataset Requirements. \\n \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n141'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='The proposed evaluation pipeline outlines (cf. \\nFigure 2) the process of assessing the RAG approach \\nacross various evaluation dimensions and focuses on \\nthe retrieval and generation stages of a typical RAG \\nsystem. The evaluation pr ocess starts with the \\nretrieval step, emphasizing context relevance as a \\ncritical dimension to assess how effectively relevant \\ninformation is retrieved. \\nSubsequently, the focus then shifts to the \\ngeneration step , examining the evaluation \\ndimensions of answer relevance, correctness, \\nfaithfulness, a n d  citation quality to determine the \\naccuracy and reliability of the generated responses. \\nEach evaluation dimension is carefully defined, and \\nquantifying metrics are proposed according to the \\nsampled papers.   \\nHow and what type of metric is ultimately used to \\ncalculate the respective evaluation dimension \\ndepends on the chosen evaluator. Lexical matching \\nmetrics focus on exact word matching and simple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='How and what type of metric is ultimately used to \\ncalculate the respective evaluation dimension \\ndepends on the chosen evaluator. Lexical matching \\nmetrics focus on exact word matching and simple \\nstatistical calculations, such as keyword frequency or \\nposition-based measures like Mean Reciprocal Rank \\n(MRR), i.e., these metrics assess how closely the \\nwords in the documents match the query without \\nconsidering deeper meanings.  \\nSemantic similarity metrics, on the other hand, \\ngo beyond surface-level text comparison to \\nunderstand the underlying meanings and concepts by \\ncomparing their semanti cal similarity based on \\ncontext and conceptual relationships between words \\nand sentences. This approac h captures the intent of \\nthe query and the documents, evaluating relevance \\nthrough semantic similarity rather than just keyword \\noccurrence.  \\nFinally, LLM as a judge uses a LLM to evaluate \\ncontent by making it context-aware, prompting the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='through semantic similarity rather than just keyword \\noccurrence.  \\nFinally, LLM as a judge uses a LLM to evaluate \\ncontent by making it context-aware, prompting the \\nmodel to consider the coherence, factuality, and \\nrelevance of the information based on its \\ncomprehensive understanding of language and \\nknowledge. Therefore, the choice of the evaluator \\ndetermines the type of metric applied to assess each \\nevaluation dimension, depending on whether the \\nfocus is on exact word matching, conceptual \\nsimilarity, or a nuanced, context-aware judgment by \\nan advanced LLM.  \\nThe upcoming sub-sections follows the rationale \\nof first explaining the evaluation dimension and then \\nthe respective evaluator by detailing how to calculate \\nthe metric proposed in the sampled papers. \\n4.1 Context Relevance \\nThe evaluation dimension of context relevance  \\npertains to the retrieval step and assesses the degree \\nto which the retrieved context contains only the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='4.1 Context Relevance \\nThe evaluation dimension of context relevance  \\npertains to the retrieval step and assesses the degree \\nto which the retrieved context contains only the \\nnecessary information to answer the query, reducing \\ncomputational costs and improving efficiency by \\nminimizing irrelevant content (Es et al., 2023; Saad-\\nFalcon et al., 2023; Yu et al., 2024). Additionally, \\nwhen retrieved passages are too    LLMs often \\nstruggle to effectively utilize the information, \\nparticularly if the relevant details are embedded in the \\nm i d d l e  o f  t h e  p a s s a g e  ( E s  e t  a l . ,  2 0 2 3 ) .  H e n c e ,  \\nconcise query-relevant passages significantly \\nimprove the LLM generation quality (Es et al., 2023; \\nYu et al., 2024).  \\nRecall@k and MRR@k a r e  k e y  lexical metrics \\nfor evaluating the retrieval performance in RAG \\nsystems (Rackauckas et al., 2024; Yu et al., 2024). \\nEach metric provides a different perspective on the \\neffectiveness'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='for evaluating the retrieval performance in RAG \\nsystems (Rackauckas et al., 2024; Yu et al., 2024). \\nEach metric provides a different perspective on the \\neffectiveness\\n of the retrieval process. Recall@k \\n \\nFigure 2: Evaluation Pipeline with Evaluation Dimensions in a naïve RAG setup. \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n142'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='measures how many relevant passages are captured \\nwithin the top k retrieved chunks, even if some \\nirrelevant ones are included. The formula is as follows:  \\n \\nRecall@k = |Relevant Passages ∩ k-Passages|\\n|Relevant Passages|  (1)\\n \\nMRR@k calculates context relevance by \\nemphasizing the rank of the first relevant passage \\nacross multiple queries (Rackauckas et al., 2024). If a \\nrelevant passage appears in the top k results, its \\ncontribution to the MRR@k score is the inverse of its \\nrank, e.g., a passage ranked two contributes as ½ with \\nMRR@5 (Rackauckas et al., 2024). If no relevant \\npassage is found in the top k the contribution is zero. \\nThe formula for MRR@k is: \\n \\nMRR@k =1\\n|Q| \\u0dcd 1\\nrank୧\\n|୕|\\n୧ୀଵ\\n (2)\\n \\nBy using the LLM as a judge , it is possible to \\ncalculate an estimated context relevance score. Given \\na query q and its retrieved context c(q), the LLM is \\nprompted to extract a subset of sentences (𝑆\\n\\u0bd8௫௧) from \\nc(q) that are relevant to answering q by using the \\nfollowing prompt:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='a query q and its retrieved context c(q), the LLM is \\nprompted to extract a subset of sentences (𝑆\\n\\u0bd8௫௧) from \\nc(q) that are relevant to answering q by using the \\nfollowing prompt: \\n \\n\"Please extract relevant sentences from the \\nprovided context that can potentially help answer the \\nfollowing question. If no relevant sentences are \\nfound, or if you believe the question cannot be \\nanswered from the given context, return the phrase \\n\\'Insufficient Information\\'. While extracting candidate \\nsentences, you\\'re not allowed to make any changes to \\nsentences from the given context.\" (Es et al., 2023) \\n \\nThe prompt instructs the LLM to select only the \\nsentences that it considers relevant to q without \\nchanging the content. The Context Relevance Score \\n(CRS) is calculated by dividing the relevant \\nsentences extracted (𝑆\\n\\u0bd8௫௧) from the context c(q) by the \\ntotal number of sentences. This can be expressed with \\nthe following formula (Es et al., 2023): \\n \\nCRS = Number of extracted sentences Sୣ୶୲'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"sentences extracted (𝑆\\n\\u0bd8௫௧) from the context c(q) by the \\ntotal number of sentences. This can be expressed with \\nthe following formula (Es et al., 2023): \\n \\nCRS = Number of extracted sentences Sୣ୶୲\\nTotal number of sentences in c(q)  (3)\\n \\nThe CRS indicates the proportion of the context \\nthat is relevant. A higher score indicates that a greater \\nproportion of the retrieved context is focused and \\nrelevant for answering the query, while a lower score \\nindicates that much of the retrieved context contains \\nirrelevant information (Es et al., 2023; Saad-Falcon et \\nal., 2023; Yu et al., 2024). \\n4.2 Faithfulness \\nThe evaluation dimensions faithfulness refers to the \\ngeneration step and evaluates how well an LLM's \\nresponse is grounded in the retrieved context, i.e., all \\ninformation in the response  can be directly inferred \\nfrom it (Adlakha et al., 2023; Es et al., 2023; Hu et \\nal., 2024). This evaluation dimension is crucial to \\nidentify possible hallucinations in the answer of\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='from it (Adlakha et al., 2023; Es et al., 2023; Hu et \\nal., 2024). This evaluation dimension is crucial to \\nidentify possible hallucinations in the answer of \\nLLMs ensuring factual correctness (Adlakha et al., \\n2023; Es et al., 2023; Ravi et al., 2024). For instance, \\nAdlakha et al. (2023) found that GPT-4 had the \\nhighest agreement with human annotations, followed \\nby GPT-3.5 and k-precision.  \\nAs the primary lexical metric  Adlakha et al. \\n(2023) propose k-precision to evaluate the degree of \\nfaithfulness since it has the highest agreement with \\nhuman judgements. It can be calculated as the \\nproportion of tokens in the LLMs response that are \\npresent in the retrieved context, i.e., it is the overlap of \\nmatching tokens with the retrieved context divided by \\nthe total number of tokens in the response (Adlakha et \\nal., 2023). Hence, the formula is as follows: \\n \\nK-Precision = Matched Tokens\\nResponse Tokens (4)\\n \\nAnother way of calculating the faithfulness is by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='al., 2023). Hence, the formula is as follows: \\n \\nK-Precision = Matched Tokens\\nResponse Tokens (4)\\n \\nAnother way of calculating the faithfulness is by \\nusing LLMs such as GPT-4/3.5 as evaluators. The \\nLLMs are prompted to judge whether the response \\nand the retrieved context match from an ordinal scale \\nof “fully”, “partially”, or “not at all” (Adlakha et al., \\n2023). In the same way, Ravi et al. (2024) propose to \\nassess the responses through an evaluator LLM \\nwhether the responses are supported, contradicted or \\nnot supported by the retrieved context. In order to \\nquantify this ordinal scale, we can assign numerical \\nscores depending on the degree of support and take \\nthe average of all individu al response scores, i.e., \\nassign 1 for “fully”, 0.5 for “partially”, and 0 for “not \\nat all”. The formula for calculating the Faithfulness \\nCoefficient (FC) is as follows: \\n \\nFC = ∑ Faithfulness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (5)\\n \\nA similar method is used by Hu et al. (2024), who'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Coefficient (FC) is as follows: \\n \\nFC = ∑ Faithfulness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (5)\\n \\nA similar method is used by Hu et al. (2024), who \\npropose a framework that e xtracts “claim triplets” \\n(subject, predicate, object) to represent fine-grained \\nknowledge assertions within the LLM response. The \\npurpose of this extraction is to break down the answer \\ninto specific atomic cla ims that can be checked \\nindividually (Hu et al., 2024; Min et al., 2023). A \\njudging LLM evaluates each triplet as “entailment” \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n143'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(supported), “contradiction” (contradicted), or \\n“neutral” (unsupported) (Hu et al., 2024). Other \\nauthors also follow this approach of breaking down \\nthe statements from the LLM response into atomic \\nfacts to obtain a fine-grained measure of the \\nfaithfulness degree (Min et al., 2023). \\nEs et al. (2023) propose a process in which the \\nanswer 𝑎\\n௦(𝑞)  is considered faithful to the context \\n𝑐(𝑞) if the statements in the LLM response can be \\ndirectly inferred from the retrieved context. The \\nprocess begins by using a LLM as a judge  t o  \\ndecompose the LLM response into a set of statements, \\n𝑆(𝑎௦(𝑞)) , which involves breaking down longer \\nsentences into shorter ones (Es et al., 2023). For each \\nstatement 𝑠\\u0bdc in 𝑆, the judging LLM verifies if it can \\nbe inferred from the given context 𝑐(𝑞)  using a \\nverification function 𝑣(𝑠\\u0bdc,𝑐 (𝑞)) (Es et al., 2023). The \\njudging LLM assesses whether each statement is \\nsupported by the information in the retrieved context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='verification function 𝑣(𝑠\\u0bdc,𝑐 (𝑞)) (Es et al., 2023). The \\njudging LLM assesses whether each statement is \\nsupported by the information in the retrieved context \\nand provides a “yes” or “no” verdict for each \\nstatement (Es et al., 2023). The Faithfulness Score \\n(𝐅𝐒) is then calculated as the  ratio of the number of \\nsupported statements 𝑉  to the total number of \\nstatements S, which can be expressed as follows: \\n \\nFS = |V|\\n|S| (6)\\n4.3 Answer Relevance \\nThe evaluation dimensions answer relevance refers \\nto the generation step and assesses whether the LLM \\nr e s p o n s e  i s  d i r e c t l y  a d d r e s s i n g  t h e  q u e r y  ( E s  e t  a l . ,  \\n2023; Rackauckas et al., 2024; Saad-Falcon et al., \\n2023; Yu et al., 2024). This evaluation dimension \\npenalizes incomplete or redundant answers, \\nregardless of factuality (Es et al., 2023; Rackauckas \\net al., 2024). \\nBy using LLM as a judge  it is possible to \\ncalculate an estimation of the answer relevance (Es et'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='regardless of factuality (Es et al., 2023; Rackauckas \\net al., 2024). \\nBy using LLM as a judge  it is possible to \\ncalculate an estimation of the answer relevance (Es et \\nal., 2023; Rackauckas et al., 2024; Saad-Falcon et al., \\n2023). Given a generated answer 𝑎\\n௦(𝑞), the judging \\nLLM is prompted to generate 𝑛 potential questions 𝑞\\u0bdc \\nthat could be answered by using 𝑎௦(𝑞)  (Es et al., \\n2023). This is done using the following prompt: \\n \\nGenerate a question for the given answer: \\nanswer:[answer] (Es et al., 2023) \\n \\nSubsequently, text embeddings for all generated \\nquestions 𝑞\\u0bdc  and the original query 𝑞 are created to \\ncalculate in the next step the cosine similarity \\nbetween their embeddings (Es et al., 2023). The \\nAnswer Relevance Score (ARS)  is obtained by \\naveraging the similarity  between the generated \\nquestions 𝑞\\n\\u0bdc  and the original query 𝑞  using the \\nfollowing formula: \\n \\nARS = 1\\nn \\u0dcds i m(q, q୧)\\n୬\\n୧ୀଵ\\n (7)\\n \\nwhere 𝑠𝑖𝑚(𝑞, 𝑞\\u0bdc) represents the cosine similarity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='questions 𝑞\\n\\u0bdc  and the original query 𝑞  using the \\nfollowing formula: \\n \\nARS = 1\\nn \\u0dcds i m(q, q୧)\\n୬\\n୧ୀଵ\\n (7)\\n \\nwhere 𝑠𝑖𝑚(𝑞, 𝑞\\u0bdc) represents the cosine similarity \\nbetween the embeddings of the generated questions \\nq୧  and the original query q (Es et al., 2023). This \\nmetric effectively measures how well the generated \\nanswer matches the intent and content of the original \\nquestion (Es et al., 2023). \\n4.4 Correctness \\nThe evaluation dimension correctness r e f e r s  t o t he  \\ngeneration step and evaluates whether the LLM’s \\nresponse accurately matches the “golden passage” \\nprovided by human annotators (Adlakha et al., 2023; \\nT .  G a o  e t  a l . ,  2 0 2 3 ) .  T h i s  m e t r i c  focuses on the \\nfactual accuracy  of the information by comparing \\nthe LLM response with a reference answer (Adlakha \\net al., 2023; T. Gao et al., 2023; Guinet et al., 2024; \\nRackauckas et al., 2024). \\nAs a primary lexical metric  Adlakha et al. \\n(2023) propose using recall as it correlates well with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"et al., 2023; T. Gao et al., 2023; Guinet et al., 2024; \\nRackauckas et al., 2024). \\nAs a primary lexical metric  Adlakha et al. \\n(2023) propose using recall as it correlates well with \\nhuman annotations. Traditional metrics like Exact \\nMatch (EM), F1, and ROUGE are often too strict due \\nto their focus on exact word matching (Adlakha et al., \\n2023). Recall measures how much of the reference \\nanswer's essential content is captured in the model's \\nresponse without penalizing additional information \\n(Adlakha et al., 2023). \\nSome authors find that semantic similarity  \\nmetrics like BERTScore is less effective than recall \\nfor correctness due to lower alignment with human \\nannotations (Adlakha et al., 2023; T. Gao et al., \\n2023). These metrics do not account for factual \\naccuracy or logical consistency, as responses can be \\ntextually similar but factually incorrect (Adlakha et \\nal., 2023; T. Gao et al., 2023). \\nBy using LLMs such as GPT-3.5 and GPT-4 as\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='accuracy or logical consistency, as responses can be \\ntextually similar but factually incorrect (Adlakha et \\nal., 2023; T. Gao et al., 2023). \\nBy using LLMs such as GPT-3.5 and GPT-4 as \\nevaluators to judge the correctness of responses by \\nprompting them with the question, the reference \\nanswer, and the LLMs response to determine whether \\nthe model response is correct, partially correct, or \\nincorrect (Adlakha et al., 2 023; Rackauckas et al., \\n2024). This approach seems to yield the highest \\nagreement with human annotations (Adlakha et al., \\n2023; Rackauckas et al., 2024). Correctness can be \\nquantified by assigning scores: 1 for “fully correct”, \\n0.5 for “partially”, and 0 for “incorrect”. The \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n144'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Correctness Coefficient (CC)  is the average of all \\nresponse scores. Therefore, the formula is as follows: \\n \\nCC = ∑  Correctness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (8)\\n4.5 Citation Quality \\nThe evaluation dimension citation quality focuses on \\nassessing whether an LLM correctly cites its sources \\nwhen generating text (T. Gao et al., 2023). Citation \\nquality is calculated using two metrics: citation \\nrecall and citation precision. Citation recall ensures \\nthat every piece of information in the generated \\nresponse is fully supported by the cited passages, \\nwhile citation precision checks whether all cited \\npassages are relevant and necessary for the statements \\nmade (T. Gao et al., 2023).  \\nTo perform this evaluation automatically, the \\nLLM acts as a judge, which is prompted with a chain-\\nof-thought method instead of simple lexical matching \\n(T. Gao et al., 2023). The LLM checks if the \\nconcatenated text from the cited passages semantically'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='of-thought method instead of simple lexical matching \\n(T. Gao et al., 2023). The LLM checks if the \\nconcatenated text from the cited passages semantically \\nsupports the generated statements (T. Gao et al., 2023). \\nFor citation recall, it evaluates whether all generated \\nstatements are substantiated by the citations. A \\nstatement receives a recall score of 1 if it is fully \\nsupported by at least one citation, otherwise, it receives \\na score of 0 (T. Gao et al., 2023). For citation precision, \\nthe LLM identifies any \"irrelevant\" citations, i.e., those \\nthat do not independently support a statement or are not \\nnecessary when other citations already provide full \\nsupport (T. Gao et al., 2023) . A citation receives a \\nprecision score of 1 if it is relevant and contributes to \\nthe statement\\'s support and 0 if it is irrelevant (T. Gao \\net al., 2023). \\nThe robustness of these metrics is validated by \\ntheir strong correlation  with human judgements'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"the statement's support and 0 if it is irrelevant (T. Gao \\net al., 2023). \\nThe robustness of these metrics is validated by \\ntheir strong correlation  with human judgements  \\n(T. Gao et al., 2023). By averaging the citation recall \\nand precision scores across all statements and \\ncitations in the generated response, an overall citation \\nquality score can be calculated, providing a \\ncomprehensive measure of how accurately and \\nappropriately an LLM uses citations in its output. \\n5 DATASETS & APPLICATION \\nBuilding on the concept matrix's evaluation \\ndimensions presented above, it's essential to consider \\nhow different datasets relate to the evaluators of these \\ndimensions and its corresponding evaluation metrics. \\nThe choice between open-domain QA datasets and \\nsynthetic datasets like RAGAS or ARES, along with \\nthe type of reasoning required (single-hop vs. multi-\\nhop), plays a crucial role in ensuring the robustness \\nand reliability of these evaluations.\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='synthetic datasets like RAGAS or ARES, along with \\nthe type of reasoning required (single-hop vs. multi-\\nhop), plays a crucial role in ensuring the robustness \\nand reliability of these evaluations. \\nOpen-domain QA datasets such as SQuAD2.0, \\nHotpotQA, and Natural Questions are based on \\nWikipedia articles. These are mainly suitable for \\nlexical matching  enable comparisons across RAG \\nsystems (Kwiatkowski et al., 2019; Rajpurkar et al., \\n2018; Yang et al., 2018). These datasets often include \\n“golden passages” which make them ideal for \\nevaluating correctness and faithfulness by providing \\na factual reference (Kwiatkowski et al., 2019; \\nRajpurkar et al., 2018; Yang et al., 2018). For \\ninstance, SQuAD2.0 includes unanswerable queries \\nrequiring RAG systems to recognize when there is \\ninsufficient information to provide a valid answer \\n(Rajpurkar et al., 2018; Rau et al., 2024). Also \\ncalculating context relevance  is straightforward \\nbecause the datasets provide clear ground truth in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(Rajpurkar et al., 2018; Rau et al., 2024). Also \\ncalculating context relevance  is straightforward \\nbecause the datasets provide clear ground truth in \\nterms of which passages are relevant, making them \\nideal for recall-oriented metrics like Recall@k o r  \\nMRR@k (Adlakha et al., 2023; Hu et al., 2024). \\nHowever, evaluating answer relevance and citation \\nquality is more challenging with open-domain QA \\ndatasets since these typically focus on finding a single \\ncorrect answer rather than assessing nuanced citation \\npractices or multi-source relevance (Kwiatkowski et \\nal., 2019; Rajpurkar et al., 2018; Yang et al., 2018). \\nSynthetic datasets such as RAGAS and ARES \\nare specifically designed to evaluate the effectiveness \\nof RAG systems by minimizing reliance on human \\nannotations (Es et al., 2023; Saad-Falcon et al., 2023). \\nThese frameworks often use synthetic datasets that \\nonly require query-context-response triples, making \\nthem suitable to evaluate every evaluation dimension'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='These frameworks often use synthetic datasets that \\nonly require query-context-response triples, making \\nthem suitable to evaluate every evaluation dimension  \\n( E s  e t  a l . ,  2 0 2 3 ;  H u  e t  a l . ,  2 0 2 4 ;  M i n  e t  a l . ,  2 0 2 3 ;  \\nSaad-Falcon et al., 2023). Synthetic datasets \\ncombined with LLM judges align well with human \\nannotations, outperforming lexical and semantic \\nsimilarity metrics (Adlakha et al., 2023; Saad-Falcon \\net al., 2023). Additionally, this approach is model-\\nagnostic, allowing flexible use across different LLMs \\nand setups (Es et al., 2023; Saad-Falcon et al., 2023). \\nThis adaptability ensures that RAG systems can \\nbe effectively assessed and fine-tuned for diverse and \\ncomplex queries, enhanci ng their performance in \\npractical, real-world setting. Table 3 summarizes the \\nkey differences between the datasets by comparing \\nthem. \\nIn terms of reasoning, single-hop and multi-hop \\nqueries require different approaches and datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='key differences between the datasets by comparing \\nthem. \\nIn terms of reasoning, single-hop and multi-hop \\nqueries require different approaches and datasets \\nSingle-hop reasoning involves deriving an answer \\nfrom\\n a single piece of evidence, i.e., one retrieved  \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n145'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 3: Comparing Open Domain QA Datasets with Synthetic Datasets for evaluating RAG. \\nAspect O pen Domain QA Dataset S ynthetic Dataset\\nExamples SQuAD2.0, HotpotQA, Natural Questions, \\nQAMPARI RAGAS, ARES \\nReasoning Type \\nSingle-hop (e.g., SQuAD2.0, Natural \\nQuestions) \\nMulti-hop (e.g., HotpotQA, QAMPARI)\\nSingle-hop and adaptable to Multi-hop \\nEvaluation \\nDimensions \\nCorrectness, Faithfulness, Context Relevance \\n(basic) \\nCorrectness, Faithfulness, Context Relevance (multi-\\nretrieval), Answer Relevance, Citation Quality \\nEvaluators Lexical Matching Semantic S imilarity, LLM as a Judge \\nStrengths High reliability for correctness and \\nfaithfulness due to golden-passages \\nModel and vendor agnostic, adaptable for various \\nqueries and rapid evaluation of RAG without the need \\nfor human annotations or gold-passages \\nLimitations \\nLess effective in evaluating multi-source \\ncitations, complex context relevance, and \\nanswer relevance in multi-hop'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='for human annotations or gold-passages \\nLimitations \\nLess effective in evaluating multi-source \\ncitations, complex context relevance, and \\nanswer relevance in multi-hop\\nAssociated costs related to token usage and potential \\nlatency due to LLM judging, different performances \\ndepending on the employed LLM model \\n \\npassage, and is well-suited for the evaluation \\ndimensions correctness, faithfulness, and basic \\ncontext relevance . Popular single-hop datasets are \\nNatural Questions (Kwiatkowski et al., 2019) and \\nSQuAD2.0 (Rajpurkar et al., 2018), where the \\nrelevant information is contained within a single \\npassage, allowing the calculation of metrics \\npredominantly with lexical matching or simple \\nsemantic similarity, e.g., focusing on metrics such as \\nprecision and recall (Adlakha et al., 2023; Ravi et al., \\n2024). In contrast, multi-hop reasoning requires to \\nconnect multiple pieces of retrieval, usually from \\ndifferent documents or distant parts of the same'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='2024). In contrast, multi-hop reasoning requires to \\nconnect multiple pieces of retrieval, usually from \\ndifferent documents or distant parts of the same \\ndocument, to be combined in order to obtain a correct \\nanswer. This approach is better suited for evaluating \\ncontext relevance  for multiple retrieval, answer  \\nrelevance, i.e., how the combined context informs the \\nanswer, and citation quality that correctly attributes \\ninformation to multiple sources (Adlakha et al., 2023; \\nEs et al., 2023; T. Gao et al., 2023; Hu et al., 2024). \\nOpen-domain QA datasets like HotpotQA (Yang et \\nal., 2018) or QAMPARI (Amouyal et al., 2022) are \\nspecifically designed for multi-hop reasoning. These \\nrequire synthesizing information from multiple \\nretrieved contexts, which involves understanding \\ncomplex connections and contextual relevance that \\ngo beyond surface-level comparisons. Employing \\nLLMs as a judge for this evaluation is the most \\nsuitable option since LLMs can comprehend the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='complex connections and contextual relevance that \\ngo beyond surface-level comparisons. Employing \\nLLMs as a judge for this evaluation is the most \\nsuitable option since LLMs can comprehend the \\ncombination of multiple contexts better by making \\nmore nuanced judgement than simple lexical \\nmatching or semantical similarity of concepts (Es et \\nal., 2023; T. Gao et al., 2023; Min et al., 2023; Saad-\\nFalcon et al., 2023). \\n \\n \\n6 PRACTICAL APPLICATION \\nIn order to apply evaluation dimensions and select the \\nappropriate datasets, it is necessary to understand \\ndataset requirements. For instance, the evaluation \\ndimension faithfulness requires data regarding the \\nretrieved passages of the RAG to be tested and a \\nreference retrieval (golden retrieval), but these are \\noften not available in real-world operations. \\nTherefore, synthetic datas ets that are applicable \\nreference-free would be more suitable for practical \\noperation, and they also have a high alignment with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Therefore, synthetic datas ets that are applicable \\nreference-free would be more suitable for practical \\noperation, and they also have a high alignment with \\nhuman annotations (Es et al., 2023; Saad-Falcon et \\nal., 2023).  \\nIt is also necessary to select suitable metrics \\naccording to the objective of optimizing the RAG \\nsystem. For this purpose, a distinction should first be \\nmade as to whether the performance of the retrieval \\nor generation step should be considered. A suitable \\nmetric is then selected according to a specific \\nproblem. For example, if the factuality of the answer \\nis to be increased, correctness is a more suitable \\nmetric than faithfulness.  \\nFinally, it is important to build an automated, \\nrobust and reliable evaluation pipeline that can be \\nused to evaluate the RAG system (Es et al., 2023). \\n7 CONCLUSION \\nThis paper proposes a comprehensive evaluation \\nframework specifically for RAG by conducting an \\nSLR and providing an extensive overview of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='7 CONCLUSION \\nThis paper proposes a comprehensive evaluation \\nframework specifically for RAG by conducting an \\nSLR and providing an extensive overview of \\ncurrently existing evaluati on approaches. Since the \\nintroduction of RAG in 2020 (Lewis et al., 2020), it \\nhas taken considerable time for methods to be \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n146'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='developed and established in the literature for \\nevaluating these approaches. Despite the rigorous \\nmethodology, it remains possible that some papers \\nwere overlooked due to the rapid pace of \\ndevelopments in the field.  \\nWith reference to RQ1, our evaluation framework \\nintroduces robust evaluation dimensions and metrics \\nto assess the different steps within RAG. Moreover, \\nthis paper advances the understanding of RAG \\nevaluation by providing reliable dimensions and \\nmetrics (Y. Gao et al., 2023; Wang et al., 2023). \\nFurthermore, Section 5 gives a comprehensive \\nsummary about RQ2 by providing an overview of the \\navailable datasets to apply the proposed evaluation \\ndimensions and metrics and what kind of \\nrequirements to consider.  \\nAs a future avenue of research, a practical \\napplication of these metrics should be conducted to \\nvalidate their use and alignment with human \\npreferences. In addition, the provision of all the \\nmetrics presented could be examined within a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='application of these metrics should be conducted to \\nvalidate their use and alignment with human \\npreferences. In addition, the provision of all the \\nmetrics presented could be examined within a \\nframework to simplify practical application. \\nACKNOWLEDGEMENTS \\nThe presented paper was produced as part of the \\nresearch project MoFaPro. This project is funded by \\nthe AUDI AG. The present  approach was developed \\nwithin the institute \"AImotion Bavaria\" at the \\nTechnische Hochschule Ingolstadt. This work is part \\nof Oğuz Caymazer\\'s master\\'s thesis and was \\nconducted during his internship at the AUDI AG. \\nREFERENCES \\nAdlakha, V., Behnamghader, P., Lu, x. H., Meade, n., & \\nReddy, S. (2023). Evaluating correctness and \\nfaithfulness of instruction-following models for \\nquestion answering. Https://doi.org/10.48550/arxiv.2 \\n307.16877 \\nAmouyal, S. J., Wolfson, T ., Rubin, O., Yoran, O., \\nHerzig, J., & Be rant, J. (2022). QAMPARI: An Open-\\ndomain Question Answering Benchmark for Questions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='307.16877 \\nAmouyal, S. J., Wolfson, T ., Rubin, O., Yoran, O., \\nHerzig, J., & Be rant, J. (2022). QAMPARI: An Open-\\ndomain Question Answering Benchmark for Questions \\nwith Many Answers from Multiple Paragraphs. \\nhttps://doi.org/10.48550/arXiv.2205.12665 \\nAsai, A., Wu, Z., Wang, Y [Yizhong], Sil, A., & \\nHajishirzi, H. (2023). Self-RAG: Learning to Retrieve, \\nGenerate, and Critique through Self-Reflection. \\nhttps://doi.org/10.48550/arXiv.2310.11511 \\nAsai, A., Zhong, Z., Chen, D [Danqi], Koh, P. W., \\nZettlemoyer, L., Hajishirzi, H., & Yih, W. (2024). \\nReliable, Adaptable, and Attributable Language \\nModels with Retrieval. https://doi.org/10.48 \\n550/arXiv.2403.03187 \\nBell, E., Bryman, A., & Harley, B. (2019). Business \\nresearch methods  (Fifth edition). Oxford University \\nPress.  \\nBenbya, H., Strich, F., & Ta mm, T. (2024). Navigating \\nGenerative Artificial Intelligence Promises and Perils \\nfor Knowledge and Creative Work. Journal of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Press.  \\nBenbya, H., Strich, F., & Ta mm, T. (2024). Navigating \\nGenerative Artificial Intelligence Promises and Perils \\nfor Knowledge and Creative Work. Journal of the \\nAssociation for Information Systems , 25(1), 23–36. \\nhttps://doi.org/10.17705/1jais.00861 \\nBrocke, J., Simons, A., Niehaves, B., Riemer, K., \\nPlattfaut, R., & Cleven, A. (2009). Reconstructing the \\nGiant: On the Importance of Rigour in Documenting \\nthe Literature Search Process. In European Conference \\non Information Systems (Chair), 17th European Conf. \\non Information Systems . https://www.wi.uni-\\nmuenster.de/research/publications/3069 \\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., \\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., \\nLundberg, S., Nori, H., Palangi, H., Ri beiro, M. T., & \\nZhang, Y [Yi]. (2023). Sparks of Artificial General \\nIntelligence: Early experiments with GPT-4. \\nhttps://doi.org/10.48550/arXiv.2303.12712 \\nEs, S., James, J., Espinosa-Anke, L., & Schockaert, S.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Zhang, Y [Yi]. (2023). Sparks of Artificial General \\nIntelligence: Early experiments with GPT-4. \\nhttps://doi.org/10.48550/arXiv.2303.12712 \\nEs, S., James, J., Espinosa-Anke, L., & Schockaert, S. \\n(2023). RAGAS: Automated Evaluation of Retrieval \\nAugmented Generation. https://doi.org/10.48550/a \\nrXiv.2309.15217 \\nGao, T., Yen, H., Yu, J., & Chen, D [Danqi]. (2023). \\nEnabling Large Language Models to Generate Text \\nwith Citations. https://doi.org/10.48550/arXiv.2305. \\n14627 \\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y \\n[Yi], Sun, J., Wang, M., & Wang, H. (2023). Retrieval-\\nAugmented Generation for Large Language Models: A \\nSurvey. https://doi.org/10.48550/arXiv.2312.10997 \\nGuinet, G., Omidvar-Tehrani, B., Deoras, A., & Callot, L. \\n(2024). Automated Evaluation of Retrieval-Augmented \\nLanguage Models with Task-Specific Exam \\nGeneration. https://github.com/amazon-science/auto-\\nrag-eval https://doi.org/10.48550/arXiv.2405.13622'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(2024). Automated Evaluation of Retrieval-Augmented \\nLanguage Models with Task-Specific Exam \\nGeneration. https://github.com/amazon-science/auto-\\nrag-eval https://doi.org/10.48550/arXiv.2405.13622 \\nHammond, G. (2024, April 10). Speed of AI development \\nstretches risk assessments to breaking point. Financial \\nTimes. https://www.ft.com/content/499c8935-f46e-\\n4ec8-a8e2-19e07e3b0438 \\nHu, X [Xiangkun], Ru, D., Qiu, L., Guo, Q., Zhang, T., \\nXu, Y., Luo, Y., Liu, P., Zhang, Y [Yue], & Zhang, Z \\n[Zheng]. (2024). RefChecker: Reference-based Fine-\\ngrained Hallucination Checker and Benchmark for \\nLarge Language Models. https://doi.org/10.48550/a \\nrXiv.2405.14486 \\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., \\nPetroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., \\nRiedel, S., & Grave, E. (2022). Atlas: Few-shot \\nLearning with Retrieval Augmented Language Models. \\nhttps://doi.org/10.48550/arXiv.2208.03299 \\nKandpal, N., Deng, H., R oberts, A., Wa llace, E., &'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Riedel, S., & Grave, E. (2022). Atlas: Few-shot \\nLearning with Retrieval Augmented Language Models. \\nhttps://doi.org/10.48550/arXiv.2208.03299 \\nKandpal, N., Deng, H., R oberts, A., Wa llace, E., & \\nRaffel, C. (2022). Large Language Models Struggle to \\nLearn Long-Tail Knowledge. https://doi.org/10.48 \\n550/arXiv.2211.08411 \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n147'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Karpukhin, V., Oğuz, B., Min , S., Lewis, P., Wu, L., \\nEdunov, S., Chen, D [Danqi], & Yih, W. (2020). Dense \\nPassage Retrieval for Open-Domain Question \\nAnswering. https://doi.org/10.48550/arXiv.2004.04906 \\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., \\nParikh, A., Alberti, C., Eps tein, D., Polosukhin, I., \\nDevlin, J., Lee, K., Toutanova, K., Jones, L., \\nKelcey, M., Chang, M.‑W., Dai, A. M., Uszkoreit, J., \\nLe, Q., & Petrov, S. (2019). Natural Questions: A \\nBenchmark for Question Answering Research. \\nTransactions of the Association for Computational \\nLinguistics, 7, 453–466. https://doi.org/10.116 \\n2/tacl_a_00276 \\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., \\nGoyal, N., Küttler, H., Lewis, M., Yih, W., \\nRocktäschel, T., Riedel , S., & Kiela, D. (2020). \\nRetrieval-Augmented Generation for Knowledge-\\nIntensive NLP Tasks. https://doi.org/10.4 \\n8550/arXiv.2005.11401 \\nMa, X., Gong, Y., He, P., Zhao, H., & Duan, N. (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Retrieval-Augmented Generation for Knowledge-\\nIntensive NLP Tasks. https://doi.org/10.4 \\n8550/arXiv.2005.11401 \\nMa, X., Gong, Y., He, P., Zhao, H., & Duan, N. (2023). \\nQuery Rewriting for Retrieval-Augmented Large \\nLanguage Models. https://doi.org/10.48550/arXiv. \\n2305.14283 \\nMallen, A., Asai, A., Zhong, V.,  Das, R., Khashabi, D., & \\nHajishirzi, H. (2022). When Not to Trust Language \\nModels: Investigating Effectiveness of Parametric and \\nNon-Parametric Memories. https://doi.org/10.48550/ \\narXiv.2212.10511 \\nMin, S., Krishna, K., Lyu, X., Lewis, M., Yih, W., Koh, P., \\nIyyer, M., Zettlemoyer, L.,  & Hajishirzi, H. (2023). \\nFActScore: Fine-grained Atomic Evaluation of Factual \\nPrecision in Long Form Text Generation. In H. \\nBouamor, J. Pino, & K. Bali (Eds.), Proceedings of the \\n2023 Conference on Empirical Methods in Natural \\nLanguage Processing (pp. 12076–12100). Association \\nfor Computational Linguistics. \\nhttps://doi.org/10.18653/v1/2023.emnlp-main.741'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='2023 Conference on Empirical Methods in Natural \\nLanguage Processing (pp. 12076–12100). Association \\nfor Computational Linguistics. \\nhttps://doi.org/10.18653/v1/2023.emnlp-main.741 \\nOpenAI, Achiam, J., Adler, S. , Agarwal, S., Ahmad, L., \\nAkkaya, I., Aleman, F. L., Almeida, D., \\nAltenschmidt, J., Altman, S. , Anadkat, S., Avila, R., \\nBabuschkin, I., Balaji, S., Balcom, V., Baltescu, P., \\nBao, H., Bavarian, M., Belgum, J., . . . Zoph, B. \\n(2023). GPT-4 Technical Report. \\nhttps://doi.org/10.48550/arXiv.2303.08774 \\nParé, G., Tate, M., Johnstone, D., & Kitsiou, S. (2016). \\nContextualizing the twin concepts of systematicity and \\ntransparency in information systems literature reviews. \\nEuropean Journal of Information Systems, 25(6), 493–\\n508. https://doi.org/10.1057/s41303-016-0020-3 \\nRackauckas, Z., Câmara, A., & Zavrel, J. (2024). \\nEvaluating RAG-Fusion with RAGElo: an Automated \\nElo-based Framework. https://doi.org/10.48550/ \\narXiv.2406.14783'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"Rackauckas, Z., Câmara, A., & Zavrel, J. (2024). \\nEvaluating RAG-Fusion with RAGElo: an Automated \\nElo-based Framework. https://doi.org/10.48550/ \\narXiv.2406.14783 \\nRajpurkar, P., Jia, R., & Liang, P. (2018). Know What You \\nDon't Know: Unanswerable Questions for SQuAD. \\nhttps://doi.org/10.48550/arXiv.1806.03822 \\nRau, D., Déjean, H.,  Chirkova, N., Form al, T., Wa ng, S., \\nNikoulina, V., & Clinchant, S. (2024). BERGEN: A \\nBenchmarking Library fo r Retrieval-Augmented \\nGeneration. https://doi.org/10.48550/arXiv.2407. \\n01102 \\nRavi, S. S., Mielczarek, B., Kannappan, A., Kiela, D., & \\nQian, R. (2024). Lynx: An Open Source Hallucination \\nEvaluation Model. https://arxiv.org/abs/2407.08488  \\nSaad-Falcon, J., Khattab, O., Potts, C., & Zaharia, M. \\n(2023). ARES: An Automated Evaluation Framework \\nfor Retrieval-Augmented Generation Systems. \\nhttps://doi.org/10.48550/arXiv.2311.09476 \\nWang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., \\nJiayang, C., Yao, Y., Gao, W., Hu, X [Xuming], Qi, Z.,\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='for Retrieval-Augmented Generation Systems. \\nhttps://doi.org/10.48550/arXiv.2311.09476 \\nWang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., \\nJiayang, C., Yao, Y., Gao, W., Hu, X [Xuming], Qi, Z., \\nWang, Y [Yidong], Yang, L., Wang, J [Jindong], \\nXie, X., Zhang, Z [Zheng], & Zhang, Y [Yue]. (2023). \\nSurvey on Factuality in Large Language Models: \\nKnowledge, Retrieval and Domain-Specificity. \\nhttps://doi.org/10.48550/arXiv.2310.07521 \\nWebster, J., & Watson, R. T. (2002). Analyzing the Past to \\nPrepare for the Future: Writing a Literature Review. \\nMIS Quarterly, 26(2), xiii–xxiii. https://www.jstor.org/ \\nstable/4132319?seq=1#metadata_info_tab_contents \\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., \\nSalakhutdinov, R., & Manning, C. D. (2018). \\nHotpotQA: A Dataset for Diverse, Explainable Multi-\\nhop Question Answering. https://doi.org/10.48550/ \\narXiv.1809.09600 \\nYu, H., Gan, A., Zhang, K., Tong, S., Liu, Q., & Liu, Z. \\n(2024). Evaluation of Retr ieval-Augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='hop Question Answering. https://doi.org/10.48550/ \\narXiv.1809.09600 \\nYu, H., Gan, A., Zhang, K., Tong, S., Liu, Q., & Liu, Z. \\n(2024). Evaluation of Retr ieval-Augmented \\nGeneration: A Survey. https://doi.org/10.48550/ \\narXiv.2405.07437 \\nZhang, Z [Zihan], Fang, M., & Chen, L. (2024). \\nRetrievalQA: Assessing Adaptive Retrieval-Augmented \\nGeneration for Short-form Open-Domain Question \\nAnswering. https://doi.org/10.48550/arXiv.2402.16457 \\nZhang, Z [Zihan], Fang, M., Chen, L., Namazi-Rad, M.‑R., \\n& Wang, J [Jun]. (2023). How Do Large Language \\nModels Capture the Ever-changing World Knowledge? \\nA Review of Recent Advances. https://doi.org/10. \\n48550/arXiv.2310.07343      \\n \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n148'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented Generation (RAG) has\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents innovative opportunities for the conceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. I NTRODUCTION\\nL'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='of RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. I NTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='constituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='mance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, Advanced\\nRAG paradigm focuses on optimizing the retrieval phase,\\naiming to enhance retrieval efficiency and strengthen the\\nutilization of retrieved chunks. As shown in Figure 1 ,typical'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='RAG paradigm focuses on optimizing the retrieval phase,\\naiming to enhance retrieval efficiency and strengthen the\\nutilization of retrieved chunks. As shown in Figure 1 ,typical\\nstrategies involve pre-retrieval processing and post-retrieval\\nprocessing. For instance, query rewriting is used to make\\nthe queries more clear and specific, thereby increasing the\\naccuracy of retrieval [10], and the reranking of retrieval results\\nis employed to enhance the LLM’s ability to identify and\\nutilize key information [11].\\nDespite the improvements in the practicality of Advanced\\nRAG, there remains a gap between its capabilities and real-\\nworld application requirements. On one hand, as RAG tech-\\nnology advances, user expectations rise, demands continue to\\nevolve, and application settings become more complex. For\\ninstance, the integration of heterogeneous data and the new\\ndemands for system transparency, control, and maintainability.\\nOn the other hand, the growth in application demands has'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='instance, the integration of heterogeneous data and the new\\ndemands for system transparency, control, and maintainability.\\nOn the other hand, the growth in application demands has\\nfurther propelled the evolution of RAG technology.\\nAs shown in Figure 2, to achieve more accurate and efficient\\ntask execution, modern RAG systems are progressively inte-\\ngrating more sophisticated function, such as organizing more\\nrefined index base in the form of knowledge graphs, integrat-\\ning structured data through query construction methods, and\\nemploying fine-tuning techniques to enable encoders to better\\nadapt to domain-specific documents.\\nIn terms of process design, the current RAG system has\\nsurpassed the traditional linear retrieval-generation paradigm.\\nResearchers use iterative retrieval [12] to obtain richer con-\\ntext, recursive retrieval [13] to handle complex queries, and\\nadaptive retrieval [14] to provide overall autonomy and flex-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Researchers use iterative retrieval [12] to obtain richer con-\\ntext, recursive retrieval [13] to handle complex queries, and\\nadaptive retrieval [14] to provide overall autonomy and flex-\\nibility. This flexibility in the process significantly enhances\\narXiv:2407.21059v1  [cs.CL]  26 Jul 2024'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='2\\nFig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\\nquestions, both encounter limitations and struggle to provide satisfactory\\nanswers. Despite the fact that Advanced RAG improves retrieval accuracy\\nthrough hierarchical indexing, pre-retrieval, and post-retrieval processes, these\\nrelevant documents have not been used correctly.\\nthe expressive power and adaptability of RAG systems, en-\\nabling them to better adapt to various application scenarios.\\nHowever, this also makes the orchestration and scheduling of\\nworkflows more complex, posing greater challenges to system\\ndesign. Specifically, RAG currently faces the following new\\nchallenges:\\nComplex data sources integration. RAG are no longer\\nconfined to a single type of unstructured text data source but\\nhave expanded to include various data types, such as semi-\\nstructured data like tables and structured data like knowledge\\ngraphs [15]. Access to heterogeneous data from multiple'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='have expanded to include various data types, such as semi-\\nstructured data like tables and structured data like knowledge\\ngraphs [15]. Access to heterogeneous data from multiple\\nsources can provide the system with a richer knowledge\\nbackground, and more reliable knowledge verification capa-\\nbilities [16].\\nNew demands for system interpretability, controllability,\\nFig. 2. Case of current Modular RAG.The system integrates diverse data\\nand more functional components. The process is no longer confined to linear\\nbut is controlled by multiple control components for retrieval and generation,\\nmaking the entire system more flexible and complex.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='3\\nand maintainability. With the increasing complexity of sys-\\ntems, system maintenance and debugging have become more\\nchallenging. Additionally, when issues arise, it is essential to\\nquickly pinpoint the specific components that require opti-\\nmization.\\nComponent selection and optimization. More neural net-\\nworks are involved in the RAG system, necessitating the\\nselection of appropriate components to meet the needs of spe-\\ncific tasks and resource configurations. Moreover, additional\\ncomponents enhance the effectiveness of RAG but also bring\\nnew collaborative work requirements [17]. Ensuring that these\\nmodels perform as intended and work efficiently together to\\nenhance the overall system performance is crucial.\\nWorkflow orchestration and scheduling. Components\\nmay need to be executed in a specific order, processed in paral-\\nlel under certain conditions, or even judged by the LLM based\\non different outputs. Reasonable planning of the workflow is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='may need to be executed in a specific order, processed in paral-\\nlel under certain conditions, or even judged by the LLM based\\non different outputs. Reasonable planning of the workflow is\\nessential for improving system efficiency and achieving the\\ndesired outcomes [18].\\nTo address the design, management, and maintenance chal-\\nlenges posed by the increasing complexity of RAG systems,\\nand to meet the ever-growing and diverse demands and ex-\\npectations, this paper proposes Modular RAG architecture.\\nIn modern computing systems, modularization is becoming\\na trend. It can enhance the system’s scalability and maintain-\\nability and achieve efficient task execution through process\\ncontrol.\\nThe Modular RAG system consists of multiple independent\\nyet tightly coordinated modules, each responsible for handling\\nspecific functions or tasks. This architecture is divided into\\nthree levels: the top level focuses on the critical stages of\\nRAG, where each stage is treated as an independent module.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='specific functions or tasks. This architecture is divided into\\nthree levels: the top level focuses on the critical stages of\\nRAG, where each stage is treated as an independent module.\\nThis level not only inherits the main processes from the\\nAdvanced RAG paradigm but also introduces an orchestration\\nmodule to control the coordination of RAG processes. The\\nmiddle level is composed of sub-modules within each module,\\nfurther refining and optimizing the functions. The bottom level\\nconsists of basic units of operation—operators. Within the\\nModular RAG framework, RAG systems can be represented\\nin the form of computational graphs, where nodes represent\\nspecific operators. The comparison of the three paradigms is\\nshown in the Figure 3. Modular RAG evolves based on the\\nprevious development of RAG. The relationships among these\\nthree paradigms are ones of inheritance and development.\\nAdvanced RAG is a special case of Modular RAG, while Naive\\nRAG is a special case of Advanced RAG.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='three paradigms are ones of inheritance and development.\\nAdvanced RAG is a special case of Modular RAG, while Naive\\nRAG is a special case of Advanced RAG.\\nThe advantages of Modular RAG are significant, as it\\nenhances the flexibility and scalability of RAG systems. Users\\ncan flexibly combine different modules and operators accord-\\ning to the requirements of data sources and task scenarios. In\\nsummary, the contributions of this paper are as follows:\\n• This paper proposes a new paradigm called modular\\nRAG, which employs a three-tier architectural design\\ncomprising modules, sub-modules, and operators to de-\\nfine the RAG system in a unified and structured manner.\\nThis design not only enhances the system’s flexibility and\\nscalability but also, through the independent design of\\noperators, strengthens the system’s maintainability and\\ncomprehensibility.\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='operators, strengthens the system’s maintainability and\\ncomprehensibility.\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which\\ncan flexibly express current RAG methods. This paper has\\nfurther summarized six typical flow patterns and specific\\nmethods have been analyzed to reveal the universality of\\nmodular RAG in practical scenarios.\\n• The Modular RAG framework offers exceptional flexi-\\nbility and extensibility. This paper delves into the new\\nopportunities brought by Modular RAG and provides a\\nthorough discussion on the adaptation and expansion of\\nnew methods in different application scenarios, offering\\nguidance for future research directions and practical ex-\\nploration.\\nII. R ELATED WORK\\nThe development of RAG technology can be summarized\\nin three stages. Initially, retrieval-augmented techniques were\\nintroduced to improve the performance of pre-trained lan-\\nguage models on knowledge-intensive tasks [19], [20]. In'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='in three stages. Initially, retrieval-augmented techniques were\\nintroduced to improve the performance of pre-trained lan-\\nguage models on knowledge-intensive tasks [19], [20]. In\\nspecific implementations, Retro [21] optimized pre-trained\\nautoregressive models through retrieval augmentation, while\\nAtlas [22] utilized a retrieval-augmented few-shot fine-tuning\\nmethod, enabling language models to adapt to diverse tasks.\\nIRCOT [23] further enriched the reasoning process during\\nthe inference phase by combining chain-of-thought and multi-\\nstep retrieval processes. Entering the second stage, as the\\nlanguage processing capabilities of LLMs significantly im-\\nproved, retrieval-augmented techniques began to serve as a\\nmeans of supplementing additional knowledge and providing\\nreferences, aiming to reduce the hallucination. For instance,\\nRRR [24] improved the rewriting phase, and LLMlingua [25]\\nremoved redundant tokens in retrieved document chunks.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='references, aiming to reduce the hallucination. For instance,\\nRRR [24] improved the rewriting phase, and LLMlingua [25]\\nremoved redundant tokens in retrieved document chunks.\\nWith the continuous progress of RAG technology, research\\nhas become more refined and focused, while also achieving\\ninnovative integration with other technologies such as graph\\nneural networks [26] and fine-tuning techniques [27]. The\\noverall pipeline has also become more flexible, such as using\\nLLMs to proactively determine the timing of retrieval and\\ngeneration [14], [28].\\nThe development of RAG technology has been acceler-\\nated by LLM technology and practical application needs.\\nResearchers are examining and organizing the RAG frame-\\nwork and development pathways from different perspectives.\\nBuilding upon the enhanced stages of RAG, Gao et al., [2] sub-\\ndivided RAG into enhancement during pre-training, inference,\\nand fine-tuning stages. Based on the main processes of RAG,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Building upon the enhanced stages of RAG, Gao et al., [2] sub-\\ndivided RAG into enhancement during pre-training, inference,\\nand fine-tuning stages. Based on the main processes of RAG,\\nrelevant works on RAG were organized from the perspectives\\nof retrieval, generation, and augmentation methods. Huang\\net al., [29] categorize RAG methods into four main classes:\\npre-retrieval, retrieval, post-retrieval, generation, and provide\\na detailed discussion of the methods and techniques within\\neach class. Hu et al., [30] discuss Retrieval-Augmented Lan-\\nguage Models (RALMs) form three key components, including\\nretrievers, language models, augmentations, and how their\\ninteractions lead to different model structures and applications.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='4\\nFig. 3. Comparison between three RAG paradigms. Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG\\nsystems.\\nThey emphasize the importance of considering robustness,\\naccuracy, and relevance when evaluating RALMs and pro-\\npose several evaluation methods. Ding et al., [31] provide a\\ncomprehensive review from the perspectives of architecture,\\ntraining strategies, and applications. They specifically discuss\\nfour training methods of RALMs: training-free methods, in-\\ndependent training methods, sequence training methods, and\\njoint training methods, and compare their advantages and\\ndisadvantages. Zhao et al., [32]analyze the applications of\\nRAG technology in various fields such as text generation,\\ncode generation, image generation, and video generation from\\nthe perspective of augmented intelligence with generative\\ncapabilities.\\nThe current collation of RAG systems primarily focuses\\non methods with a fixed process, mainly concerned with'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the perspective of augmented intelligence with generative\\ncapabilities.\\nThe current collation of RAG systems primarily focuses\\non methods with a fixed process, mainly concerned with\\noptimizing the retrieval and generation stages. However, it has\\nnot turned its attention to the new characteristics that RAG\\nresearch is continuously evolving, namely the characteristics\\nof process scheduling and functional componentization. There\\nis currently a lack of comprehensive analysis of the overall\\nRAG system, which has led to research on paradigms lagging\\nbehind the development of RAG technology.\\nIII. F RAMEWORK AND NOTATION\\nFor query Q = {qi}, a typical RAG system mainly consists\\nof three key components. 1) Indexing. Given documents D =\\n{d1, d2, . . . , dn} , where di represents the document chunk.\\nIndexing is the process of converting di into vectors through\\nan embedding model fe(·) , and then store vectors in vector\\ndatabase.\\nI = {e1, e2, . . . , en} and e i = fe(di) ∈ Rd (1)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Indexing is the process of converting di into vectors through\\nan embedding model fe(·) , and then store vectors in vector\\ndatabase.\\nI = {e1, e2, . . . , en} and e i = fe(di) ∈ Rd (1)\\nNotation Description\\nq The original query\\ny The output of LLM\\nD A document retrieval repository composed of chunks di.\\nR(q, D) Retriever,find similar chunks from D based on q.\\nF RAG Flow\\nP RAG Flow pattern\\nfqe Query expansion function\\nfqc Query transform function\\nfcomp Chunk compression function\\nfsel Chunk selection function\\nfr Routing function\\nM Module in modular RAG\\nop The specific operators within the Module.\\nTABLE I\\nIMPORTANT NOTATION\\n2) Retrieval . Transform the query into a vector using the\\nsame encoding model, and then filter out the top k document\\nchunks that are most similar based on vector similarity.\\nR : topk\\ndi∈D\\nSim(q, di) → Dq (2)\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='R : topk\\ndi∈D\\nSim(q, di) → Dq (2)\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.\\nSim(q, di) = eq · edi or eq · edi\\n∥eq∥ · ∥edi∥ (3)\\n3) Generation . After getting the relevant documents. The\\nquery q and the retrieved document Dq chunks are inputted\\ntogether to the LLM to generate the final answer, where [·, ·]\\nstands for concatenation.\\ny = LLM([Dq, q]) (4)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='5\\nWith the evolution of RAG technology, more and more func-\\ntional components are being integrated into systems. Modular\\nRAG paradigm includes three levels, ranging from large to\\nsmall:\\nL1 Module (M = {Ms}). The core process in RAG\\nsystem.\\nL2 Sub-module (Ms = {Op}).The functional modules in\\nmodule.\\nL3 Operator (Op = {fθi}). The the specific functional\\nimplementation in a module or sub-module. As a result, a\\nModular RAG system can be represented as:\\nG = {q, D,M, {Ms}, {Op}} (5)\\nThe arrangement between modules and operators constitutes\\nthe RAG Flow F = ( Mϕ1 , . . . , Mϕn) where ϕ stands for\\nthe set of module parameters. A modular rag flow can be\\ndecomposed into a graph of sub-functions. In the simplest\\ncase,the graph is a linear chain.\\nNaiveRAG : q\\nR(q,D)\\n− − − − − − − − − − − →\\nText−Embedding\\nDq LLM([q,Dq])\\n− − − − − − − − − − − →\\nOpenAI/GPT −4\\ny\\n(6)\\nIV. M ODULE AND OPERATOR\\nThis chapter will specifically introduce modules and op-\\nerators under the Modular RAG framework. Based on the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Dq LLM([q,Dq])\\n− − − − − − − − − − − →\\nOpenAI/GPT −4\\ny\\n(6)\\nIV. M ODULE AND OPERATOR\\nThis chapter will specifically introduce modules and op-\\nerators under the Modular RAG framework. Based on the\\ncurrent stage of RAG development, we have established\\nsix main modules: Indexing, Pre-retrieval, Retrieval, Post-\\nretrieval, Generation, and Orchestration.\\nA. Indexing\\nIndexing is the process of split document into manageable\\nchunks and it is a key step in organizing a system. Indexing\\nfaces three main challenges. 1) Incomplete content represen-\\ntation.The semantic information of chunks is influenced by the\\nsegmentation method, resulting in the loss or submergence of\\nimportant information within longer contexts. 2) Inaccurate\\nchunk similarity search . As data volume increases, noise in\\nretrieval grows, leading to frequent matching with erroneous\\ndata, making the retrieval system fragile and unreliable. 3)\\nUnclear reference trajectory. The retrieved chunks may orig-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='retrieval grows, leading to frequent matching with erroneous\\ndata, making the retrieval system fragile and unreliable. 3)\\nUnclear reference trajectory. The retrieved chunks may orig-\\ninate from any document, devoid of citation trails, potentially\\nresulting in the presence of chunks from multiple different\\ndocuments that, despite being semantically similar, contain\\ncontent on entirely different topics.\\n1) Chunk Optimization: The size of the chunks and the\\noverlap between the chunks play a crucial role in the overall\\neffectiveness of the RAG system. Given a chunk di, its chunk\\nsize is denoted as Li = |di|, and the overlap is denoted as\\nLo\\ni = |di ∩ di+1|. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise [17].\\nSliding Window using overlapping chunks in a sliding win-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='time and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise [17].\\nSliding Window using overlapping chunks in a sliding win-\\ndow enhances semantic transitions. However, it has limitations\\nsuch as imprecise context size control, potential truncation of\\nwords or sentences, and lacking semantic considerations.\\nMetadata Attachment. Chunks can be enriched with meta-\\ndata like page number, file name, author, timestamp, sum-\\nmary, or relevant questions. This metadata allows for filtered\\nretrieval, narrowing the search scope.\\nSmall-to-Big [33] separate the chunks used for retrieval\\nfrom those used for synthesis. Smaller chunks enhance re-\\ntrieval accuracy, while larger chunks provide more context.\\nOne approach is to retrieve smaller summarized chunks and\\nreference their parent larger chunks. Alternatively, individual\\nsentences could be retrieved along with their surrounding text.\\n2) Structure Organization: One effective method for en-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='reference their parent larger chunks. Alternatively, individual\\nsentences could be retrieved along with their surrounding text.\\n2) Structure Organization: One effective method for en-\\nhancing information retrieval is to establish a hierarchical\\nstructure for the documents. By constructing chunks structure,\\nRAG system can expedite the retrieval and processing of\\npertinent data.\\nHierarchical Index . In the hierarchical structure of docu-\\nments, nodes are arranged in parent-child relationships, with\\nchunks linked to them. Data summaries are stored at each\\nnode, aiding in the swift traversal of data and assisting the\\nRAG system in determining which chunks to extract. This\\napproach can also mitigate the illusion caused by chunk\\nextraction issues. The methods for constructing a structured\\nindex primarily include: 1) Structural awareness based on\\nparagraph and sentence segmentation in docs. 2) Content\\nawareness based on inherent structure in PDF, HTML, and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='index primarily include: 1) Structural awareness based on\\nparagraph and sentence segmentation in docs. 2) Content\\nawareness based on inherent structure in PDF, HTML, and\\nLatex. 3) Semantic awareness based on semantic recognition\\nand segmentation of text.\\nKG Index [34]. Using Knowledge Graphs (KGs) to struc-\\nture documents helps maintain consistency by clarifying con-\\nnections between concepts and entities, reducing the risk of\\nmismatch errors. KGs also transform information retrieval\\ninto instructions intelligible to language models, improving re-\\ntrieval accuracy and enabling contextually coherent responses.\\nThis enhances the overall efficiency of the RAG system.\\nFor example, organizing a corpus in the format of graph\\nG = {V, E, X}, where node V = {vi}n\\ni=1 represent document\\nstructures (e.g.passage, pages, table) , edge E ⊂ V × Vrep-\\nresent semantic or lexical similarity and belonging relations,\\nand node features X = {Xi}n\\ni=1 represent text or markdown\\ncontent for passage.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='resent semantic or lexical similarity and belonging relations,\\nand node features X = {Xi}n\\ni=1 represent text or markdown\\ncontent for passage.\\nB. Pre-retrieval\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nThe primary challenges in this module include: 1) Poorly\\nworded queries . The question itself is complex, and the\\nlanguage is not well-organized. 2) Language complexity and\\nambiguity. Language models often struggle when dealing\\nwith specialized vocabulary or ambiguous abbreviations with\\nmultiple meanings. For instance, they may not discern whether\\nLLM refers to Large Language Model or a Master of Laws in\\na legal context.\\n1) Query Expansion : Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='6\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nfqe(q) = {q1, q2, . . . , qn} ∀qi ∈ {q1, q2, . . . , qn}, qi /∈ Q\\n(7)\\nMulti-Query uses prompt engineering to expand queries\\nvia LLMs, allowing for parallel execution. These expansions\\nare meticulously designed to ensure diversity and coverage.\\nHowever, this approach can dilute the user’s original intent.\\nTo mitigate this, the model can be instructed to assign greater\\nweight to the original query.\\nSub-Query. By decomposing and planning for complex\\nproblems, multiple sub-problems are generated. Specifically,\\nleast-to-most prompting [35] can be employed to decom-\\npose the complex problem into a series of simpler sub-\\nproblems. Depending on the structure of the original problem,\\nthe generated sub-problems can be executed in parallel or\\nsequentially. Another approach involves the use of the Chain-\\nof-Verification (CoVe) [36]. The expanded queries undergo'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the generated sub-problems can be executed in parallel or\\nsequentially. Another approach involves the use of the Chain-\\nof-Verification (CoVe) [36]. The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing hallu-\\ncinations.\\n2) Query Transformation: Retrieve and generate based on\\na transformed query instead of the user’s original query.\\nfqt(q) = q′ (8)\\nRewrite. Original queries often fall short for retrieval in\\nreal-world scenarios. To address this, LLMs can be prompted\\nto rewrite. Specialized smaller models can also be employed\\nfor this purpose [24]. The implementation of the query rewrite\\nmethod in Taobao has significantly improved recall effective-\\nness for long-tail queries, leading to an increase in GMV [10].\\nHyDE [37]. In order to bridge the semantic gap between\\nquestions and answers, it constructs hypothetical documents\\n(assumed answers) when responding to queries instead of\\ndirectly searching the query. It focuses on embedding simi-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='questions and answers, it constructs hypothetical documents\\n(assumed answers) when responding to queries instead of\\ndirectly searching the query. It focuses on embedding simi-\\nlarity from answer to answer rather than seeking embedding\\nsimilarity for the problem or query. In addition, it also in-\\ncludes reverse HyDE, which generate hypothetical query for\\neach chunks and focuses on retrieval from query to query.\\nStep-back Prompting [38]. The original query is abstracted\\ninto a high-level concept question (step-back question). In the\\nRAG system, both the step-back question and the original\\nquery are used for retrieval, and their results are combined\\nto generate the language model’s answer.\\n3) Query Construction: In addition to text data, an in-\\ncreasing amount of structured data, such as tables and graph\\ndata, is being integrated into RAG systems. To accommodate\\nvarious data types, it is necessary to restructure the user’s\\nquery. This involve converting the query into another query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='data, is being integrated into RAG systems. To accommodate\\nvarious data types, it is necessary to restructure the user’s\\nquery. This involve converting the query into another query\\nlanguage to access alternative data sources, with common\\nmethods including Text-to-SQL or Text-to-Cypher . In many\\nscenarios, structured query languages (e.g., SQL, Cypher)\\nare often used in conjunction with semantic information and\\nmetadata to construct more complex queries.\\nfqc(q) = q∗, q∗ ∈ Q∗ = {SQL, Cypher, . . .} (9)\\nC. Retrieval\\nThe retrieval process is pivotal in RAG systems. By lever-\\naging powerful embedding models, queries and text can be\\nefficiently represented in latent spaces, which facilitates the\\nestablishment of semantic similarity between questions and\\ndocuments, thereby enhancing retrieval. Three main consider-\\nations that need to be addressed include retrieval efficiency,\\nquality, and the alignment of tasks, data and models.\\n1) Retriever Selection: With the widespread adoption of'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='ations that need to be addressed include retrieval efficiency,\\nquality, and the alignment of tasks, data and models.\\n1) Retriever Selection: With the widespread adoption of\\nRAG technology, the development of embedding models has\\nbeen in full swing. In addition to traditional models based\\non statistics and pre-trained models based on the encoder\\nstructure, embedding models fine-tuned on LLMs have also\\ndemonstrated powerful capabilities [39]. However, they often\\ncome with more parameters, leading to weaker inference\\nand retrieval efficiency. Therefore, it is crucial to select the\\nappropriate retriever based on different task scenarios.\\nSparse Retriever uses statistical methods to convert queries\\nand documents into sparse vectors. Its advantage lies in its\\nefficiency in handling large datasets, focusing only on non-zero\\nelements. However, it may be less effective than dense vectors\\nin capturing complex semantics. Common methods include\\nTF-IDF and BM25.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='efficiency in handling large datasets, focusing only on non-zero\\nelements. However, it may be less effective than dense vectors\\nin capturing complex semantics. Common methods include\\nTF-IDF and BM25.\\nDense Retriever employs pre-trained language models\\n(PLMs) to provide dense representations of queries and doc-\\numents. Despite higher computational and storage costs, it\\noffers more complex semantic representations. Typical models\\ninclude BERT structure PLMs, like ColBERT, and multi-task\\nfine-tuned models like BGE [40] and GTE [41].\\nHybrid Retriever is to use both sparse and dense retrievers\\nsimultaneously. Two embedding techniques complement each\\nother to enhance retrieval effectiveness. Sparse retriever can\\nprovide initial screening results. Additionally, sparse models\\nenhance the zero-shot retrieval capabilities of dense models,\\nparticularly in handling queries with rare entities, thereby\\nincreasing system robustness.\\n2) Retriever Fine-tuning: In cases where the context may'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='particularly in handling queries with rare entities, thereby\\nincreasing system robustness.\\n2) Retriever Fine-tuning: In cases where the context may\\ndiverge from pre-trained corpus, particularly in highly special-\\nized fields like healthcare, law, and other domains abundant in\\nproprietary terminology. While this adjustment demands addi-\\ntional effort, it can substantially enhance retrieval efficiency\\nand domain alignment.\\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval\\nmodel based on labeled domain data is typically done using\\ncontrastive learning. This involves reducing the distance be-\\ntween positive samples while increasing the distance between\\nnegative samples. The commonly used loss calculation is\\nshown in the following:\\nL(DR) = − 1\\nT\\nTX\\ni=1\\nlog e(sim(qi,d+\\ni ))\\ne(sim(qi,d+\\ni )) + PN\\nj=1 e(sim(qi,d−\\ni ))\\n(10)\\nwhere d+\\ni is the positive sample document corresponding to\\nthe i-th query, d−\\ni is several negative sample, T is the total'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='TX\\ni=1\\nlog e(sim(qi,d+\\ni ))\\ne(sim(qi,d+\\ni )) + PN\\nj=1 e(sim(qi,d−\\ni ))\\n(10)\\nwhere d+\\ni is the positive sample document corresponding to\\nthe i-th query, d−\\ni is several negative sample, T is the total\\nnumber of queries, N is the number of negative samples, and\\nDR is the fine-tuning dataset.\\nLM-supervised Retriever (LSR) . In contrast to directly\\nconstructing a fine-tuning dataset from the dataset, LSR uti-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='7\\nlizes the LM-generated results as supervisory signals to fine-\\ntune the embedding model during the RAG process.\\nPLSR(d|q, y) = ePLM(y|d,q)/β\\nP\\nd′∈D ePLM(y|d,q)/β) (11)\\nPLM (y|d, q) is LM probability of the ground truth output y\\ngiven the input context d and query q, and β is a hyper-\\nparamter.\\nAdapter. At times, fine-tuning a large retriever can be\\ncostly, especially when dealing with retrievers based on LLMs\\nlike gte-Qwen. In such cases, it can mitigate this by incorpo-\\nrating an adapter module and conducting fine-tuning. Another\\nbenefit of adding an adapter is the ability to achieve better\\nalignment with specific downstream tasks [42].\\nD. Post-retrieval\\nFeeding all retrieved chunks directly into the LLM is not an\\noptimal choice. Post-processing the chunks can aid in better\\nleveraging the contextual information. The primary challenges\\ninclude: 1) Lost in the middle . Like humans, LLM tends\\nto remember only the beginning or the end of long texts,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='leveraging the contextual information. The primary challenges\\ninclude: 1) Lost in the middle . Like humans, LLM tends\\nto remember only the beginning or the end of long texts,\\nwhile forgetting the middle portion [43]. 2) Noise/anti-fact\\nchunks. Retrieved noisy or factually contradictory documents\\ncan impact the final retrieval generation [44]. 3) Context\\nWindow. Despite retrieving a substantial amount of relevant\\ncontent, the limitation on the length of contextual information\\nin large models prevents the inclusion of all this content.\\n1) Rerank: Rerank the retrieved chunks without altering\\ntheir content or length, to enhance the visibility of the more\\ncrucial document chunks. Given the retrieved set Dq and a\\nre-ranking method frerank to obtain the re-ranked set:\\nDq\\nr = frerank(q, Dq) = {d′\\n1, d′\\n2, . . . , d′\\nk}\\nwheref(d′\\n1) ≥ f(d′\\n2) ≥ . . .≥ f(d′\\nk). (12)\\nRule-base rerank. Metrics are calculated to rerank chunks\\naccording to certain rules. Common metrics include: diversity,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='1, d′\\n2, . . . , d′\\nk}\\nwheref(d′\\n1) ≥ f(d′\\n2) ≥ . . .≥ f(d′\\nk). (12)\\nRule-base rerank. Metrics are calculated to rerank chunks\\naccording to certain rules. Common metrics include: diversity,\\nrelevance and MRR (Maximal Marginal Relevance) [45]. The\\nidea is to reduce redundancy and increase result diversity.\\nMMR selects phrases for the final key phrase list based on a\\ncombined criterion of query relevance and information novelty.\\nModel-base rerank. Utilize a language model to reorder the\\ndocument chunks, commonly based on the relevance between\\nthe chunks and the query. Rerank models have become an\\nimportant component of RAG systems, and relevant model\\ntechnologies are also being iteratively upgraded. The scope\\nreordering has also been extended to multimodal data such as\\ntables and images [46].\\n2) Compression: A common misconception in the RAG\\nprocess is the belief that retrieving as many relevant docu-\\nments as possible and concatenating them to form a lengthy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='tables and images [46].\\n2) Compression: A common misconception in the RAG\\nprocess is the belief that retrieving as many relevant docu-\\nments as possible and concatenating them to form a lengthy\\nretrieval prompt is beneficial. However, excessive context can\\nintroduce more noise, diminishing the LLM’s perception of\\nkey information. A common approach to address this is to\\ncompress and select the retrieved content.\\nDq\\nc = fcomp(q, Dq), where|dqc\\ni | < |dq\\ni | ∀dq\\ni ∈ Dq (13)\\n(Long)LLMLingua [47]. By utilizing aligned and trained\\nsmall language models, such as GPT-2 Small or LLaMA-\\n7B, the detection and removal of unimportant tokens from\\nthe prompt is achieved, transforming it into a form that is\\nchallenging for humans to comprehend but well understood by\\nLLMs. This approach presents a direct and practical method\\nfor prompt compression, eliminating the need for additional\\ntraining of LLMs while balancing language integrity and\\ncompression ratio.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='LLMs. This approach presents a direct and practical method\\nfor prompt compression, eliminating the need for additional\\ntraining of LLMs while balancing language integrity and\\ncompression ratio.\\n3) Selection: Unlike compressing the content of document\\nchunks, Selection directly removes irrelevant chunks.\\nDq\\ns = fsel(Dq) = {di ∈ D | ¬P(di)} (14)\\nWhere fsel is the function for deletion operation and P(di) is\\na conditional predicate indicating that document ( di) satisfies\\na certain condition. If document ( di) satisfies ( P(di)), it will\\nbe deleted. Conversely, documents for which ( ¬P(di)) is true\\nwill be retained.\\nSelective Context. By identifying and removing redundant\\ncontent in the input context, the input is refined, thus improv-\\ning the language model’s reasoning efficiency. In practice, se-\\nlective context assesses the information content of lexical units\\nbased on the self-information computed by the base language\\nmodel. By retaining content with higher self-information, this'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='lective context assesses the information content of lexical units\\nbased on the self-information computed by the base language\\nmodel. By retaining content with higher self-information, this\\nmethod offers a more concise and efficient textual representa-\\ntion, without compromising their performance across diverse\\napplications. However, it overlooks the interdependence be-\\ntween compressed content and the alignment between the\\ntargeted language model and the small language model utilized\\nfor prompting compression [48].\\nLLM-Critique. Another straightforward and effective ap-\\nproach involves having the LLM evaluate the retrieved content\\nbefore generating the final answer. This allows the LLM\\nto filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [49], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nE. Generation\\nUtilize the LLM to generate answers based on the user’s'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='to self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nE. Generation\\nUtilize the LLM to generate answers based on the user’s\\nquery and the retrieved contextual information. Select an\\nappropriate model based on the task requirements, considering\\nfactors such as the need for fine-tuning, inference efficiency,\\nand privacy protection.\\n1) Generator Fine-tuning: In addition to direct LLM usage,\\ntargeted fine-tuning based on the scenario and data character-\\nistics can yield better results. This is also one of the greatest\\nadvantages of using an on-premise setup LLMs.\\nInstruct-Tuning. When LLMs lack data in a specific do-\\nmain, additional knowledge can be provided to the LLM\\nthrough fine-tuning. General fine-tuning dataset can also be\\nused as an initial step. Another benefit of fine-tuning is the\\nability to adjust the model’s input and output. For example, it\\ncan enable LLM to adapt to specific data formats and generate'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='used as an initial step. Another benefit of fine-tuning is the\\nability to adjust the model’s input and output. For example, it\\ncan enable LLM to adapt to specific data formats and generate\\nresponses in a particular style as instructed [50].\\nReinforcement learning. Aligning LLM outputs with hu-\\nman or retriever preferences through reinforcement learning is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='8\\na potential approach [51]. For instance, manually annotating\\nthe final generated answers and then providing feedback\\nthrough reinforcement learning. In addition to aligning with\\nhuman preferences, it is also possible to align with the\\npreferences of fine-tuned models and retrievers.\\nDual Fine-tuing Fine-tuning both generator and retriever\\nsimultaneously to align their preferences. A typical approach,\\nsuch as RA-DIT [27], aligns the scoring functions between\\nretriever and generator using KL divergence. Retrieval likeli-\\nhood of each retrieved document d is calculated as :\\nPR(d|q) = e(sim(d,q))/γP\\nd∈Dq e(sim(d,q)/γ (15)\\nPLM (y|d, q) is the LM probability of the ground truth output y\\ngiven the input context d, question q, and γ is a hyperparamter.\\nThe overall loss is calculated as:\\nL = 1\\n|T|\\nTX\\ni=1\\nKL(PR(d|q)||PLSR(d|q, y|)) (16)\\n2) Verification : Although RAG enhances the reliability\\nof LLM-generated answers, in many scenarios, it requires to'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='The overall loss is calculated as:\\nL = 1\\n|T|\\nTX\\ni=1\\nKL(PR(d|q)||PLSR(d|q, y|)) (16)\\n2) Verification : Although RAG enhances the reliability\\nof LLM-generated answers, in many scenarios, it requires to\\nminimize the probability of hallucinations. Therefore, it can\\nfilter out responses that do not meet the required standards\\nthrough additional verification module. Common verification\\nmethods include knowledge-base and model-base .\\nyk = fverify (q, Dq, y) (17)\\nKnowledge-base verification refers to directly validating the\\nresponses generated by LLMs through external knowledge.\\nGenerally, it extracts specific statements or triplets from re-\\nsponse first. Then, relevant evidence is retrieved from verified\\nknowledge base such as Wikipedia or specific knowledge\\ngraphs. Finally, each statement is incrementally compared with\\nthe evidence to determine whether the statement is supported,\\nrefuted, or if there is insufficient information [52].\\nModel-based verification refers to using a small language'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the evidence to determine whether the statement is supported,\\nrefuted, or if there is insufficient information [52].\\nModel-based verification refers to using a small language\\nmodel to verify the responses generated by LLMs [53].\\nGiven the input question, the retrieved knowledge, and the\\ngenerated answer, a small language model is trained to de-\\ntermine whether the generated answer correctly reflects the\\nretrieved knowledge. This process is framed as a multiple-\\nchoice question, where the verifier needs to judge whether the\\nanswer reflects correct answer . If the generated answer does\\nnot correctly reflect the retrieved knowledge, the answer can\\nbe iteratively regenerated until the verifier confirms that the\\nanswer is correct.\\nF . Orchestration\\nOrchestration pertains to the control modules that govern the\\nRAG process. Unlike the traditional, rigid approach of a fixed\\nprocess, RAG now incorporates decision-making at pivotal\\njunctures and dynamically selects subsequent steps contingent'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='RAG process. Unlike the traditional, rigid approach of a fixed\\nprocess, RAG now incorporates decision-making at pivotal\\njunctures and dynamically selects subsequent steps contingent\\nupon the previous outcomes. This adaptive and modular ca-\\npability is a hallmark of modular RAG, distinguishing it from\\nthe more simplistic Naive and Advance RAG paradigm.\\n1) Routing: In response to diverse queries, the RAG system\\nroutes to specific pipelines tailored for different scenario, a\\nfeature essential for a versatile RAG architecture designed\\nto handle a wide array of situations. A decision-making\\nmechanism is necessary to ascertain which modules will be\\nengaged, based on the input from the model or supplementary\\nmetadata. Different routes are employed for distinct prompts\\nor components. This routing mechanism is executed through\\na function, denoted as fr(·), which assigns a score αi to\\neach module. These scores dictate the selection of the active'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='or components. This routing mechanism is executed through\\na function, denoted as fr(·), which assigns a score αi to\\neach module. These scores dictate the selection of the active\\nsubset of modules. Mathematically, the routing function is\\nrepresented as:\\nfr : Q → F (18)\\nwhere fr(·) maps the identified query to its corresponding\\nRAG flow.\\nMetadata routing involves extracting key terms, or entities,\\nfrom the query, applying a filtration process that uses these\\nkeywords and associated metadata within the chunks to refine\\nthe routing parameters. For a specific RAG flow, denoted as\\nFi, the pre-defined routing keywords are represented as the\\nset Ki = {ki1, ki2, . . . , kin}. The keyword identified within\\nthe query qi is designated as K′\\ni. The matching process for\\nthe query q is quantified by the key score equation:\\nscorekey(qi, Fj) = 1\\n|K′\\nj||Ki ∩ K′\\nj| (19)\\nThis equation calculates the overlap between the pre-defined\\nkeywords and those identified in the query, normalized by the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='scorekey(qi, Fj) = 1\\n|K′\\nj||Ki ∩ K′\\nj| (19)\\nThis equation calculates the overlap between the pre-defined\\nkeywords and those identified in the query, normalized by the\\ncount of keywords in K′\\nj. The final step is to determine the\\nmost relevant flow for the query q:\\nFi(q) = argmaxFj∈Fscore(q, Fj) (20)\\nSemantic routing routes to different modules based on the\\nsemantic information of the query. Given a pre-defined intent\\nΘ = {θ1, θ2, . . . , θn}, the possibility of intent for query q is\\nPΘ(θ|q) = ePLM(θ|q)\\nP\\nθ∈Θ ePLM(θ|q)) . Routing to specific RAG flow is\\ndetermined by the semantic score:\\nsocresemantic(q, Fj) = argmaxθj∈ΘP(Θ) (21)\\nThe function δ(·) serves as a mapping function that assigns\\nan intent to a distinct RAG flow Fi = δ(θi)\\nHybrid Routing can be implemented to improve query\\nrouting by integrating both semantic analysis and metadata-\\nbased approaches, which can be defined as follows:\\nαi = a·scorekey(q, Fj)+(1−α)·maxθj∈Θsocresemantic(q, Fj)\\n(22)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='routing by integrating both semantic analysis and metadata-\\nbased approaches, which can be defined as follows:\\nαi = a·scorekey(q, Fj)+(1−α)·maxθj∈Θsocresemantic(q, Fj)\\n(22)\\na is a weighting factor that balances the contribution of the\\nkey-based score and the semantic score.\\n2) Scheduling: The RAG system evolves in complexity\\nand adaptability, with the ability to manage processes through\\na sophisticated scheduling module. The scheduling module\\nplays a crucial role in the modular RAG , identifying critical\\njunctures that require external data retrieval, assessing the\\nadequacy of the responses, and deciding on the necessity for\\nfurther investigation. It is commonly utilized in scenarios that\\ninvolve recursive, iterative, and adaptive retrieval, ensuring'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='9\\nthat the system makes informed decisions on when to cease\\ngeneration or initiate a new retrieval loop.\\nRule judge. The subsequent steps are dictated by a set of\\nestablished rules. Typically, the system evaluates the quality of\\ngenerated answers through scoring mechanisms. The decision\\nto proceed or halt the process is contingent upon whether these\\nscores surpass certain predetermined thresholds, often related\\nto the confidence levels of individual tokens, which can be\\ndefined as follow:\\nyt =\\n(\\nˆst if all tokens of ˆst have probs ≥ τ\\nst = LM([Dqt, x, y<t]) otherwise\\nHere, ˆst represents the tentative answer, and st is the output\\nfrom the language model. The condition for accepting ˆst is that\\nall tokens within it must have associated probabilities greater\\nthan or equal to the threshold τ. If this condition is not met,\\nthe system reverts to generating a new answer.\\nLLM judge. The LLM independently determines the sub-\\nsequent course of action. Two primary approaches facilitate'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the system reverts to generating a new answer.\\nLLM judge. The LLM independently determines the sub-\\nsequent course of action. Two primary approaches facilitate\\nthis capability. The first method leverages LLM ’s in-context\\nlearning capability, and make judgments through prompt\\nengineering. A significant advantage of this method is the\\nelimination of model fine-tuning. Nonetheless, the format of\\nthe judgment output is contingent upon the LLM’s adherence\\nto the provided instructions.\\nThe second approach involves the LLM generating specific\\ntokens that initiate targeted actions through fine-tuning. This\\ntechnique, with roots in the Toolformer [50], has been inte-\\ngrated into frameworks like Self-RAG [28]. This allows for a\\nmore direct control mechanism over the LLM’s actions, en-\\nhancing the system’s responsiveness to specific triggers within\\nthe conversational context. However, it requires generating a\\nlarge number of compliant instruction sets to fine-tune LLM.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='hancing the system’s responsiveness to specific triggers within\\nthe conversational context. However, it requires generating a\\nlarge number of compliant instruction sets to fine-tune LLM.\\nKnowledge-guide scheduling. Beyond the confines of rule-\\nbased methods and the complete reliance on LLMs for process\\ncontrol, a more adaptable intermediate approach emerges with\\nknowledge-guided scheduling [26]. These methods harness\\nthe power of knowledge graphs, to steer the retrieval and\\ngeneration processes. Specifically, it involves extracting infor-\\nmation relevant to the question from a knowledge graph and\\nconstructing a reasoning chain. This reasoning chain consists\\nof a series of logically interconnected nodes, each containing\\ncritical information for the problem-solving process. Based\\non the information from the nodes in this reasoning chain,\\ninformation retrieval and content generation can be performed\\nseparately. By integrating this approach, it enhance not only'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='on the information from the nodes in this reasoning chain,\\ninformation retrieval and content generation can be performed\\nseparately. By integrating this approach, it enhance not only\\nthe efficacy and precision of problem-solving but also the\\nclarity of the explanations provided.\\n3) Fusion: As RAG process has evolved beyond a linear\\npipeline, it frequently necessitates broadening the retrieval\\nscope or enhancing diversity by exploring multiple pipelines.\\nConsequently, after the expansion into various branches, the\\nfusion module effectively integrates the information, ensuring\\na comprehensive and coherent response. The fusion module’s\\nreliance is not just for merging answers but also for ensuring\\nthat the final output is both rich in content and reflective of\\nthe multifaceted nature of the inquiry.\\nLLM fusion .One of the most straightforward methods for\\nmulti-branch aggregation is to leverage the powerful capa-\\nbilities of LLMs to analyze and integrate information from'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='LLM fusion .One of the most straightforward methods for\\nmulti-branch aggregation is to leverage the powerful capa-\\nbilities of LLMs to analyze and integrate information from\\ndifferent branches. However, this approach also faces some\\nchallenges, particularly when dealing with long answers that\\nexceeds the LLM’s context window limitation. To mitigate this\\nissue, it is common practice to first summarize each branch’s\\nanswer, extracting the key information before inputting it into\\nthe LLM, thus ensuring that the most important content is\\nretained even within length constraints.\\nWeighted ensemble is based on the weighted values of\\ndifferent tokens generated from multiple branches, leading to\\nthe comprehensive selection of the final output. This approach\\ncan be calculated as :\\np(y|q, Dq) =\\nX\\nd∈Dq\\np(y|d, q) · λ(d, q) (23)\\nThe weight λ(d, q) is determined by the similarity score\\nbetween the document d and the input query q. This weight is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='can be calculated as :\\np(y|q, Dq) =\\nX\\nd∈Dq\\np(y|d, q) · λ(d, q) (23)\\nThe weight λ(d, q) is determined by the similarity score\\nbetween the document d and the input query q. This weight is\\ncalculated using the softmax function, which ensures that the\\nweights are normalized and sum up to one.\\nλ(d, q) = es(d,q)\\nP\\nd∈Dq es(d,q) (24)\\nRRF (Reciprocal Rank Fusion) is an ensemble technique\\nthat synthesizes multiple retrieval result rankings into a co-\\nhesive, unified list [54]. It employs a tailored weighted aver-\\naging approach to enhance collective predictive performance\\nand ranking precision. The method’s strength is its dynamic\\nweight assignment, which is informed by the interplay among\\nbranches. RRF is especially potent in scenarios characterized\\nby model or source heterogeneity, where it can markedly\\namplify the accuracy of predictions.\\nV. RAG F LOW AND FLOW PATTERN\\nThe collaboration between operators forms the workflow\\nof the module, which we refer to as RAG flow F ='),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='amplify the accuracy of predictions.\\nV. RAG F LOW AND FLOW PATTERN\\nThe collaboration between operators forms the workflow\\nof the module, which we refer to as RAG flow F =\\n(Mϕ1 , . . . , Mϕn), where ϕ stands for the set of module param-\\neters. A modular rag flow can be decomposed into a graph of\\nsub-functions. Through control logic, the operators can execute\\nin a predetermined pipeline, while also performing conditional,\\nbranching or looping when necessary. In the simplest case. the\\ngraph is a linear chain.\\nAfter conducting an in-depth analysis of current RAG meth-\\nods, we have identified a set of common RAG flow patterns,\\ndenoted as P. These patterns transcend various application\\ndomains and demonstrate a high level of consistency and\\nreusability, revealing the prevalent structures and behaviors in\\nprocess design. A RAG flow pattern can be defined as P =\\n{Mϕ1 : {Op1} →Mϕ2 : {Op2} →. . .→ Mϕn : {Opn}}\\nA. Linear Pattern\\nThe modules in the modular RAG system are organized in'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='process design. A RAG flow pattern can be defined as P =\\n{Mϕ1 : {Op1} →Mϕ2 : {Op2} →. . .→ Mϕn : {Opn}}\\nA. Linear Pattern\\nThe modules in the modular RAG system are organized in\\na linear way, and can be described as Algorithm 1.\\nPlinear = {M1 → M2 → . . .→ Mn} (25)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='10\\nFig. 4. Linear RAG flow pattern. Each module is processed in a fixed\\nsequential order.\\nFig. 5. RRR [24] is a typical linear flow that introduces a learnable query\\nrewrite module before retrieval. This module employs reinforcement based on\\nthe output results of the LLM.\\nThe linear flow pattern is the simplest and most com-\\nmonly used pattern. As shown in Figure 4, the full linear\\nRAG flow pattern mainly includes pre-retrieval processing,\\nretrieval, post-retrieval processing, and generation modules.\\nPlinearfull = {Mindexing → Mpre-retrieval → Mretrieval →\\nMpost-retrieval → Mgenerate}. If there are no pre-retrieval and\\npost-retrieval modules, it follows the Naive RAG paradigm.\\nAlgorithm 1 Linear RAG Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM, pre-processing function fpre, post-\\nprocessing function fpost\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='guage model LLM, pre-processing function fpre, post-\\nprocessing function fpost\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve documents related to the pre-\\nprocessed query\\n4: ˆDq′\\n← fpost(q′, Dq′\\n) // Post-process the retrieved docu-\\nments\\n5: ˆy ← LLM([q, ˆDq′\\n]) // Generate output using the lan-\\nguage model with the original query and post-processed\\ndocuments\\n6: return ˆy // Return the final output\\nCommon linear RAG flow involves a query transform\\nmodule (such as rewrite or HyDE operators) at the pre-retrieval\\nstage and utilize rerank at the post-retrieval stage. Rewrite-\\nRetrieve-Read (RRR) [24] is a typical linear structure. As\\nillustrated in Figure 5, the query rewrite module frewrite is a\\nsmaller trainable language model fine-tuned on T5-large, and\\nin the context of reinforcement learning, the optimization of\\nthe rewriter is formalized as a Markov decision process, with'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='smaller trainable language model fine-tuned on T5-large, and\\nin the context of reinforcement learning, the optimization of\\nthe rewriter is formalized as a Markov decision process, with\\nthe final output of the LLM serving as the reward. The retriever\\nutilizes a sparse encoding model, BM25.\\nB. Conditional Pattern\\nThe RAG flow with conditional structure involves select-\\ning different RAG pipeline based on different conditions,\\nas illustrated in Figure 6. A detailed definition is shown in\\nAlgorithm 2. Typically, pipleline selection is accomplished\\nFig. 6. The conditional flow pattern. There is a routing module that controls\\nwhich RAG flow the query is directed to. Typically, different flows are used for\\nvarious configurations to meet the general requirements of the RAG system.\\nFig. 7. Pre-retrieval branching flow pattern.Each branch performs retrieval\\nand generation separately, and then they are aggregated at the end.\\nthrough a routing module that determines the next module\\nin the flow.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='and generation separately, and then they are aggregated at the end.\\nthrough a routing module that determines the next module\\nin the flow.\\nPconditional = {Mi\\nfr\\n− →Mj ∨ Mk} (26)\\nWhere\\nfr\\n− →represents that based on routing function fr(·), the\\nflow can go to module Mj or Mk.\\nAlgorithm 2 Conditional RAG Flow Pattern\\nRequire: original query q, documents D, language model\\nLM, retriever R, routing function fr\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← QueryTransform(q) // Pre-process the initial query\\nif needed\\n3: D′ ← R(q′, D) // Retrieve or update documents related\\nto the query\\n4: Mnext ← fr(q′, D′) // Determine the next module using\\nthe routing function\\n5: if Mnext = Mj then\\n6: ˆy ← Mj(q′, D′) // Execute module Mj\\n7: else if Mnext = Mk then\\n8: ˆy ← Mk(q′, D′) Mk\\n9: end if\\n10: return ˆy\\nPipeline selection is determined by the nature of the ques-\\ntion, directing different flows tailored to specific scenarios. For\\nexample, the tolerance for responses generated by LLMs varies'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Pipeline selection is determined by the nature of the ques-\\ntion, directing different flows tailored to specific scenarios. For\\nexample, the tolerance for responses generated by LLMs varies\\nacross questions related to serious issues, political matters,\\nor entertainment topics. These routing flow often diverge in\\nterms of retrieval sources, retrieval processes, configurations,\\nmodels, and prompts.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='11\\nFig. 8. Post-retrieval branching flow pattern.Only one retrieval performed, and\\nthen generation is carried out separately for each retrieved document chunks,\\nfollowed by aggregation.\\nC. Branching\\nIn many cases, the RAG flow system may have multiple\\nparallel running branches , usually to increase the diver-\\nsity of generated results. Assuming multiple branches bi are\\ngenerated in module B = Msplit(·) = {b1, b2, . . . , bm}.\\nFor each branch bi ∈ B, the same or different RAG pro-\\ncesses can be executed, passing through multiple processing\\nmodules {M1, M2, . . . , Mk} to obtain branch output result\\npi = Mik(. . . Mi2(Mi1(bi)) . . .). The results of multiple\\nbranches are aggregated using an aggregation function to\\nobtain intermediate output results. ˆO = Mmerge({pi | bi ∈\\nB}). However, aggregation is not necessarily the end of the\\nRAG flow, as it can continue to connect to other modules,\\nMjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='B}). However, aggregation is not necessarily the end of the\\nRAG flow, as it can continue to connect to other modules,\\nMjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating\\nmultiple model responses, they can continue through a val-\\nidation module. Therefore, the entire branch flow pattern can\\nbe represented as:\\nPbranch =Mjn(. . . Mj1(Mmerge({Mik\\n(. . . Mi1(bi) . . .) | bi ∈ Msplit(q)})) . . .) (27)\\nAlgorithm 3 Pre-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, query expand mod-\\nule Mexpand, retriever Mretrieve, language model LLM,\\nmerge module Mmerge\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q′ ← Mexpand(q) // Expand the original query to multiple\\nsub-queries\\n3: for all q′\\ni ∈ Q′ do\\n4: D′\\ni ← Mretrieve(q′\\ni, D) // Retrieve documents for each\\nsub-query\\n5: Gi ← ∅// Initialize an empty set for generated results\\nof the sub-query\\n6: for all d′\\nij ∈ D′\\ni do\\n7: yij ← LLM([q′\\ni, d′\\nij]) // Generate results for each\\ndocument of the sub-query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='sub-query\\n5: Gi ← ∅// Initialize an empty set for generated results\\nof the sub-query\\n6: for all d′\\nij ∈ D′\\ni do\\n7: yij ← LLM([q′\\ni, d′\\nij]) // Generate results for each\\ndocument of the sub-query\\n8: Oi ← Oi ∪ {yij} // Add generated results to the set\\n9: end for\\n10: ˆy ← Mmerge(Oi) // Merge generated results of the sub-\\nquery into the final result\\n11: end for\\n12: return ˆy\\nThe RAG flow with a branching structure differs from\\nthe conditional approach in that it involves multiple parallel\\nbranches, as opposed to selecting one branch from multiple\\noptions in the conditional approach. Structurally, it can be\\ncategorized into two types, which are depicted in Figure 7\\nand Figure 8.\\nPre-Retrieval Branching (Multi-Query, Parallel Retrieval).\\nAs shown in Algorithm 3, the process involves initially taking\\na query q and expanding it through a module Mexpand to gen-\\nerate multiple sub-queries Q′. Each sub-query q′\\ni is then used\\nto retrieve relevant documents via Mretrieve, forming document\\nsets D′'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='a query q and expanding it through a module Mexpand to gen-\\nerate multiple sub-queries Q′. Each sub-query q′\\ni is then used\\nto retrieve relevant documents via Mretrieve, forming document\\nsets D′\\ni. These document sets, along with the corresponding\\nsub-queries, are fed into a generation module Mgenerate to\\nproduce a set of answers Gi. Ultimately, all these generated\\nanswers are combined using a merging module Mmerge to\\nform the final result y. This entire flow can be mathematically\\nrepresented as:\\nPbranchpre =Mmerge(q′\\ni∈Mexpand(q){Mgenerate(q′\\ni, d′\\nij) |\\nd′\\nij ∈ Mretrieve(q′\\ni)}) (28)\\nPost-Retrieval Branching (Single Query, Parallel Genera-\\ntion). As shown in Algorithm 4, in the post-retrieval branching\\npattern, the process starts with a single query q which is\\nused to retrieve multiple document chunks through a retrieval\\nmodule Mretrieve, resulting in a set of documents Dq. Each\\ndocument dq\\ni from this set is then independently processed by'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='used to retrieve multiple document chunks through a retrieval\\nmodule Mretrieve, resulting in a set of documents Dq. Each\\ndocument dq\\ni from this set is then independently processed by\\na generation module Mgenerate to produce a set of generated\\nresults G. These results are subsequently merged using a\\nmerge module Mmerge to form the final result y. The process\\ncan be succinctly represented as y = Mmerge(Oi), where Oi is\\nthe collection of all generated results from each document dq\\ni\\nin Dq. Therefore, the entire process can be represented as:\\nPbranchpost = Mmerge({Mgenerate(dq\\ni ) | dq\\ni ∈ Mretrieve(q)})\\n(29)\\nAlgorithm 4 Post-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM, merge module Mmerge\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve a set of documents based on\\nthe pre-processed query\\n4: G ← ∅// Initialize an empty set to store generated results'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve a set of documents based on\\nthe pre-processed query\\n4: G ← ∅// Initialize an empty set to store generated results\\n5: for all di ∈ Dq′\\ndo\\n6: yi ← LLM([q, di]) // Generate results independently\\nfor each document chunk using the language model\\n7: Oi ← Oi ∪ {yi} // Add the generated result to the set\\nof results\\n8: end for\\n9: ˆy ← Mmerge(Oi) // Merge all generated results using the\\nmerge function\\n10: return ˆy\\nREPLUG [55] embodies a classic post-retrieval branching\\nstructure, wherein the probability of each token is predicted\\nfor each branch. Through weighted possibility ensemble, the\\ndifferent branches are aggregated, and the final generation'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='12\\nFig. 9. The RAG flow in REPLUG [55], which follows a typical post-retrieval\\nbranching pattern. Each retrieved chunks undergoes parallel generation, and\\nthen they are aggregated using a weighted probability ensemble.\\nresult is used to fine-tune the retriever, known as Contriever,\\nthrough feedback.\\nD. Loop Pattern\\nThe RAG flow with a loop structure, as an important char-\\nacteristic of Modular RAG, involves interdependent retrieval\\nand generation steps. It typically includes a scheduling module\\nfor flow control. The modular RAG system can be abstracted\\nas a directed graph G = (V, E), where V is the set of vertices\\nrepresenting the various modules Mi in the system, and E is\\nthe set of edges representing the control flow or data flow be-\\ntween modules. If there is a vertex sequence Mi1 , Mi2 , ..., Min\\nsuch that Min can reach Mi1 (i.e., Min → Mi1 ), then this\\nRAG system forms a loop. If Mj is the successor module of\\nMi and Mi decides whether to return to Mj or a previous'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='such that Min can reach Mi1 (i.e., Min → Mi1 ), then this\\nRAG system forms a loop. If Mj is the successor module of\\nMi and Mi decides whether to return to Mj or a previous\\nmodule Mk through a Judge module, it can be represented\\nas: Mi\\nJudge\\n− − − →Mj or Mi\\nJudge\\n− − − →Mk where Mk is the\\npredecessor module of Mj. If Mi return to Mj, it can be\\nrepresented as: ∃Judge(Mi, Mj) s.t. (Mi, Mj) ∈ E and\\nJudge(Mi, Mj) = true. If the Judge module not to return\\nto any previous module, it can be represented as: ∀Mi ∈\\nV, Judge(Mi, Mj) = false for all Mj that are predecessors\\nof Mi. Loop pattern can be further categorized into iterative,\\nrecursive, and adaptive (active) retrieval approaches.\\nIterative retrieval At times, a single retrieval and genera-\\ntion may not effectively address complex questions requiring\\nextensive knowledge. Therefore, an iterative approach can be\\nused in RAG (see Algorithm 5), typically involving a fixed\\nnumber of iterations for retrieval. At step t, given the query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='extensive knowledge. Therefore, an iterative approach can be\\nused in RAG (see Algorithm 5), typically involving a fixed\\nnumber of iterations for retrieval. At step t, given the query\\nqt and the previous output sequence y<t = [ y0, . . . , yt−1] ,\\niterations proceed under the condition that t is less than the\\nmaximum allowed iterations T. In each loop, it retrieves a\\ndocument chunks Dt−1 using the last output yt−1 and the\\ncurrent query qt. Subsequently, a new output yt is generated.\\nThe continuation of the iteration is determined by a Judge\\nmodule, which makes its decision based on the yt, y<t, qt,\\nand the Dt−1.\\nAn exemplary case of iterative retrieval is ITER-\\nRETGEN [56] (Figure 11), which iterates retrieval-augmented\\ngeneration and generation-augmented retrieval. Retrieval-\\naugmented generation outputs a response to a task input based\\non all retrieved knowledge. In each iteration, ITER-RETGEN\\nleverages the model output from the previous iteration as a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='augmented generation outputs a response to a task input based\\non all retrieved knowledge. In each iteration, ITER-RETGEN\\nleverages the model output from the previous iteration as a\\nspecific context to help retrieve more relevant knowledge.\\nFig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\\nof retrieval and generation. It can be categorized into three forms: iterative,\\nrecursive, and adaptive.\\nAlgorithm 5 Iterative RAG Flow Pattern\\nRequire: original query q, documents D, maximum iterative\\ntimes T, language model LLM, retriever R, initial output\\ny<1 = ∅\\nEnsure: final output ˆy\\n1: Initialize:\\n2: qt ← q // Initialize query for the first iteration\\n3: y<1 ← ∅// Initialize previous outputs as empty\\n4: t ← 1 // Initialize iteration step\\n5: while t ≤ T do\\n6: qt ← QueryTransform(y<t−1, qt−1) // Generate query\\nbased on previous output and original query\\n7: Dt ← R(yt−1||qt, D) // Retrieve or update documents\\nrelated to the current query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='5: while t ≤ T do\\n6: qt ← QueryTransform(y<t−1, qt−1) // Generate query\\nbased on previous output and original query\\n7: Dt ← R(yt−1||qt, D) // Retrieve or update documents\\nrelated to the current query\\n8: yt ← LLM([y<t−1, qt, Dt]) // Generate output using\\nthe language model\\n9: y<t ← [y<t−1, yt] // Update the list of previous outputs\\n10: if Judge(yt, q) = false then\\n11: break\\n12: end if\\n13: t ← t + 1 // Increment iteration step\\n14: end while\\n15: yfinal = synthesizeOutput(y≤t) // Synthesize final output\\nfrom the list of outputs\\n16: return ˆy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='13\\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\\nof retrieval and generation are performed within the limit of the maximum\\nnumber of iterations.\\nTermination of the loop is determined by a predefined number\\nof iterations.\\nRecursive retrieval The characteristic feature of recursive\\nretrieval (see Algorithm 6), as opposed to iterative retrieval, is\\nits clear dependency on the previous step and its continuous\\ndeepening of retrieval. Typically, it follows a tree-like structure\\nand there is a clear termination mechanism as an exit condition\\nfor recursive retrieval. In RAG systems, recursive retrieval usu-\\nally involves query transform, relying on the newly rewritten\\nquery for each retrieval.\\nAlgorithm 6 Recursive RAG Flow Pattern\\nRequire: initial query q, document D, retriever R, language\\nmodel LM, maximum recursive depth Kmax\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q ← {q}\\n3: k ← 0 // Initialize recursion depth\\n4: while Q ̸= ∅ and k < Kmax do'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='model LM, maximum recursive depth Kmax\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q ← {q}\\n3: k ← 0 // Initialize recursion depth\\n4: while Q ̸= ∅ and k < Kmax do\\n5: Q′ ← ∅// To store queries for the next recursion level\\n6: for all q ∈ Q do\\n7: Dq ← R(q, D) // Retrieve or update documents\\nrelated to the current query\\n8: Y ← LM([q, Dq]) // Generate outputs using the\\nlanguage model\\n9: Q′′ ← deriveNewQueries(q, Dq, Y) // Derive new\\nqueries from generated outputs\\n10: for all q′ ∈ Q′′ do\\n11: if q′ /∈ Q′ and q′ /∈ Q then\\n12: Q′ ← Q′ ∪ {q′}\\n13: end if\\n14: end for\\n15: end for\\n16: Q ← Q′ // Update the set of queries for the next\\nrecursion\\n17: k ← k + 1 // Increment recursion depth\\n18: end while\\n19: ˆy = synthesizeOutput(Y ) // Synthesize final output from\\ngenerated outputs\\n20: return ˆy\\nA typical implementation of recursive retrieval, such as\\nToC [13] (see Figure 12 ), involves recursively executing RAC\\n(Recursive Augmented Clarification) to gradually insert sub-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='20: return ˆy\\nA typical implementation of recursive retrieval, such as\\nToC [13] (see Figure 12 ), involves recursively executing RAC\\n(Recursive Augmented Clarification) to gradually insert sub-\\nnodes into the clarification tree from the initial ambiguous\\nquestion (AQ). At each expansion step, paragraph re-ranking\\nis performed based on the current query to generate a disam-\\nFig. 12. RAG flow of ToC [13]. A typical characteristic of this process is\\nthat each recursive retrieval uses the new query generated from the previous\\nstep, thereby progressively deepening analysis of the original complex query.\\nbiguous Question (DQ). The exploration of the tree concludes\\nupon reaching the maximum number of valid nodes or the\\nmaximum depth. Once the clarification tree is constructed,\\nToC gathers all valid nodes and generates a comprehensive\\nlong-text answer to address AQ.\\nAdaptive (Active) retrieval With the advancement of RAG,\\nthere has been a gradual shift beyond passive retrieval to the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='long-text answer to address AQ.\\nAdaptive (Active) retrieval With the advancement of RAG,\\nthere has been a gradual shift beyond passive retrieval to the\\nemergence of adaptive retrieval (see Algorithm 7) , also known\\nas active retrieval, which is partly attributed to the powerful\\ncapabilities of LLM. This shares a core concept with LLM\\nAgent [57]. RAG systems can actively determine the timing\\nof retrieval and decide when to conclude the entire process and\\nproduce the final result. Based on the criteria for judgment,\\nthis can be further categorized into Prompt-base and Tuning-\\nbase approaches.\\nAlgorithm 7 Active RAG Flow Pattern\\nRequire: original query Q, documents D, maximum iterative\\ntimes T, language model LLM, retriever R\\nEnsure: final output ˆy\\n1: Initialize:\\n2: t ← 1 // Initialize loop step\\n3: qt ← q // Initialize query for the first iteration\\n4: y<1 ← ∅// Initialize previous outputs as empty\\n5: while t ≤ T do\\n6: Qt ← QueryTransform(y<t−1, qt−1) // Derive new'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='3: qt ← q // Initialize query for the first iteration\\n4: y<1 ← ∅// Initialize previous outputs as empty\\n5: while t ≤ T do\\n6: Qt ← QueryTransform(y<t−1, qt−1) // Derive new\\nquery from previous output and query\\n7: if Evaluate(Qt, y<t−1) then\\n8: Dt ← R(qt, D) // Retrieve documents based on the\\nnew query\\n9: yt ← LLM([qt, Dt]) // Generate output using the\\nlanguage model\\n10: else\\n11: yt ← ∅// Set output as empty if query evaluation is\\nfalse\\n12: end if\\n13: y<t ← [y<t−1, yt] // Update the list of previous outputs\\n14: if isOutputAcceptable(yt, y<t, qt) = false then\\n15: break // Break if the output is not acceptable\\n16: end if\\n17: t ← t + 1 // Increment iteration step\\n18: end while\\n19: ˆy = synthesizeOutput(y≤t) // Synthesize final output from\\nthe list of outputs\\n20: return ˆy\\nPrompt-base. The prompt-base approach involves control-\\nling the flow using Prompt Engineering to direct LLM. A'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='14\\nFig. 13. RAG flow of FLARE [14]. The generated provisional answer will\\nundergo confidence assessment. If it does not meet the required confidence\\nlevel, the process will return to the retrieval stage and generate anew. The\\nassessment criteria are implemented through prompt\\nFig. 14. RAG flow of SELF-RAG [28]. First, it prompt GPT-4 to obtain\\na suitable instruct fine-tuning dataset to fine-tune the deployed open-source\\nLLM. This allows the model to output four specific tokens during generation,\\nwhich are used to control the RAG process.\\ntypical implementation example is FLARE [14]. Its core\\nconcept is that LLMs should only retrieve when essential\\nknowledge is lacking, to avoid unnecessary or inappropriate\\nretrieval in an enhanced LM. FLARE iteratively generates the\\nnext provisional sentence and checks for the presence of low-\\nprobability tokens. If found, the system retrieves relevant docu-\\nments and regenerates the sentence. Tuning-base. The tuning-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='next provisional sentence and checks for the presence of low-\\nprobability tokens. If found, the system retrieves relevant docu-\\nments and regenerates the sentence. Tuning-base. The tuning-\\nbased approach involves fine-tuning LLM to generate special\\ntokens, thereby triggering retrieval or generation. This concept\\ncan be traced back to Toolformer [50], where the generation of\\nspecific content assists in invoking tools. In RAG systems, this\\napproach is used to control both retrieval and generation steps.\\nA typical case is Self-RAG [28](see Figure 14). Given an\\ninput prompt and the preceding generation result, first predict\\nwhether the special token Retrieve is helpful for enhancing\\nthe continued generation through retrieval. Then, if retrieval\\nis needed, the model generates a critique token to evaluate the\\nretrieved passage’s relevance. and a critique token to evaluate\\nif the information in the response is supported by the retrieved'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='is needed, the model generates a critique token to evaluate the\\nretrieved passage’s relevance. and a critique token to evaluate\\nif the information in the response is supported by the retrieved\\npassage. Finally, a critique token evaluates the overall utility of\\nthe response and selects the optimal result as the final output.\\nE. Tuning Pattern\\nRAG is continuously integrating with more LLM-related\\ntechnologies. In Modular RAG, many components are com-\\nposed of trainable language models. Through fine-tuning, the\\nperformance of the components and the compatibility with\\nthe overall flow can be further optimized. This section will\\nintroduce three main patterns of fine-tuning stages, namely\\nretriever fine-tuning, generator fine-tuning, and dual fine-\\ntuning.\\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.\\n1) Retriever FT: In the RAG flow, common methods for'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='tuning.\\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.\\n1) Retriever FT: In the RAG flow, common methods for\\nfine-tuning the retriever is shown in Figure 15 ,which include:\\n• Direct supervised fine-tuning of the retriever. Construct-\\ning a specialized dataset for retrieval and fine-tuning the\\ndense retriever. For example, using open-source retrieval\\ndatasets or constructing one based on domain-specific\\ndata.\\n• Adding trainable adapter modules. Sometimes, direct\\nfine-tuning of the API-base embedding model (e.g., Ope-\\nnAI Ada-002 and Cohere) is not feasible. Incorporating\\nan adapter module can enhance the representation of\\nyour data. Additionally, the adapter module facilitates\\nbetter alignment with downstream tasks, whether for task-\\nspecific (e.g., PRCA [42]) or general purposes (e.g.,\\nAAR [58]).\\n• LM-supervised Retrieval (LSR). Fine-tuning the retriever\\nbased on the results generated by LLM.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='specific (e.g., PRCA [42]) or general purposes (e.g.,\\nAAR [58]).\\n• LM-supervised Retrieval (LSR). Fine-tuning the retriever\\nbased on the results generated by LLM.\\n• LLM Reward RL. Still using the LLM output results as\\nthe supervisory signal. Employing reinforcement learning\\nto align the retriever with the generator. The whole re-\\ntrieval process is disassembled in the form of a generative\\nMarkov chain.\\n2) Generator FT: The primary methods for fine-tuning a\\ngenerator in RAG flow is shown in Figure 16, which include:\\n• Direct supervised fine-tuning . Fine-tuning through an\\nexternal dataset can supplement the generator with ad-\\nditional knowledge. Another benefit is the ability to\\ncustomize input and output formats. By setting the Q&A\\nformat, LLM can understand specific data formats and\\noutput according to instructions.\\n• Distillation. When using on-premise deployment of open-\\nsource models, a simple and effective Optimization\\nmethod is to use GPT-4 to batch construct fine-tuning'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='output according to instructions.\\n• Distillation. When using on-premise deployment of open-\\nsource models, a simple and effective Optimization\\nmethod is to use GPT-4 to batch construct fine-tuning\\ndata to enhance the capabilities of the open-source model.\\n• RL from LLM/human feedback. Reinforcement learning\\nbased on feedback from the final generated answers. In\\naddition to using human evaluations, powerful LLMs can\\nalso serve as an evaluative judge.\\n3) Dual FT: In the RAG system, fine-tuning both the\\nretriever and the generator simultaneously is a unique feature\\nof the RAG system. It is important to note that the emphasis\\nof system fine-tuning is on the coordination between the\\nretriever and the generator. An exemplary implementation is\\nRA-DIT [27], which fine-tunes both the LLM and the retriever.\\nThe LM-ft component updates the LLM to maximize the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='15\\nFig. 16. Generator fine-tuning pattern, The main methods include SFT,\\ndistillation and RL from LLM/human feedback.\\nFig. 17. Dual fine-tuning pattern. In this mode, both the retriever and\\ngenerator participate in fine-tuning, and their preferences will be aligned.\\nlikelihood of the correct answer given the retrieval-augmented\\ninstructions while the R-ft component updates the retriever\\nto minimize the KL-Divergence between the retriever score\\ndistribution and the LLM preference.\\nVI. D ISCUSSION\\nIn this chapter, we explore the innovative horizons opened\\nby the modular RAG paradigm. We examine its compatibility\\nwith cutting-edge methodologies in the progression of RAG\\ntechnology, emphasizing its scalability. It not only fosters a\\nfertile ground for model innovation but also paves the way for\\nseamless adaptation to the dynamic requirements of various\\napplications.\\nA. Opportunities in Modular RAG\\nThe benefits of Modular RAG are evident, providing a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='seamless adaptation to the dynamic requirements of various\\napplications.\\nA. Opportunities in Modular RAG\\nThe benefits of Modular RAG are evident, providing a\\nfresh and comprehensive perspective on existing RAG-related\\nwork. Through modular organization, relevant technologies\\nand methods are clearly summarized.\\nFrom a research perspective. Modular RAG is highly\\nscalable, it empowers researchers to introduce innovative mod-\\nules and operators, leveraging a deep understanding of RAG’s\\nevolving landscape. This flexibility enables the exploration of\\nnew theoretical and practical dimensions in the field.\\nFrom an application perspective . The modularity of RAG\\nsystems simplifies their design and implementation. Users can\\ntailor RAG flows to fit their specific data, use cases, and\\ndownstream tasks, enhancing the adaptability of the system\\nto diverse requirements. Developers can draw from existing\\nflow architectures and innovate by defining new flows and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='downstream tasks, enhancing the adaptability of the system\\nto diverse requirements. Developers can draw from existing\\nflow architectures and innovate by defining new flows and\\npatterns that are tailored to various application contexts and\\ndomains. This approach not only streamlines the development\\nprocess but also enriches the functionality and versatility of\\nRAG applications.\\nB. Compatibility with new methods\\nModular RAG paradigm demonstrates exceptional compati-\\nbility with new developments. To gain a deeper understanding\\nof this, we list three typical scalability cases, which clearly\\nshows that Modular RAG paradigm provides robust support\\nand flexibility for the innovation and development of RAG\\ntechnology.\\n1) Recombination of the current modules: In this scenario,\\nno new modules or operators are proposed; rather, specific\\nproblems are addressed through the combination of existing\\nmodules.DR-RAG [59] employs a two-stage retrieval strategy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='no new modules or operators are proposed; rather, specific\\nproblems are addressed through the combination of existing\\nmodules.DR-RAG [59] employs a two-stage retrieval strategy\\nand classifier selection mechanism, incorporating a branching\\nretrieval structure. In the first stage, retrieving chunks relevant\\nto the query. In the second stage, the query is combined\\nindividually with each chunk retrieved in the first stage, and a\\nparallel secondary retrieval is conducted. The retrieved content\\nis then input into a classifier to filter out the most relevant\\ndynamic documents. This ensures that the retrieved documents\\nare highly relevant to the query while reducing redundant\\ninformation. DR-RAG improved retrieval method significantly\\nenhances the accuracy and efficiency of answers, bolstering\\nRAG’s performance in multi-hop question-answering scenar-\\nios.\\n2) New flow without adding new operators.: This refers\\nto redesigning the processes for retrieval and generation to'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='RAG’s performance in multi-hop question-answering scenar-\\nios.\\n2) New flow without adding new operators.: This refers\\nto redesigning the processes for retrieval and generation to\\naddress more complex scenarios without proposing new mod-\\nules. The core idea of PlanRAG [18] lies in its introduction of\\na preliminary planning stage, a crucial step that occurs before\\nretrieval and generation. Initially, the system employs a judge\\nmodule to assess whether the current context necessitates the\\nformulation of a new plan or adjustments to an existing one.\\nWhen encountering a problem for the first time, the system\\ninitiates the planning process, while in subsequent interactions,\\nit decides whether to execute re-planning based on previous\\nplans and retrieved data.\\nNext, the system devises an execution plan tailored to the\\nquery, treating this process as a logical decomposition of\\ncomplex queries. Specifically, PlanRAG uses a query expan-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Next, the system devises an execution plan tailored to the\\nquery, treating this process as a logical decomposition of\\ncomplex queries. Specifically, PlanRAG uses a query expan-\\nsion module to extend and refine the query. For each derived\\nsub-query, the system conducts targeted retrieval. Following\\nretrieval, another judge module evaluates the current results to\\ndecide whether further retrieval is required or if it should return\\nto the planning stage for re-planning. Through this strategy,\\nPlanRAG is able to handle complex decision-making problems\\nthat require multi-step data analysis more efficiently.\\n3) New flow derived from new operators.: New operators\\noften introduce novel flow design, exemplified by Multi-Head\\nRAG [60]. Existing RAG solutions do not focus on queries that\\nmay require retrieving multiple documents with significantly\\ndifferent content. Such queries are common but difficult to\\nhandle because embeddings of these documents may be far'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='may require retrieving multiple documents with significantly\\ndifferent content. Such queries are common but difficult to\\nhandle because embeddings of these documents may be far\\napart in the embedding space. Multi-Head RAG addresses this\\nby designing a new retriever that uses the activations of the\\nmulti-head attention layers of the Transformer, rather than the\\ndecoder layers, as keys for retrieving multifaceted documents.\\nDifferent attention heads can learn to capture different aspects\\nof the data. By using the corresponding activation results,\\nembeddings that represent different aspects of the data items\\nand the query can be generated, thereby enhancing the retrieval\\naccuracy for complex queries.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='16\\nVII. C ONCLUSION\\nRAG is emerging as a pivotal technology for LLM applica-\\ntions. As technological landscapes evolve and the intricacies of\\napplication requirements escalate, RAG systems are being en-\\nhanced by integrating a diverse suite of technologies, thereby\\nachieving a higher level of complexity and functionality. This\\npaper introduces the innovative paradigm of Modular RAG.\\nThis approach systematically disassembles the complex archi-\\ntecture of RAG systems into well-defined, discrete functional\\nmodules. Each module is meticulously characterized by its\\nspecific operational functions, ensuring clarity and precision.\\nTherefore, the entire system is composed of those modules\\nand operators, akin to Lego bricks. By conducting an in-\\ndepth analysis of numerous studies, the paper also distills\\ncommon RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='common RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for\\nthe design and application of RAG systems but also en-\\nables a scenario-based customization of these systems. The\\nmodularity inherent in this design facilitates ease of tracking\\nand debugging, significantly enhancing the maintainability and\\nscalability of RAG systems. Furthermore, Modular RAG opens\\nup new avenues for the future progression of RAG technology.\\nIt encourages the innovation of novel functional modules and\\nthe crafting of innovative workflows, thereby driving forward\\nthe frontiers of RAG systems.\\nREFERENCES\\n[1] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[2] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='lucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[2] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and\\nH. Wang, “Retrieval-augmented generation for large language models:\\nA survey,” arXiv preprint arXiv:2312.10997 , 2023.\\n[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\\nand Z. Li, “Retrieval-augmented generation with knowledge graphs\\nfor customer service question answering,” in Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2024, pp. 2905–2909.\\n[4] C. Zhang, S. Wu, H. Zhang, T. Xu, Y . Gao, Y . Hu, and E. Chen,\\n“Notellm: A retrievable large language model for note recommendation,”\\nin Companion Proceedings of the ACM on Web Conference 2024 , 2024,\\npp. 170–179.\\n[5] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='pp. 170–179.\\n[5] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[6] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[7] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[8] D. S. Asudani, N. K. Nagwani, and P. Singh, “Impact of word embedding\\nmodels on text analytics in deep learning environment: a review,”\\nArtificial intelligence review, vol. 56, no. 9, pp. 10 345–10 425, 2023.\\n[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Y . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[10] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\\n“Large language model based long-tail query rewriting in taobao search,”\\narXiv preprint arXiv:2311.03758 , 2023.\\n[11] Y . Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\\nY . Yu, “A bird’s-eye view of reranking: from list level to page level,”\\nin Proceedings of the Sixteenth ACM International Conference on Web\\nSearch and Data Mining , 2023, pp. 1075–1083.\\n[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.\\n[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,” arXiv\\npreprint arXiv:2305.06983, 2023.\\n[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\\nand J. Larson, “From local to global: A graph rag approach to query-\\nfocused summarization,” arXiv preprint arXiv:2404.16130 , 2024.\\n[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi, Z. Wang,\\nS. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\\ngeneration,” arXiv preprint arXiv:2407.01219 , 2024.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='S. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\\ngeneration,” arXiv preprint arXiv:2407.01219 , 2024.\\n[18] M. Lee, S. An, and M.-S. Kim, “Planrag: A plan-then-retrieval aug-\\nmented generation for generative large language models as decision\\nmakers,” arXiv preprint arXiv:2406.12430 , 2024.\\n[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158 , 2023.\\n[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems , vol. 33, pp. 9459–9474, 2020.\\n[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\\n“Improving language models by retrieving from trillions of tokens,” in\\nInternational conference on machine learning. PMLR, 2022, pp. 2206–\\n2240.\\n[22] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509 , 2022.\\n[24] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='ing for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” in Proceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track and\\nGovernment Track), J. Campbell, S. Larocca, J. Marciano, K. Savenkov,\\nand A. Yanishevsky, Eds. Orlando, USA: Association for Machine\\nTranslation in the Americas, Sep. 2022, pp. 202–209. [Online].\\nAvailable: https://aclanthology.org/2022.amta-upg.14\\n[26] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faith-\\nful and interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez,\\nJ. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\\ninstruction tuning,” arXiv preprint arXiv:2310.01352 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\\ninstruction tuning,” arXiv preprint arXiv:2310.01352 , 2023.\\n[28] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning\\nto retrieve, generate, and critique through self-reflection,” arXiv preprint\\narXiv:2310.11511, 2023.\\n[29] Y . Huang and J. Huang, “A survey on retrieval-augmented text gen-\\neration for large language models,” arXiv preprint arXiv:2404.10981 ,\\n2024.\\n[30] Y . Hu and Y . Lu, “Rag and rau: A survey on retrieval-augmented\\nlanguage model in natural language processing,” arXiv preprint\\narXiv:2404.19543, 2024.\\n[31] Y . Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and\\nQ. Li, “A survey on rag meets llms: Towards retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2405.06211 , 2024.\\n[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,\\nand B. Cui, “Retrieval-augmented generation for ai-generated content:'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,\\nand B. Cui, “Retrieval-augmented generation for ai-generated content:\\nA survey,” arXiv preprint arXiv:2402.19473 , 2024.\\n[33] S. Yang, “Advanced rag 01: Small-to-\\nbig retrieval,” https://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='17\\n[34] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730 , 2023.\\n[35] D. Zhou, N. Sch ¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al. , “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[37] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\\n[38] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='and D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.\\n[39] H. Cao, “Recent advances in text embedding: A comprehensive review\\nof top-performing methods on the mteb benchmark,” arXiv preprint\\narXiv:2406.01607, 2024.\\n[40] BAAI, “Flagembedding,” https://github.com/FlagOpen/FlagEmbedding,\\n2023.\\n[41] Z. Li, X. Zhang, Y . Zhang, D. Long, P. Xie, and M. Zhang, “Towards\\ngeneral text embeddings with multi-stage contrastive learning,” arXiv\\npreprint arXiv:2308.03281, 2023.\\n[42] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\\n“Prca: Fitting black-box large language models for retrieval question an-\\nswering via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172 , 2023.\\n[44] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[45] L. Xia, J. Xu, Y . Lan, J. Guo, and X. Cheng, “Learning maximal\\nmarginal relevance model via directly optimizing diversity evaluation\\nmeasures,” in Proceedings of the 38th international ACM SIGIR con-\\nference on research and development in information retrieval , 2015, pp.\\n113–122.\\n[46] Cohere, “Say goodbye to irrelevant search results: Cohere rerank is\\nhere,” https://txt.cohere.com/rerank/, 2023.\\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context sce-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='here,” https://txt.cohere.com/rerank/, 2023.\\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context sce-\\nnarios via prompt compression,” arXiv preprint arXiv:2310.06839, 2023.\\n[48] R. Litman, O. Anschel, S. Tsiper, R. Litman, S. Mazor, and R. Man-\\nmatha, “Scatter: selective context attentional scene text recognizer,” in\\nproceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, 2020, pp. 11 962–11 972.\\n[49] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092 , 2023.\\n[50] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='can teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in neural\\ninformation processing systems , vol. 35, pp. 27 730–27 744, 2022.\\n[52] S. J. Semnani, V . Z. Yao, H. C. Zhang, and M. S. Lam, “Wikichat:\\nStopping the hallucination of large language model chatbots by few-\\nshot grounding on wikipedia,” arXiv preprint arXiv:2305.14292 , 2023.\\n[53] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[54] G. V . Cormack, C. L. Clarke, and S. Buettcher, “Reciprocal rank\\nfusion outperforms condorcet and individual rank learning methods,”\\nin Proceedings of the 32nd international ACM SIGIR conference on\\nResearch and development in information retrieval , 2009, pp. 758–759.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='in Proceedings of the 32nd international ACM SIGIR conference on\\nResearch and development in information retrieval , 2009, pp. 758–759.\\n[55] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box language\\nmodels,” arXiv preprint arXiv:2301.12652 , 2023.\\n[56] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[57] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou et al., “Metagpt: Meta programming for\\nmulti-agent collaborative framework,” arXiv preprint arXiv:2308.00352,\\n2023.\\n[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='improves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\\n“Dr-rag: Applying dynamic document relevance to retrieval-augmented\\ngeneration for question-answering,” arXiv preprint arXiv:2406.07348 ,\\n2024.\\n[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\\ndorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M ¨uller et al. , “Multi-\\nhead rag: Solving multi-aspect problems with llms,” arXiv preprint\\narXiv:2406.05085, 2024.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541fe665",
   "metadata": {},
   "source": [
    "## Embedding and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d428a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22738a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str=\"all-MiniLM-L6-v2\"):\n",
    "        '''\n",
    "        Initialize the embedding manager \n",
    "        \n",
    "        Args:\n",
    "            model_name (str): HuggingFace model name for sentence embeddings\n",
    "        '''\n",
    "        self.model_name = model_name\n",
    "        self.model=None\n",
    "        self._load_model() \n",
    "    \n",
    "    def _load_model(self):\n",
    "        '''Load the SentenceTransformer model'''\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def generate_embeddings(self, texts:List[str]) -> np.ndarray:\n",
    "        '''\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of text strings to embed\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        '''\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "# Initialize embedding manager\n",
    "embedding_manager = EmbeddingManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a86dcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1f21dfa4890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820efa8",
   "metadata": {},
   "source": [
    "## VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6abd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized with collection: pdf_documents\n",
      "Existing documents in store: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1f21dc516d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    '''Manages document embeddings in chromaDB vector store'''\n",
    "    \n",
    "    def __init__(self, collection_name:str='pdf_documents', persist_directory:str='../data/vector_store'):\n",
    "        '''\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name (str): Name of the chromaDB collection\n",
    "            persist_directory (str): Directory to persist chromaDB data\n",
    "        '''\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        '''Initialize chromaDB client and collection'''\n",
    "        try:\n",
    "            # Create persistwnt ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized with collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in store: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02d219b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Finetune-RAG: Fine-Tuning Language Models to\\nResist Hallucination in Retrieval-Augmented\\nGeneration\\nZhan Peng Lee\\nPints AI Labs\\nzhanpeng.lee@pints.co\\nAndre Lin∗\\nPints AI Labs\\nandre_lin@u.nus.edu\\nandrelim444@gmail.com\\nCalvin Tan\\nPints AI Labs\\ncalvin@pints.co\\nAbstract\\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to\\nimprove factuality in large language models (LLMs) by grounding their outputs in\\nretrieved documents. However, ensuring perfect retrieval of relevant information\\nremains challenging, and when irrelevant content is passed downstream to an LLM,\\nit can lead to hallucinations. In this work, we propose Finetune-RAG, a simple\\nand effective fine-tuning approach that features the first-of-its-kind RAG training\\ndataset constructed to mimic real-world imperfections. Experimental results show\\nthat Finetune-RAG improves factual accuracy by 21.2% over the base model. We\\nalso propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='that Finetune-RAG improves factual accuracy by 21.2% over the base model. We\\nalso propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests\\nmodels under realistic imperfect retrieval scenarios. Our codebase2 and dataset3\\nare fully open sourced for community use.\\n1 Introduction\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of\\nnatural language processing tasks (Wang et al., 2023; Rozière et al., 2024; Cui et al., 2025; Yasunaga\\net al., 2022; Liu et al., 2024). However, their tendency to \"hallucinate\", that is, to produce fluent\\nbut factually incorrect information, remains a persistent challenge (Li et al., 2024a; Duan et al.,\\n2024; Zhang et al., 2023), particularly in high-stakes domains such as healthcare, law, and finance\\n(Agarwal et al., 2024; Dahl et al., 2024; Kang and Liu, 2023). To address this,Retrieval-Augmented\\nGeneration (RAG)has become a popular solution. Instead of relying solely on parametric memory,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='(Agarwal et al., 2024; Dahl et al., 2024; Kang and Liu, 2023). To address this,Retrieval-Augmented\\nGeneration (RAG)has become a popular solution. Instead of relying solely on parametric memory,\\nRAG systems retrieve external documents and condition the model’s response on this evidence.\\nIn practice, retrieval accuracy in RAG is far from flawless. Retrieved documents may be outdated,\\nmisleading, or topically adjacent but factually incorrect. These errors can propagate downstream,\\nleading models to blend inaccurate context into fluent but false answers. This is especially concerning\\nin domains such as law, compliance, financial reporting, or medicine, where mistakes can have\\nwide-ranging repercussions.\\nMost prior work has addressed this issue from the retrieval perspective, focusing on improving\\nretrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024; Dong et al.,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Most prior work has addressed this issue from the retrieval perspective, focusing on improving\\nretrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024; Dong et al.,\\n2024; Zhou and Chen, 2025). In contrast, relatively little attention has been given to improving the\\nmodel’s ability to resist using the incorrect information.\\nIn this paper, we introduceFinetune-RAG, a method that directly targets hallucination by fine-tuning\\nthe model with imperfect RAG samples that mimic real-world retrieval scenarios. We constructed a\\n∗Work was done during an internship at Pints AI\\n2https://github.com/Pints-AI/Finetune-Bench-RAG\\n3https://huggingface.co/datasets/pints-ai/Finetune-RAG\\narXiv:2505.10792v3  [cs.CL]  3 Dec 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='diverse dataset covering legal documents, scientific literature, books, and web data, each paired with\\na plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta’s\\nLlama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: aBaseline\\nformatand aStructured XMLvariant. This setup allows us to assess generalization and prompt\\nsensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing\\nRAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the\\nLLM’s ability to extract coherent answers from relevant chunks.\\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye,\\nsuch as financial reports from a similarly named company or outdated information based on dates\\nindicated by document metadata. Through fine-tuning models with a controlled mixture of true and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='such as financial reports from a similarly named company or outdated information based on dates\\nindicated by document metadata. Through fine-tuning models with a controlled mixture of true and\\nfalse context placed alongside, we teach them to ground their answers exclusively in the reliable\\ninformation provided.\\nWe evaluated the effectiveness of Finetune-RAG usingBench-RAG, a custom benchmarking suite\\nwe have created that leveragesGPT-4o(OpenAI, 2024) as an automated judge to assess the accu-\\nracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG\\nsubstantially improves factual correctness while maintaining output quality across other dimensions,\\ndemonstrating that generation-time defenses are a viable complement to improved retrieval.\\nOur contributions are as follows:\\n• Fine-tuning Approach.We propose a novel fine-tuning strategy for RAG systems that\\nteaches models to ignore misleading context and generate answers based solely on factual\\ninput.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Fine-tuning Approach.We propose a novel fine-tuning strategy for RAG systems that\\nteaches models to ignore misleading context and generate answers based solely on factual\\ninput.\\n• Training Dataset.We release a curated, multi-domain dataset designed for hallucination\\nresistance training, with both factual and fictitious content.\\n• Evaluation Setup.We benchmark the effectiveness of our approach using GPT-4o-based\\nevaluations and show significant gains in factual accuracy without compromising helpfulness\\nor relevance.\\n• Open-source release.We make our code, models, dataset, and evaluation framework\\npublicly available to facilitate further research. They can be accessed in our open-source\\nrepository4 and dataset5.\\nBy fine-tuning LLMs on RAG examples containing both factual and fictitious documents, we show\\nthat it is possible to build models that can reliably choose truth over noise. Our dataset reflects noisy,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='that it is possible to build models that can reliably choose truth over noise. Our dataset reflects noisy,\\ndomain-diverse retrieval as encountered in practice, making it a strong foundation for stress-testing\\nhallucination resistance in future RAG systems.\\n2 Background\\n2.1 Retrieval-Augmented Generation\\nRetrieval-Augmented Generation (RAG) augments large language models by incorporating external\\ndocuments into the generation process. Rather than relying solely on the model’s internal parameters,\\nRAG retrieves relevant passages from a knowledge base and feeds them, along with the user query,\\ninto the model to guide its response (Zhou et al., 2024).\\nA standard RAG system operates in two phases:\\n•Retrieval.A retriever model selects the top-kmost relevant documents for a given query.\\n• Generation.A language model generates a response conditioned on both the query and the\\nretrieved documents.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='•Retrieval.A retriever model selects the top-kmost relevant documents for a given query.\\n• Generation.A language model generates a response conditioned on both the query and the\\nretrieved documents.\\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information,\\nwhich is especially useful in fast-changing or specialized fields. However, it also introduces new\\nfailure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).\\n4https://github.com/Pints-AI/Finetune-Bench-RAG\\n5https://huggingface.co/datasets/pints-ai/Finetune-RAG\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='2.2 Hallucination in Language Models\\nHallucination refers to the phenomenon where language models produce outputs that are factually\\nincorrect or unsupported by the input, resulting in unfaithful outputs (Rawte et al., 2023). In RAG\\nsystems, hallucination can be especially problematic when the model is presented with a mixture of\\nrelevant and irrelevant (or even misleading) context. Even with carefully worded prompts, models\\ncan inadvertently \"trust\" incorrect sources and generate plausible but wrong answers (Yoran et al.,\\n2024).\\nDespite the presence of external context, most current models lack mechanisms to actively filter or\\nignore misleading information once it is included in the prompt (Shi et al., 2023). Finetune-RAG\\nspecifically targets this weakness by training models to develop this filtering capability.\\n3 Related Works\\n3.1 Mitigating Hallucination with Synthetic Prompt Tuning\\nSYNTRA (Jones et al., 2023) reduces hallucinations in large language models by modifying the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='3 Related Works\\n3.1 Mitigating Hallucination with Synthetic Prompt Tuning\\nSYNTRA (Jones et al., 2023) reduces hallucinations in large language models by modifying the\\nmodel’s instructions rather than adjusting its internal weights. SYNTRA does this by attaching a\\nsmall, trainable embedding vector to the system message, which acts as an additional instruction\\nprefix. This vector is optimized using a synthetic task where hallucinations are easy to measure. For\\nexample, the model is prompted to return names starting with a specific letter from a visible list, and\\nany incorrect or invented names are counted as hallucinations. By learning to avoid such mistakes in\\na controlled setting, the model can generalize to reduce hallucinations in downstream tasks. However,\\nbecause SYNTRA focuses on modifying prompts and not the model’s internal reasoning, it does not\\nenable the model to distinguish between factual and misleading content, failing to address real-world'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='because SYNTRA focuses on modifying prompts and not the model’s internal reasoning, it does not\\nenable the model to distinguish between factual and misleading content, failing to address real-world\\nRAG scenarios (Barnett et al., 2024)(Shi et al., 2023).\\n3.2 Refusal-Aware Fine-Tuning\\nZhang et al. (2024) propose a fine-tuning method, R-Tuning, that teaches language models to express\\nuncertainty and decline to answer when a question falls outside their pre-trained knowledge. This is\\nachieved by identifying questions the model answers incorrectly during training and appending an\\nuncertainty statement such as “I am unsure” to those responses. The result is a model that behaves\\nmore conservatively and with improved confidence calibration. However, R-Tuning is designed for\\nclosed-book settings, where the model relies only on its internal knowledge without a RAG system.\\n3.3 Constrained Reasoning with Decompose-and-Query (D&Q)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='closed-book settings, where the model relies only on its internal knowledge without a RAG system.\\n3.3 Constrained Reasoning with Decompose-and-Query (D&Q)\\nCao et al. (2023) propose the Decompose-and-Query (D&Q) framework, which extends retrieval-\\naugmented generation (RAG) by teaching language models to break down complex queries, retrieve\\nrelevant information using external tools, and generate answers based on a structured knowledge\\nsource. In particular, D&Q introduces a curated question–answer (QA) base, which is a collection of\\nverified QA pairs that the model consults during reasoning. This setup helps reduce hallucinations\\nby constraining the model to reliable content and allowing it to backtrack when inconsistencies are\\ndetected.\\nHowever, the effectiveness of D&Q depends strongly on the quality and coverage of its QA base. In\\npractical RAG applications, where retrieved content can be noisy, ambiguous, or incomplete (Shi'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='However, the effectiveness of D&Q depends strongly on the quality and coverage of its QA base. In\\npractical RAG applications, where retrieved content can be noisy, ambiguous, or incomplete (Shi\\net al., 2023), relying on a fixed and curated source may become a limitation. Since the framework\\nlacks mechanisms to dynamically assess the reliability of new information, it remains susceptible to\\nhallucinations caused by misleading or inaccurate context.\\n4 Methodology\\nWe introduceFinetune-RAG, a fine-tuning method designed to train large language models (LLMs)\\nto distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG)\\nsetup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase,\\nFinetune-RAG focuses on improving the model’s generation behavior when faced with imperfect or\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='misleading inputs. Our core idea is to fine-tune the model using examples where both correct and\\nincorrect information are explicitly presented to model, allowing it to learn the ability to sift out the\\ncorrect information to use for its response.\\n4.1 Problem Setup\\nIn a typical RAG system, the model is given a user query q and a set of retrieved documents\\n{d1, d2, ..., dk} (Zhou et al., 2024). When any of the documents is irrelevant or misleading, the model\\nmay generate incorrect responses (Yoran et al., 2024).\\nIn Finetune-RAG, we simulate this scenario during training by constructing prompts that include:\\n• One correct (factual) document chunkd correct\\n• One fictitious (misleading) document chunkd fictitious\\n• A corresponding questionq\\n• A reference answera, written using onlyd correct as the reference\\nThe model is then trained using supervised fine-tuning to produce the answer a despite having access\\nto bothd correct andd fictitious in the input.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='The model is then trained using supervised fine-tuning to produce the answer a despite having access\\nto bothd correct andd fictitious in the input.\\nIn Bayesian modeling, we can think of the task as a conditional generation problem where the goal is\\nto maximize the probability of generating a truthful answer a given a question q and a mixed set of\\ncontexts (some correctd correct, some fictitiousd fictitious).\\nWe aim to model:\\nP(a|q, d correct, dfictitious)(1)\\nHowever, this is the observed conditional probability, and what we want the model to learn is to\\nignore dfictitious and generate the answer as if conditioned only on dcorrect. So our training objective is\\nto align to the following idealized posterior:\\nP∗(a|q, dcorrect, dfictitious)→P(a|q, d correct)(2)\\nIn other words, even though the model receives both correct and fictitious information, it must assign\\nzero (or negligible) attention/mass tod fictitious during decoding.\\n4.2 Prompt Construction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='In other words, even though the model receives both correct and fictitious information, it must assign\\nzero (or negligible) attention/mass tod fictitious during decoding.\\n4.2 Prompt Construction\\nEach training example in Finetune-RAG is processed to include asystem messageand auser\\nmessage, following the standard instruction-tuning format (Ouyang et al., 2022) used in chat-style\\nlanguage models. The system message defines the behavior of the assistant, while the user message\\nprovides the question along with correct and fictitious information.\\n4.2.1 System Message\\nThe system message is consistent in all training examples. It instructs the assistant to rely solely on\\nthe provided context and discourages the use of prior knowledge or hallucination:\\n\"Some information is retrieved from the database as provided based on the\\nuser’s question. The assistant is to answer the question to the best of\\nhis/her ability, using only the information provided. The assistant must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='user’s question. The assistant is to answer the question to the best of\\nhis/her ability, using only the information provided. The assistant must\\nnot add his/her own knowledge.\"\\n4.2.2 User Message\\nTo help the model distinguish between factual and fictitious context more effectively, we explore the\\nuse of XML-like (Bray et al., 1998) structured input. We hypothesize that introducing a consistent\\nand explicit hierarchy, where document chunks are clearly labeled and separated, can make it easier\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='for the model to parse and evaluate different sources of information. This is especially important in\\nRAG settings, where hallucinations often result from the model blending or misattributing content\\nacross documents. Our approach aligns with findings from recent work such as StructRAG (Li et al.,\\n2024b) and SRAG (Lin et al., 2025), which demonstrates that task-specific structured representations\\nsuch as tables or graphs can significantly improve the performance of LLMs on knowledge-intensive\\nreasoning tasks. Our use of XML aims to impose syntactic clarity and boundary enforcement at the\\ninput level.\\nTo test this, we compare two user message formats: an unstructuredBaseline Formatand a structured\\nXML Format. Both present a question along with two document chunks, one factual and one\\nfictitious, but differ in how the information is presented. Refer to Section 6.4 for the exact prompt\\nstructure.\\n5 Experimental Setup\\n5.1 Model'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='fictitious, but differ in how the information is presented. Refer to Section 6.4 for the exact prompt\\nstructure.\\n5 Experimental Setup\\n5.1 Model\\nWe fine-tuned Meta’s Llama 3.1–8B-Instruct (Grattafiori et al., 2024), an instruct-tuned model that\\nsupports chat-style interaction and long context windows. We adapt the system and user message\\nformatting based on the chosen prompt structure described in Section 4.2.2.\\n5.2 Dataset and Preprocessing\\nOur dataset contains a total of 1,653 examples from diverse domains, such as legal documents,\\nscientific papers, news articles, and technical reports. For the complete structure of each example in\\nthe dataset, refer to Annex A.\\nEach example is formatted in both the baseline and XML structures. The dataset is then partitioned\\ninto training (80%), validation (10%), and test (10%) sets.\\n5.3 Hyperparameters\\nWe selected hyperparameter values that balance model performance with computational efficiency.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='into training (80%), validation (10%), and test (10%) sets.\\n5.3 Hyperparameters\\nWe selected hyperparameter values that balance model performance with computational efficiency.\\nRefer to Table 1 for the complete set of hyperparameters used.\\nTable 1: Fine-tuning hyperparameters used on Llama 3.1-8B-Instruct\\nParameter Value\\nSteps 20\\nBatch size 64\\nLearning rate 2e-5\\nWarmup ratio 0.1\\nLR Scheduler Cosine decay\\nOptimizer AdamW\\nβ1 0.9\\nβ2 0.95\\nWeight decay 0.1\\nMixed precision BF16\\n5.4 Checkpoints and Reproducibility\\nWe have released the model checkpoints fine-tuned with both Baseline6 and XML7 formats on Hug-\\ngingFace. Each prompt structure has two repositories, and each repository contains five checkpoints,\\ntotaling 10 checkpoints each.\\n6https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-2\\n7https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-2\\n7https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-2\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='6 Evaluation\\nWe evaluate Finetune-RAG’s ability to generate factually accurate answers when presented with both\\ncorrect and fictitious context. Our evaluation framework focuses on measuring whether the model\\nis able toselectively use only the correct information, and we assess output quality across four key\\ndimensions.\\n6.1 Bench-RAG\\nWe adopt a custom benchmarking pipeline, namelyBench-RAG, usingGPT-4omodel (OpenAI,\\n2024) in a LLM-as-a-judge inspired by prior work(Zheng et al., 2023; Gu et al., 2025; Li et al., 2025).\\nUsing structured prompts to elicit consistent evaluations for each model output, we measure:\\n• Accuracy: A binary metric indicating whether the generated answer is factually correct and\\nbased solely on the correct chunk. (True/False)\\n• Helpfulness: A score from 1 to 10 assessing how useful the answer is in addressing the\\nuser’s question.\\n•Relevance: A score from 1 to 10 measuring how relevant the content is to the query.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Helpfulness: A score from 1 to 10 assessing how useful the answer is in addressing the\\nuser’s question.\\n•Relevance: A score from 1 to 10 measuring how relevant the content is to the query.\\n•Depth: A score from 1 to 10 reflecting the level of detail or insight present in the answer.\\nEach generated output is rated using a structured prompt format, which requests scores across these\\ncategories and a brief justification. Refer to Appendix B for the full structure. This methodology\\ndraws from recent research demonstrating that LLMs can align closely with human preferences when\\nprompted properly, achieving high inter-rater agreement, i.e. multiple evaluators provide consistent\\nratings for the same outputs (Gu et al., 2025; Li et al., 2025).\\n6.2 Checkpoints Evaluated\\nFor each prompt structure, we evaluate all 10 model checkpoints saved during training (see Section\\n5.4). These checkpoints represent the model’s learning trajectory over the course of a single fine-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='For each prompt structure, we evaluate all 10 model checkpoints saved during training (see Section\\n5.4). These checkpoints represent the model’s learning trajectory over the course of a single fine-\\ntuning epoch. At each checkpoint, we generate answers to the test dataset questions using both the\\ncorrect context dcorrect and the fictitious context dfictitious. The generated answers are then submitted to\\nthe evaluator for scoring. Refer to Appendix B.1 and B.2 for the structure of the prompt used for\\nevaluation.\\n6.3 Results\\nWe report quantitative results from our fine-tuning experiments scored across 4 dimensions:factual\\naccuracy, helpfulness, relevance, and depth. Evaluation was performed using GPT-4o (OpenAI,\\n2024) as an LLM judge, as described in Section 6.1. We then aggregate the scores of each sequence\\nin the test dataset to derive the final evaluation result for each checkpoint:\\n¯Accuracy=\\n \\n1\\nntest\\nntestX\\ni=1\\n1[Accuracyi =T rue]\\n!\\n×100%(3)\\n¯Helpfulness= 1\\nntest\\nntestX\\ni=1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='in the test dataset to derive the final evaluation result for each checkpoint:\\n¯Accuracy=\\n \\n1\\nntest\\nntestX\\ni=1\\n1[Accuracyi =T rue]\\n!\\n×100%(3)\\n¯Helpfulness= 1\\nntest\\nntestX\\ni=1\\nHelpfulness i (4)\\n¯Relevance= 1\\nntest\\nntestX\\ni=1\\nRelevancei (5)\\n¯Depth= 1\\nntest\\nntestX\\ni=1\\nDepthi (6)\\nFigures 1 and 2 summarize performance trends across training steps. We observe consistent improve-\\nments in factual accuracy over time, particularly in the Baseline format. In most cases, gains in\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='accuracy are achieved without sacrificing helpfulness or relevance, and in later checkpoints, all four\\nmetrics reach strong levels of performance.\\nNotably, accuracy rises from 76.97% at step 0 to 98.18% at step 20 in the Baseline format, demon-\\nstrating the model’s increasing ability to ignore fictitious context. Helpfulness and depth also improve\\nsteadily, with a dip at the first generated checkpoint.\\nFigure 1: Evaluation results across training steps (Baseline format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. (%) Help Rel. Depth\\n0 76.97 8.81 9.55 8.32\\n2 67.88 7.08 7.48 6.76\\n4 91.52 8.08 8.47 7.15\\n6 93.94 9.58 9.83 8.81\\n8 96.36 9.38 9.61 8.55\\n10 97.58 9.33 9.62 8.51\\n12 96.36 9.52 9.78 8.80\\n14 96.97 9.73 9.91 9.01\\n16 97.589.789.959.06\\n18 97.58 9.77 9.95 9.05\\n2098.189.779.959.02 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='16 97.589.789.959.06\\n18 97.58 9.77 9.95 9.05\\n2098.189.779.959.02 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\nFigure 2: Evaluation results across training steps (XML format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. Help Rel Depth\\n0 78.79 8.81 9.56 8.19\\n2 52.73 5.79 6.16 5.24\\n4 87.88 6.56 7.09 5.47\\n6 95.76 9.46 9.73 8.75\\n8 94.55 9.09 9.35 8.21\\n10 94.55 8.93 9.32 8.01\\n12 95.76 8.95 9.33 8.05\\n14 95.76 9.28 9.59 8.52\\n16 97.58 9.35 9.61 8.61\\n1897.589.28 9.50 8.50\\n20 96.979.40 9.64 8.64 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='6.4 Ablation: Effect of Prompt Structure\\nTo assess the impact of prompt formatting on hallucination resistance, we perform an ablation study\\ncomparing two versions of Finetune-RAG: one trained using theBaseline formatand another using\\na more structuredXML format. Both models were fine-tuned on the same dataset with identical\\nhyperparameters and evaluated using the same GPT-4o-based benchmarking pipeline.\\nPrompt Format DifferencesThe Baseline format presents context in a flat, unstructured layout,\\nwhile the XML format uses nested tags to explicitly delineate retrieved content blocks (see Section\\n4.2.2). We hypothesized that structured formatting might help the model better separate and reason\\nabout distinct chunks.\\nBaseline FormatThis format presents the retrieved content in a plain and direct layout:\\nFilename: {filename1}\\nInformation:\\n{content1}\\nFilename: {filename2}\\nInformation:\\n{content2}\\nQuestion: {question}'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Filename: {filename1}\\nInformation:\\n{content1}\\nFilename: {filename2}\\nInformation:\\n{content2}\\nQuestion: {question}\\nXML FormatThis version wraps the content in an XML-like structure for clearer boundaries:\\n<Results>\\n<Result>\\n<Filename>{filename1}</Filename>\\n<Information>{content1}</Information>\\n</Result>\\n<Result>\\n<Filename>{filename2}</Filename>\\n<Information>{content2}</Information>\\n</Result>\\n</Results>\\nQuestion: {question}\\nResultsAs shown in Figures 1 and 2, both models demonstrate strong improvements over time.\\nHowever, the Baseline model consistently achieves higher accuracy and better overall scores in the\\nlater checkpoints:\\n• At step 20, the Baseline-tuned model achieves an accuracy of 98.18%, compared to 96.97%\\nfor the XML-tuned model.\\n• The Baseline-tuned model also maintains slightly higher scores for helpfulness (9.77 vs\\n9.40) and depth (9.02 vs 8.64).\\nInterpretationThese results suggest that while XML-style formatting introduces clear structural'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='9.40) and depth (9.02 vs 8.64).\\nInterpretationThese results suggest that while XML-style formatting introduces clear structural\\nboundaries that aid human readers, it did not consistently outperform the simpler Baseline prompt. We\\noffer two possible explanations: (1) the model may have developed inductive biases from pretraining\\nthat favor interpreting flat, plain-text layouts, such as those seen in summaries or abstracts, and (2)\\nfine-tuning datasets used in LLaMA or similar models may have predominantly featured unstructured\\nprompts, making the model more adept at handling them.\\nThis suggests that while prompt formatting is an important factor, training data design and supervision\\nsignal play a larger role in hallucination resistance.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='7 Discussion\\nOur results show that Finetune-RAG significantly improves a model’s ability to resist hallucinations\\nin a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning\\nwith dual-context examples leads to consistent improvements in factual accuracy, while preserving\\nhelpfulness, relevance, and depth.\\n7.1 Inductive Bias Emergence in Structure-Agnostic Learning\\nA significant and perhaps unexpected result in our study is that models trained on unstructured\\nprompts (Baseline format) performed better, especially in factual accuracy, compared to those trained\\nwith structured XML prompts. This challenges the common belief that clear structure always aids\\nreasoning. Instead, it suggests a deeper learning process, which involves the development of stronger\\nbuilt-in tendencies for selecting content when structure is absent. This raises a potential area that can\\nbe further researched upon.\\n7.2 Limitations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='built-in tendencies for selecting content when structure is absent. This raises a potential area that can\\nbe further researched upon.\\n7.2 Limitations\\nDespite promising results, several limitations remain:\\n• Synthetic dataset generation: The fictitious content is generated using GPT-4o (OpenAI,\\n2024), which may introduce distributional artifacts that differ from real-world retrieval\\nerrors. Additionally, the size of the dataset can be further increased for effective fine-tuning\\nin larger models.\\n• Binary supervision: We treat hallucination as a binary decision at the generation level.\\nHowever, hallucination is often more nuanced, involving partial truths, omissions, or subtle\\nphrasing, which our current framework may not sufficiently address.\\n• Controlled context pairing: During training, each example includes exactly one correct\\nand one incorrect document chunk. This creates a simplified binary contrast that may not'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Controlled context pairing: During training, each example includes exactly one correct\\nand one incorrect document chunk. This creates a simplified binary contrast that may not\\ngeneralize to real-world scenarios where multiple retrieved documents vary in quality. A\\nstronger training approach can be constructed using our existing dataset to create more\\nvaried and robust scenarios that the model can train on.\\n• Compute requirements: While our method is simpler and less resource-intensive than\\nalternatives such as full retraining or reinforcement learning, it still requires access to a\\nhigh-memory GPU (e.g., H100) to fine-tune long-context models with large batch sizes.\\nThis may limit accessibility for some users or institutions.\\n7.3 Future Work\\nThere are several promising extensions to Finetune-RAG that could further improve its robustness\\nand applicability:\\n• Training with more in-context RAG: Real-world retrieval often returns more than two'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='and applicability:\\n• Training with more in-context RAG: Real-world retrieval often returns more than two\\ndocuments, and the context window of LLMs are increasing rapidly. At the time of our\\nwork, we focused on relatively low context window of 8k, which would realistically be used\\nfor two to three RAG documents using up to 3k context window. With increasing context\\nwindow, future work can explore training with more RAG chunks to optimize LLMs RAG\\nperformance even at high level of stresses caused by more retrieved chunks. To support this,\\nwe future-proofed our dataset by including two additional relevant chunks per example to\\nsupport generating more complex multi-document training scenarios.\\n• Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the\\ngeneration component, combining it with learned retrieval mechanisms such as reranker-\\naware retrievers or contrastively trained retrievers could lead to further improvements in'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='generation component, combining it with learned retrieval mechanisms such as reranker-\\naware retrievers or contrastively trained retrievers could lead to further improvements in\\nfactual accuracy and context filtering.\\n• Multimodal extensions: Hallucination is not limited to text-based models. Ex-\\ntending Finetune-RAG to multimodal settings, such as image-caption retrieval or\\ncode+documentation generation, may help build more robust grounded systems in other\\ndomains.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='• Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucina-\\ntion settings, future work should assess Finetune-RAG’s impact on end-to-end performance\\nin downstream RAG applications such as open-domain question answering, legal document\\nsummarization, and domain-specific information retrieval.\\n8 Conclusion\\nIn this work, we presentFinetune-RAG, a simple yet effective method for reducing hallucination in\\nRetrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on\\nretrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while\\nignoring misleading information, with no architectural changes required.\\nWe constructed a diverse training set and evaluate usingBench-RAG, a technique that leverages\\nGPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving\\nhelpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts\\nrobustness, with less structured formats sometimes aiding discrimination.\\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully\\nimprove hallucination resistance in noisy retrieval environments. We release our code, dataset, and\\ncheckpoints to support further research in this direction, and highlight future extensions including\\nmulti-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='References\\nAgarwal, V ., Jin, Y ., Chandra, M., Choudhury, M. D., Kumar, S., and Sastry, N. (2024). Medhalu:\\nHallucinations in responses to healthcare queries by large language models.\\nBarnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., and Abdelrazek, M. (2024). Seven failure\\npoints when engineering a retrieval augmented generation system.\\nBray, T., Paoli, J., Sperberg-McQueen, C. M., Maler, E., and Yergeau, F. (1998). Extensible markup\\nlanguage (xml) 1.0.https://www.w3.org/TR/REC-xml/. W3C Recommendation.\\nCao, H., An, Z., Feng, J., Xu, K., Chen, L., and Zhao, D. (2023). A step closer to comprehensive\\nanswers: Constrained multi-stage question decomposition with large language models.\\nCui, M., Gao, P., Liu, W., Luan, J., and Wang, B. (2025). Multilingual machine translation with open\\nlarge language models at practical scale: An empirical study.\\nDahl, M., Magesh, V ., Suzgun, M., and Ho, D. E. (2024). Large legal fictions: Profiling legal'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='large language models at practical scale: An empirical study.\\nDahl, M., Magesh, V ., Suzgun, M., and Ho, D. E. (2024). Large legal fictions: Profiling legal\\nhallucinations in large language models.Journal of Legal Analysis, 16(1):64–93.\\nDong, J., Fatemi, B., Perozzi, B., Yang, L. F., and Tsitsulin, A. (2024). Don’t forget to connect!\\nimproving rag with graph-based reranking.\\nDuan, H., Yang, Y ., and Tam, K. Y . (2024). Do llms know about hallucination? an empirical\\ninvestigation of llm’s hidden states.\\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., and et al.\\n(2024). The llama 3 herd of models.\\nGu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y ., Ma, S., Liu, H., Wang, S., Zhang,\\nK., Wang, Y ., Gao, W., Ni, L., and Guo, J. (2025). A survey on llm-as-a-judge.\\nJones, E., Palangi, H., Simões, C., Chandrasekaran, V ., Mukherjee, S., Mitra, A., Awadallah, A., and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='K., Wang, Y ., Gao, W., Ni, L., and Guo, J. (2025). A survey on llm-as-a-judge.\\nJones, E., Palangi, H., Simões, C., Chandrasekaran, V ., Mukherjee, S., Mitra, A., Awadallah, A., and\\nKamar, E. (2023). Teaching language models to hallucinate less with synthetic tasks.\\nKang, H. and Liu, X.-Y . (2023). Deficiency of large language models in finance: An empirical\\nexamination of hallucination.\\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z., Bhattacharjee, A., Jiang, Y ., Chen, C.,\\nWu, T., Shu, K., Cheng, L., and Liu, H. (2025). From generation to judgment: Opportunities and\\nchallenges of llm-as-a-judge.\\nLi, J., Chen, J., Ren, R., Cheng, X., Zhao, W. X., Nie, J.-Y ., and Wen, J.-R. (2024a). The dawn after\\nthe dark: An empirical study on factuality hallucination in large language models.\\nLi, Z., Chen, X., Yu, H., Lin, H., Lu, Y ., Tang, Q., Huang, F., Han, X., Sun, L., and Li, Y . (2024b).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='the dark: An empirical study on factuality hallucination in large language models.\\nLi, Z., Chen, X., Yu, H., Lin, H., Lu, Y ., Tang, Q., Huang, F., Han, X., Sun, L., and Li, Y . (2024b).\\nStructrag: Boosting knowledge intensive reasoning of llms via inference-time hybrid information\\nstructurization.\\nLin, T., Zhu, Y ., Luo, Y ., and Tang, N. (2025). Srag: Structured retrieval-augmented generation for\\nmulti-entity question answering over wikipedia graph.\\nLiu, Y ., Shi, K., He, K., Ye, L., Fabbri, A., Liu, P., Radev, D., and Cohan, A. (2024). On learning to\\nsummarize with large language models as references. In Duh, K., Gomez, H., and Bethard, S.,\\neditors,Proceedings of the 2024 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\\n8647–8664, Mexico City, Mexico. Association for Computational Linguistics.\\nOpenAI (2024). Gpt-4o system card.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\\n8647–8664, Mexico City, Mexico. Association for Computational Linguistics.\\nOpenAI (2024). Gpt-4o system card.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\\nWelinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow\\ninstructions with human feedback.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Rawte, V ., Chakraborty, S., Pathak, A., Sarkar, A., Tonmoy, S. M. T. I., Chadha, A., Sheth, A. P., and\\nDas, A. (2023). The troubling emergence of hallucination in large language models – an extensive\\ndefinition, quantification, and prescriptive remediations.\\nRozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Sauvestre, R.,\\nRemez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori,\\nA., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T.,\\nand Synnaeve, G. (2024). Code llama: Open foundation models for code.\\nSawarkar, K., Mangal, A., and Solanki, S. R. (2024). Blended rag: Improving rag (retriever-\\naugmented generation) accuracy with semantic search and hybrid query-based retrievers. In2024\\nIEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),\\nvolume 24, page 155–161. IEEE.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),\\nvolume 24, page 155–161. IEEE.\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., Schärli, N., and Zhou, D. (2023). Large\\nlanguage models can be easily distracted by irrelevant context.\\nWang, Y ., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and Hoi, S. C. H. (2023). Codet5+: Open\\ncode large language models for code understanding and generation.\\nYasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J. (2022). Qa-gnn: Reasoning with\\nlanguage models and knowledge graphs for question answering.\\nYoran, O., Wolfson, T., Ram, O., and Berant, J. (2024). Making retrieval-augmented language models\\nrobust to irrelevant context.\\nZhang, H., Diao, S., Lin, Y ., Fung, Y . R., Lian, Q., Wang, X., Chen, Y ., Ji, H., and Zhang, T. (2024).\\nR-tuning: Instructing large language models to say ‘i don’t know’.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Zhang, H., Diao, S., Lin, Y ., Fung, Y . R., Lian, Q., Wang, X., Chen, Y ., Ji, H., and Zhang, T. (2024).\\nR-tuning: Instructing large language models to say ‘i don’t know’.\\nZhang, Y ., Li, Y ., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y ., Chen, Y ., Wang,\\nL., Luu, A. T., Bi, W., Shi, F., and Shi, S. (2023). Siren’s song in the ai ocean: A survey on\\nhallucination in large language models.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\\nE. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and\\nchatbot arena.\\nZhou, J. and Chen, L. (2025). Openrag: Optimizing rag end-to-end via in-context retrieval learning.\\nZhou, Y ., Liu, Y ., Li, X., Jin, J., Qian, H., Liu, Z., Li, C., Dou, Z., Ho, T.-Y ., and Yu, P. S. (2024).\\nTrustworthiness in retrieval-augmented generation systems: A survey.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='A Dataset Example Format\\nAligning with Section 4.1, each example in the dataset is structured as follows:\\n{\\n\"content\": <factual chunk>,\\n\"filename\": <original document filename>,\\n\"fictitious_content\": <misleading chunk>,\\n\"fictitious_filename\": <filename of misleading chunk>,\\n\"question\": <user query>,\\n\"answer\": <GPT-4o generated answer based only on correct content>,\\n}\\nB Bench-RAG Prompt Structure\\nGiven both the correct and fictitious document chunks, the fine-tuned model checkpoints are used\\nto generate answers for questions on the test dataset. The outputs are stored in a structured jsonl\\nformat, with each entry containing the following fields:\\n{\\n\"filename\": <original document filename>,\\n\"content\": <factual chunk>,\\n\"question\": <user query>,\\n\"response\": <model’s generated answer>\\n}\\nWith these output, we curate a prompt for the four measurements derived from our evaluation.\\nB.1 System Message for Evaluation\\nAccuracy\\n\"Please act as an impartial judge and evaluate the quality of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='B.1 System Message for Evaluation\\nAccuracy\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the accuracy of the\\nresponse.\\nYou will check whether the response contains extra details not found\\nin the piece of information provided. If extra details are found,\\naccuracy is false. Otherwise, accuracy is true. Take note that if the\\nresponse partially addresses the question, but did not provide extra\\ndetails not found in the piece of information provided, the response\\nwill still be considered accurate (hence accuracy = true).\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the accuracy\\nwith true or false by strictly following this JSON format:\\n{\\n\"accuracy_explanation\":'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='as possible. After providing your explanation, you must rate the accuracy\\nwith true or false by strictly following this JSON format:\\n{\\n\"accuracy_explanation\":\\n<provide an explanation on accuracy, whether extra details\\noutside the content were found.>,\\n\"accuracy\": <true/false>\\n}\"\\nHelpfulness\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='below, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the helpfulness of the\\nresponse.\\nYou will check whether the AI assistant is helpful in answering the\\nquestion based on the response.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nhelpfulness on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"helpfulness_explanation\": <provide an explanation on helpfulness>,\\n\"helpfulness\": <score>\\n}\"\\nRelevance\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the relevance of the\\nresponse.\\nYou will check the relevance of the response by evaluating whether the\\nresponse fully addresses the question.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='provided below. Your evaluation should consider the relevance of the\\nresponse.\\nYou will check the relevance of the response by evaluating whether the\\nresponse fully addresses the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nrelevance on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"relevance_explanation\": <provide an explanation on relevance>,\\n\"relevance\": <score>\\n}\"\\nDepth\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the depth of the\\nresponse.\\nYou will check the depth of the response by evaluating the level of\\ndetail of the response in answering the question.\\nBegin your evaluation by providing a short explanation. Be as objective'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='response.\\nYou will check the depth of the response by evaluating the level of\\ndetail of the response in answering the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\ndepth on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"depth_explanation\": <provide an explanation on depth>,\\n\"depth\": <score>\\n}\"\\nB.2 User Message for Evaluation\\nAll measurements utilizes the same user message structure for evaluation. Note that the content used\\nis the correct content, rather than the fictitious one:\\n[The Start of Provided Information Extracted from a File]\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:4177c2c)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.28 (TeX Live 2025) kpathsea version 6.4.1', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v3', 'source': '..\\\\data\\\\pdf_files\\\\finetune.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'finetune.pdf', 'file_type': 'pdf'}, page_content='Filename: {filename}\\nInformation: {content}\\n[The End of Provided Information]\\n[Question]\\n{question}\\n[The Start of Assistant’s Response]\\n{response}\\n[The End of Assistant’s Response]\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Rethinking the Role of Demonstrations:\\nWhat Makes In-Context Learning Work?\\nSewon Min1,2 Xinxi Lyu1 Ari Holtzman1 Mikel Artetxe2\\nMike Lewis2 Hannaneh Hajishirzi1,3 Luke Zettlemoyer1,2\\n1University of Washington 2Meta AI 3Allen Institute for AI\\n{sewon,alrope,ahai,hannaneh,lsz}@cs.washington.edu\\n{artetxe,mikelewis}@meta.com\\nAbstract\\nLarge language models (LMs) are able to in-\\ncontext learn—perform a new task via infer-\\nence alone by conditioning on a few input-\\nlabel pairs (demonstrations) and making pre-\\ndictions for new inputs. However, there has\\nbeen little understanding of how the model\\nlearns and which aspects of the demonstra-\\ntions contribute to end task performance. In\\nthis paper, we show that ground truth demon-\\nstrations are in fact not required—randomly\\nreplacing labels in the demonstrations barely\\nhurts performance on a range of classiﬁcation\\nand multi-choce tasks, consistently over 12 dif-\\nferent models including GPT-3. Instead, we\\nﬁnd that other aspects of the demonstrations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='hurts performance on a range of classiﬁcation\\nand multi-choce tasks, consistently over 12 dif-\\nferent models including GPT-3. Instead, we\\nﬁnd that other aspects of the demonstrations\\nare the key drivers of end task performance, in-\\ncluding the fact that they provide a few exam-\\nples of (1) the label space, (2) the distribution\\nof the input text, and (3) the overall format of\\nthe sequence. Together, our analysis provides\\na new way of understanding how and why\\nin-context learning works, while opening up\\nnew questions about how much can be learned\\nfrom large language models through inference\\nalone.\\n1 Introduction\\nLarge language models (LMs) have shown impres-\\nsive performance on downstream tasks by simply\\nconditioning on a few input-label pairs (demonstra-\\ntions); this type of inference has been referred to as\\nin-context learning (Brown et al., 2020). Despite in-\\ncontext learning consistently outperforming zero-\\nshot inference on a wide range of tasks (Zhao et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='in-context learning (Brown et al., 2020). Despite in-\\ncontext learning consistently outperforming zero-\\nshot inference on a wide range of tasks (Zhao et al.,\\n2021; Liu et al., 2021), there is little understanding\\nof how it works and which aspects of the demon-\\nstrations contribute to end task performance.\\nIn this paper, we show that ground truth demon-\\nstrations are in fact not required for effective in-\\ncontext learning (Section 4). Speciﬁcally, replac-\\ning the labels in demonstrations with random labels\\nbarely hurts performance in a range of classiﬁca-\\ntion and multi-choice tasks (Figure 1). The result\\nMetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nMetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 1: Results in classiﬁcation (top) and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (774M) GPT-J (6B) GPT-3 (175B)\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 1: Results in classiﬁcation (top) and multi-\\nchoice tasks (bottom), using three LMs with varying\\nsize. Reported on six datasets on which GPT-3 is eval-\\nuated; the channel method is used. See Section 4 for\\nthe full results. In-context learning performance drops\\nonly marginally when labels in the demonstrations are\\nreplaced by random labels.\\nis consistent over 12 different models including the\\nGPT-3 family (Radford et al., 2019; Min et al.,\\n2021b; Wang and Komatsuzaki, 2021; Artetxe\\net al., 2021; Brown et al., 2020). This strongly\\nsuggests, counter-intuitively, that the model does\\nnot rely on the input-label mapping in the demon-\\nstrations to perform the task.\\nFurther analysis investigates which parts of\\ndemonstrations actually do contribute to the perfor-\\nmance. We identify possible aspects of demonstra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='strations to perform the task.\\nFurther analysis investigates which parts of\\ndemonstrations actually do contribute to the perfor-\\nmance. We identify possible aspects of demonstra-\\ntions (e.g., the label space and the distribution of\\nthe input text) and evaluate a series of variants of\\nthe demonstrations to quantify the impact of each\\n(Section 5). We ﬁnd that: (1) the label space and\\nthe distribution of the input text speciﬁed by the\\ndemonstrations are both key to in-context learn-\\ning (regardless of whether the labels are correct\\nfor individual inputs); (2) specifying the overall\\nformat is also crucial, e.g., when the label space\\nis unknown, using random English words as la-\\nbels is signiﬁcantly better than using no labels; and\\narXiv:2202.12837v2  [cs.CL]  20 Oct 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='(3) meta-training with an in-context learning objec-\\ntive (Min et al., 2021b) magniﬁes these effects—the\\nmodels almost exclusively exploit simpler aspects\\nof the demonstrations like the format rather than\\nthe input-label mapping.\\nIn summary, our analysis provides a new way\\nof understanding the role of the demonstrations in\\nin-context learning. We empirically show that the\\nmodel (1) counter-intuitively does not rely on the\\nground truth input-label mapping provided in the\\ndemonstrations as much as we thought (Section 4),\\nand (2) nonetheless still beneﬁts from knowing the\\nlabel space and the distribution of inputs speciﬁed\\nby the demonstrations (Section 5). We also include\\na discussion of broader implications, e.g., what we\\ncan say about the model learning at test time, and\\navenues for future work (Section 6).\\n2 Related Work\\nLarge language models have been key to strong per-\\nformance in a wide range of downstream tasks (De-\\nvlin et al., 2019; Radford et al., 2019; Liu et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='2 Related Work\\nLarge language models have been key to strong per-\\nformance in a wide range of downstream tasks (De-\\nvlin et al., 2019; Radford et al., 2019; Liu et al.,\\n2019; Raffel et al., 2020; Lewis et al., 2020). While\\nﬁnetuning has been a popular approach to transfer\\nto new tasks (Devlin et al., 2019), it is often imprac-\\ntical to ﬁnetune a very large model (e.g. ≥10B pa-\\nrameters). Brown et al. (2020) propose in-context\\nlearning as an alternative way to learn a new task.\\nAs depicted in Figure 2, the LM learns a new task\\nvia inference alone by conditioning on a concatena-\\ntion of the training data as demonstrations, without\\nany gradient updates.\\nIn-context learning has been the focus of signif-\\nicant study since its introduction. Prior work pro-\\nposes better ways of formulating the problem (Zhao\\net al., 2021; Holtzman et al., 2021; Min et al.,\\n2021a), better ways of choosing labeled exam-\\nples for the demonstrations (Liu et al., 2021; Lu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='poses better ways of formulating the problem (Zhao\\net al., 2021; Holtzman et al., 2021; Min et al.,\\n2021a), better ways of choosing labeled exam-\\nples for the demonstrations (Liu et al., 2021; Lu\\net al., 2021; Rubin et al., 2021), meta-training\\nwith an explicit in-context learning objective (Chen\\net al., 2021; Min et al., 2021b), and learning to\\nfollow instructions as a variant of in-context learn-\\ning (Mishra et al., 2021b; Efrat and Levy, 2020;\\nWei et al., 2022a; Sanh et al., 2022). At the\\nsame time, some work reports brittleness and over-\\nsensitivity for in-context learning (Lu et al., 2021;\\nZhao et al., 2021; Mishra et al., 2021a).\\nRelatively less work has been done to understand\\nwhy in-context learning works. Xie et al. (2022)\\nprovide theoretical analysis that in-context learn-\\ning can be formalized as Bayesian inference that\\nCirculation revenue has increased by 5% in Finland.         \\\\n    Positive \\nPanostaja did not disclose the purchase price.                  \\\\n    Neutral'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Circulation revenue has increased by 5% in Finland.         \\\\n    Positive \\nPanostaja did not disclose the purchase price.                  \\\\n    Neutral \\nPaying off the national debt will be extremely painful.      \\\\n    Negative \\nThe acquisition will have an immediate positive impact.  \\\\n    ________\\n =\\nDemonstrations\\nLM\\nPositive\\nTest input\\nPrediction\\nFigure 2: An overview of in-context learning. The\\ndemonstrations consist of k input-label pairs from the\\ntraining data (k = 3in the ﬁgure).\\nModel # Params Public Meta-trained\\nGPT-2 Large 774M \\x13 \\x17\\nMetaICL 774M \\x13 \\x13\\nGPT-J 6B \\x13 \\x17\\nfairseq 6.7B† 6.7B \\x13 \\x17\\nfairseq 13B† 13B \\x13 \\x17\\nGPT-3 175B ‡ \\x17 \\x17\\nTable 1: A list of LMs used in the experiments:\\nGPT-2 (Radford et al., 2019), MetaICL (Min et al.,\\n2021b), GPT-J (Wang and Komatsuzaki, 2021), fairseq\\nLMs (Artetxe et al., 2021) and GPT-3 (Brown et al.,\\n2020). ‘Public’ indicates whether the model weights\\nare public; ‘Meta-trained’ indicates whether the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='LMs (Artetxe et al., 2021) and GPT-3 (Brown et al.,\\n2020). ‘Public’ indicates whether the model weights\\nare public; ‘Meta-trained’ indicates whether the model\\nis meta-trained with an in-context learning objective.\\n†We use dense models in Artetxe et al. (2021) and re-\\nfer them as fairseq LMs for convenience. ‡We use the\\nDavinci API (the base version, not the instruct version)\\nand assume it to be 175B, following Gao et al. (2021)\\nand Artetxe et al. (2021).\\nuses the demonstrations to recover latent concepts.\\nRazeghi et al. (2022) show that in-context learn-\\ning performance is highly correlated with term fre-\\nquencies in the pretraining data. To the best of our\\nknowledge, this paper is the ﬁrst that provides an\\nempirical analysis that investigates why in-context\\nlearning achieves performance gains over zero-shot\\ninference. We ﬁnd that the ground truth input-label\\nmapping in the demonstrations has only a marginal\\neffect, and measure the impact of ﬁner-grained as-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='inference. We ﬁnd that the ground truth input-label\\nmapping in the demonstrations has only a marginal\\neffect, and measure the impact of ﬁner-grained as-\\npects of the demonstrations.\\n3 Experimental Setup\\nWe describe the experimental setup used in our\\nanalysis (Section 4 and 5).\\nModels. We experiment with 12 models in to-\\ntal. We include 6 language models (Table 1), all\\nof which are decoder-only, dense LMs. We use\\neach LM with two inference methods, direct and\\nchannel, following Min et al. (2021a). The sizes\\nof LMs vary from 774M to 175B. We include the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nDirect\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 3: Results when using no-demonstrations, demonstrations with gold labels, and demonstrations with ran-\\ndom labels in classiﬁcation (top) and multi-choice tasks (bottom). The ﬁrst eight models are evaluated on 16\\nclassiﬁcation and 10 multi-choice datasets, and the last four models are evaluated on 3 classiﬁcation and 3 multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='classiﬁcation and 10 multi-choice datasets, and the last four models are evaluated on 3 classiﬁcation and 3 multi-\\nchoice datasets. See Figure 11 for numbers comparable across all models. Model performance with random\\nlabels is very close to performance with gold labels (more discussion in Section 4.1).\\nlargest dense LM (GPT-3) and the largest publicly\\nreleased dense LM (fairseq 13B) at the time of con-\\nducting experiments. We also include MetaICL,\\nwhich is initialized from GPT-2 Large and then\\nmeta-trained on a collection of supervised datasets\\nwith an in-context learning objective, and ensure\\nthat our evaluation datasets do not overlap with\\nthose used at meta-training time.\\nEvaluation Data. We evaluate on 26 datasets,\\nincluding sentiment analysis, paraphrase detection,\\nnatural language inference, hate speech detection,\\nquestion answering, and sentence completion (full\\nlist and references provided in Appendix A).1 All\\ndatasets are classiﬁcation and multi-choice tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='question answering, and sentence completion (full\\nlist and references provided in Appendix A).1 All\\ndatasets are classiﬁcation and multi-choice tasks.\\nWe use these datasets because they (1) are true\\nlow-resource datasets with less than 10K train-\\ning examples, (2) include well-studied bench-\\nmarks from GLUE (Wang et al., 2018) and Super-\\nGLUE (Wang et al., 2019a), and (3) cover diverse\\ndomains including science, social media, ﬁnance,\\nand more.\\nOther Details. We use k = 16 examples as\\ndemonstrations by default for all experiments in\\nthe paper, unless otherwise speciﬁed. Examples\\nare sampled at uniform from the training data.\\nWe choose a set of k training examples using\\n5 different random seeds and run experiments 5\\ntimes. For fairseq 13B and GPT-3, due to lim-\\nited resources, we experiment with a subset of 6\\n1For convenience, we use ‘labels’ to refer to the output for\\nthe task, though our datasets include non-classiﬁcation tasks.\\ndatasets2 and 3 random seeds. We report Macro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='1For convenience, we use ‘labels’ to refer to the output for\\nthe task, though our datasets include non-classiﬁcation tasks.\\ndatasets2 and 3 random seeds. We report Macro-\\nF13 for classiﬁcation tasks and Accuracy for multi-\\nchoice tasks. We compute per-dataset average over\\nseeds, and then report macro-average over datasets.\\nWe use the minimal templates in forming an in-\\nput sequence from an example. We refer to Ap-\\npendix B for more details. All experiments are\\nreproducible from github.com/Alrope123/\\nrethinking-demonstrations.\\n4 Ground Truth Matters Little\\n4.1 Gold labels vs. random labels\\nTo see the impact of correctly-paired inputs and\\nlabels in the demonstrations—which we call the\\nground truth input-label mapping—we compare the\\nfollowing three methods.4\\nNo demonstrations is a typical zero-shot method\\nthat does not use any labeled data. A prediction\\nis made via argmaxy∈CP(y|x), where x is the test\\ninput and Cis a small discrete set of possible labels.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='that does not use any labeled data. A prediction\\nis made via argmaxy∈CP(y|x), where x is the test\\ninput and Cis a small discrete set of possible labels.\\nDemonstrations w/ gold labels are used in a typi-\\ncal in-context learning method with k labeled ex-\\namples (x1, y1)...(xk, yk). A concatenation of k\\ninput-label pairs is used to make a prediction via\\nargmaxy∈CP(y|x1, y1...xk, yk, x).\\n2Three classiﬁcation and three multi-choice: MRPC, RTE,\\nTweet_eval-hate, OpenbookQA, CommonsenseQA, COPA.\\n3Known to be better for imbalanced classes.\\n4Without loss of generality, all methods in Section 4 and 5\\nare described based on the direct method, but can be trivially\\nconverted to the channel method by ﬂipping x and y.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (Classification) GPT-J (Classification) MetaICL (Multi-choice) GPT-J (Multi-choice)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Accuracy (%)\\n100% correct 75% correct 50% correct 25% correct 0% correct No Demos\\nFigure 4: Results with varying number of correct labels in the demonstrations. Channel and Direct used for\\nclassiﬁcation and multi-choice, respectively. Performance with no demonstrations (blue) is reported as a reference.\\nDemonstrations w/ random labels are formed\\nwith random labels, instead of gold labels from\\nthe labeled data. Each xi (1 ≤ i ≤\\nk) is paired with ˜yi that is randomly sam-\\npled at uniform from C. A concatenation of\\n(x1, ˜y1)...(xk, ˜yk) is then used to make a predic-\\ntion via argmaxy∈CP(y|x1, ˜y1...xk, ˜yk, x).\\nResults are reported in Figure 3. First, using the\\ndemonstrations with gold labels signiﬁcantly im-\\nproves the performance over no demonstrations,5\\nas it has been consistently found in much of prior\\nwork (Brown et al., 2020; Zhao et al., 2021; Liu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='demonstrations with gold labels signiﬁcantly im-\\nproves the performance over no demonstrations,5\\nas it has been consistently found in much of prior\\nwork (Brown et al., 2020; Zhao et al., 2021; Liu\\net al., 2021). We then ﬁnd that replacing gold la-\\nbels with random labels only marginally hurts\\nperformance. The trend is consistent over nearly\\nall models: models see performance drop in the\\nrange of 0–5% absolute. There is less impact in\\nreplacing labels in multi-choice tasks (1.7% on av-\\nerage) than in classiﬁcation tasks (2.6% absolute).\\nThis result indicates that the ground truth input-\\nlabel pairs are not necessary to achieve perfor-\\nmance gains. This is counter-intuitive, given that\\ncorrectly paired training data is critical in typical\\nsupervised training—it informs the model of the ex-\\npected input-label correspondence required to per-\\nform the downstream task. Nonetheless, the mod-\\nels do achieve non-trivial performance on the down-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='pected input-label correspondence required to per-\\nform the downstream task. Nonetheless, the mod-\\nels do achieve non-trivial performance on the down-\\nstream tasks. This strongly suggests that the mod-\\nels are capable of recovering the expected input-\\nlabel correspondence for the task; however, it isnot\\ndirectly from the pairings in the demonstrations.\\nIt is also worth noting that there is particularly\\nlittle performance drop in MetaICL: 0.1–0.9% ab-\\nsolute. This suggests that meta-training with an\\nexplicit in-context learning objective actually en-\\ncourages the model to essentially ignore the input-\\n5There are some exceptions, e.g., in the classiﬁcation tasks,\\nDirect GPT-2, Direct GPT-J and Direct fairseq 6.7B models\\nare not signiﬁcantly better than random guessing on many\\ndatasets; Channel fairseq 13B has signiﬁcantly better no-\\ndemonstrations performance compared to demonstrations with\\ngold labels. We thus discuss the results from these models less'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='datasets; Channel fairseq 13B has signiﬁcantly better no-\\ndemonstrations performance compared to demonstrations with\\ngold labels. We thus discuss the results from these models less\\nsigniﬁcantly for the rest of analysis.\\n0 4 8 16 32\\nk\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDemos w/ gold\\nDemos w/ random\\n0 4 8 16 32\\nk\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\nDemos w/ gold\\nDemos w/ random\\nFigure 5: Ablations on varying numbers of examples\\nin the demonstrations (k). Models that are the best un-\\nder 13B in each task category (Channel MetaICL and\\nDirect GPT-J, respectively) are used.\\nlabel mapping and exploit other components of the\\ndemonstrations (more discussion in Section 5.4).\\nIn Appendix C.2, we provide additional results\\nshowing that (1) selecting random labels from a\\ntrue distribution of labels (instead of a uniform\\ndistribution) reduces the gap even further, and (2)\\nthe trends may depend on the dataset, although the\\noverall trend is consistent over most datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='distribution) reduces the gap even further, and (2)\\nthe trends may depend on the dataset, although the\\noverall trend is consistent over most datasets.\\n4.2 Ablations\\nFor additional ablations, we experiment with 5 clas-\\nsiﬁcation and 4 multi-choice datasets.6\\nDoes the number of correct labels matter? To\\nfurther examine the impact of correctness of la-\\nbels in the demonstrations, we conduct an ablation\\nstudy by varying the number of correct labels in the\\ndemonstrations. We evaluate “Demonstrations w/\\na% correct labels” (0 ≤a ≤100) which consist\\nof k ×a/100 correct pairs and k ×(1 −a/100)\\nincorrect pairs (see Algorithm 1 in Appendix B).\\nHere, a = 100 is the same as typical in-context\\nlearning, i.e., demonstrations w/ gold labels.\\nResults are reported in Figure 4. Model perfor-\\nmance is fairly insensitive to the number of correct\\nlabels in the demonstrations. In fact, always us-\\ning incorrect labels signiﬁcantly outperforms no-\\n6Classiﬁcation includes: MRPC, RTE, Tweet_eval-hate,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='labels in the demonstrations. In fact, always us-\\ning incorrect labels signiﬁcantly outperforms no-\\n6Classiﬁcation includes: MRPC, RTE, Tweet_eval-hate,\\nSICK, poem-sentiment; Multi-choice includes OpenbookQA,\\nCommonsenseQA, COPA and ARC.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='MetaICL (Classification) GPT-J (Classification) MetaICL (Multi-choice) GPT-J (Multi-choice)\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65Accuracy (%)\\nNo demos Gold labels Random labels No demos + T Gold labels + T Random labels + T\\nFigure 6: Results with minimal templates and manual templates. ‘+T’ indicates that manual templates are used.\\nChannel and Direct used for classiﬁcation and multi-choice, respectively.\\ndemonstrations, e.g., preserving 92%, 100% and\\n97% of improvements from using the demonstra-\\ntions with MetaICL in classiﬁcation, MetaICL in\\nmulti-choice, and GPT-J in multi-choice, respec-\\ntively. In contrast, GPT-J in classiﬁcation sees\\nrelatively signiﬁcant performance drop with more\\nincorrect labels, e.g., nearly 10% drop in perfor-\\nmance when always using incorrect labels. Still,\\nalways using incorrect labels is signiﬁcantly better\\nthan no demonstrations.\\nIs the result consistent with varying k? We\\nstudy the impact of the number of input-label pairs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='always using incorrect labels is signiﬁcantly better\\nthan no demonstrations.\\nIs the result consistent with varying k? We\\nstudy the impact of the number of input-label pairs\\n(k) in the demonstrations. Results are reported in\\nFigure 5. First, using the demonstrations signiﬁ-\\ncantly outperforms the no demonstrations method\\neven with small k (k = 4), and performance drop\\nfrom using gold labels to using random labels is\\nconsistently small across varying k, in the range of\\n0.8–1.6%.7 Interestingly, model performance does\\nnot increase much as k increases when k ≥8, both\\nwith gold labels and with random labels. This is\\nin contrast with typical supervised training where\\nmodel performance rapidly increases ask increases,\\nespecially when k is small. We hypothesize that\\nlarger labeled data is beneﬁcial mainly for super-\\nvising the input-label correspondence, and other\\ncomponents of the data like the example inputs,\\nexample labels and the data format are easier to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='larger labeled data is beneﬁcial mainly for super-\\nvising the input-label correspondence, and other\\ncomponents of the data like the example inputs,\\nexample labels and the data format are easier to\\nrecover from the small data, which is potentially a\\nreason for minimal performance gains from larger\\nk (more discussion in Section 5).\\nIs the result consistent with better templates?\\nWhile we use minimal templates by default, we\\nalso explore manual templates, i.e., templates that\\nare manually written in a dataset-speciﬁc manner,\\ntaken from prior work (details in Appendix B). Fig-\\nure 6 shows that the trend—replacing gold labels\\nwith random labels barely hurting performance—\\nholds with manual templates. It is worth noting\\n7With an exception of 4.4% in classiﬁcation with k = 4,\\nlikely due to a high variance with a very small value of k.\\nCirculation revenue has increased by 5% in Finland.         \\\\n         Positive\\nFormat \\n(The use \\nof pairs)\\n =\\nDistribution of inputs Label spaceDemonstrations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Circulation revenue has increased by 5% in Finland.         \\\\n         Positive\\nFormat \\n(The use \\nof pairs)\\n =\\nDistribution of inputs Label spaceDemonstrations\\nTest example Input-label mapping\\nPanostaja did not disclose the purchase price.                  \\\\n         Neutral\\nPaying off the national debt will be extremely painful.      \\\\n         Negative\\nThe acquisition will have an immediate positive impact.  \\\\n         ?\\nFigure 7: Four different aspects in the demonstrations:\\nthe input-label mapping, the distribution of the input\\ntext, the label space, and the use of input-label pairing\\nas the format of the demonstrations.\\nthat using manual templates does not always out-\\nperform using minimal templates.\\n5 Why does In-Context Learning work?\\nSection 4 shows that the ground truth input-label\\nmapping in the demonstrations has little impact to\\nperformance gains from in-context learning. This\\nsection further examines what other aspects of the\\ndemonstrations lead to good performance of in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='mapping in the demonstrations has little impact to\\nperformance gains from in-context learning. This\\nsection further examines what other aspects of the\\ndemonstrations lead to good performance of in-\\ncontext learning.\\nWe identify four aspects of the demonstrations\\n(x1, y1)...(xk, yk) that potentially provide learning\\nsignal (depicted in Figure 7).\\n1. The input-label mapping, i.e., whether each\\ninput xi is paired with a correct label yi.\\n2. The distribution of the input text , i.e., the\\nunderlying distribution that x1...xk are from.\\n3. The label space , i.e., the space covered by\\ny1...yk.\\n4. The format—speciﬁcally, the use of input-\\nlabel pairing as the format.\\nAs Section 4 does for the input-label mapping,\\nwe design a series of variants of the demonstrations\\nthat quantify the impact of each aspect in isolation\\n(Section 5.1–5.3). We then additionally discuss the\\ntrend of the models meta-trained with an in-context\\nlearning objective (Section 5.4). For all experi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='(Section 5.1–5.3). We then additionally discuss the\\ntrend of the models meta-trained with an in-context\\nlearning objective (Section 5.4). For all experi-\\nments, models are evaluated on ﬁve classiﬁcation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nFigure 8: Impact of the distribution of the inputs. Evaluated in classiﬁcation (top) and multi-choice (bottom). The\\nimpact of the distribution of the input text can be measured by comparing■ and ■. The gap is substantial, with an\\nexception in Direct MetaICL (discussion in Section 5.1).\\nand four multi-choice datasets as in Section 4.2.\\nSee Appendix B and Table 4 for implementation\\ndetails and example demonstrations, respectively.\\n5.1 Impact of the distribution of the input\\ntext\\nWe experiment with OOD demonstrations which\\ninclude out-of-distribution (OOD) text instead of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='details and example demonstrations, respectively.\\n5.1 Impact of the distribution of the input\\ntext\\nWe experiment with OOD demonstrations which\\ninclude out-of-distribution (OOD) text instead of\\nthe inputs from unlabeled training data. Specif-\\nically, a set of k sentences {xi,rand}k\\ni=1 are ran-\\ndomly sampled from an external corpus, and re-\\nplace x1...xk in the demonstrations. This variant\\nassesses the impact of the distribution of the input\\ntext, while keeping the label space and the format\\nof the demonstrations.\\nResults. Figure 8 shows that using out-of-\\ndistribution inputs instead of the inputs from the\\ntraining data signiﬁcantly drops the performance\\nwhen Channel MetaICL, Direct GPT-J or Channel\\nGPT-J are used, both in classiﬁcation and multi-\\nchoice, by 3–16% in absolute. In the case of Di-\\nrect GPT-J in multi-choice, it is even signiﬁcantly\\nworse than no demonstrations. Direct MetaICL\\nis an exception, which we think is the effect of\\nmeta-training (discussion in Section 5.4).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='rect GPT-J in multi-choice, it is even signiﬁcantly\\nworse than no demonstrations. Direct MetaICL\\nis an exception, which we think is the effect of\\nmeta-training (discussion in Section 5.4).\\nThis suggests that in-distribution inputs in the\\ndemonstrations substantially contribute to perfor-\\nmance gains. This is likely because conditioning on\\nthe in-distribution text makes the task closer to lan-\\nguage modeling, since the LM always conditioned\\non the in-distribution text during training.\\n5.2 Impact of the label space\\nWe also experiment with demonstrations w/ ran-\\ndom English words that use random English\\nwords as labels for all k pairs. Speciﬁcally, we\\nsample a random subset of English words Crand\\nwhere |Crand|= |C|, and randomly pair ˜yi ∈Crand\\nwith xi. This variant assesses the impact of the\\nlabel space, while keeping the distribution of the\\ninput text and the format of the demonstrations.\\nResults. Based on Figure 9, direct models and\\nchannel models exhibit different patterns. With di-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='label space, while keeping the distribution of the\\ninput text and the format of the demonstrations.\\nResults. Based on Figure 9, direct models and\\nchannel models exhibit different patterns. With di-\\nrect models, the performance gap between using\\nrandom labels within the label space and using ran-\\ndom English words is signiﬁcant, ranging between\\n5–16% absolute. This indicates that conditioning\\non the label space signiﬁcantly contributes to per-\\nformance gains. This is true even for multi-choice\\ntasks where there is no ﬁxed set of labels—we\\nhypothesize that multi-choice tasks still do have\\na particular distribution of the choices (e.g., ob-\\njects like “Bolts” or “Screws” in the OpenBookQA\\ndataset) that the model uses.\\nOn the other hand, removing the output space\\ndoes not lead to signiﬁcant drop in the channel\\nmodels: there is 0–2% drop in absolute, or some-\\ntimes even an increase. We hypothesize that this is\\nbecause the channel models only condition on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='does not lead to signiﬁcant drop in the channel\\nmodels: there is 0–2% drop in absolute, or some-\\ntimes even an increase. We hypothesize that this is\\nbecause the channel models only condition on the\\nlabels, and thus are not beneﬁting from knowing\\nthe label space. This is in contrast to direct models\\nwhich must generate the correct labels.\\n5.3 Impact of input-label pairing\\nSection 5.1 and 5.2 focus on variants which keep\\nthe format of the demonstrations as much as possi-\\nble. This section explores variants that change the\\nformat. While there are many aspects of the format,\\nwe make minimal modiﬁcations to remove the pair-\\nings of inputs to labels. Speciﬁcally, we evaluate\\ndemonstrations with no labels where the LM is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ Random English words\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nFigure 9: Impact of the label space. Evaluated in classiﬁcation (top) and multi-choice (bottom). The impact of\\nthe label space can be measured by comparing ■ and ■. The gap is signiﬁcant in the direct models but not in the\\nchannel models (discussion in Section 5.2).\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nDirect MetaICL Channel MetaICL Direct GPT-J Channel GPT-J\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ Random labels only\\n■ Random English words\\n■ No labels'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Accuracy (%)\\nMulti-choice\\n■ Gold labels\\n■ Random labels\\n■ OOD + Random labels\\n■ Random labels only\\n■ Random English words\\n■ No labels\\n■ No demonstrations\\nF: Format\\nL: Label space\\nI: Input distribution\\nM: Input-Label Mapping\\nF\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x13\\n\\x17\\n\\x17\\nL\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\nI\\n\\x13\\n\\x13\\n\\x17\\n\\x17\\n\\x13\\n\\x13\\n\\x17\\nM\\n\\x13\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\n\\x17\\nFigure 10: Impact of the format, i.e., the use of the input-label pairs. Evaluated in classiﬁcation (top) and multi-\\nchoice (bottom). Variants of demonstrations without keeping the format ( ■ and ■) are overall not better than no\\ndemonstrations (■). Keeping the format is especially signiﬁcant when it is possible to achieve substantial gains\\nwith the label space but without the inputs (■ vs. ■ in Direct MetaICL), or with the input distribution but without\\nthe labels (■ vs. ■ in Channel MetaICL and Channel GPT-J). More discussion in Section 5.3.\\nconditioned on the concatenation of x1...xk, and\\ndemonstrations with labels onlywhere the LM is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='the labels (■ vs. ■ in Channel MetaICL and Channel GPT-J). More discussion in Section 5.3.\\nconditioned on the concatenation of x1...xk, and\\ndemonstrations with labels onlywhere the LM is\\nconditioned on the concatenation of y1...yk. These\\nablations provide the no-format counterparts of the\\n‘demonstrations with random English words’ and\\n‘demonstrations with OOD inputs’, respectively.\\nResults. Based on Figure 10, removing the for-\\nmat is close to or worse than no demonstrations,\\nindicating the importance of the format. This is\\nlikely because conditioning on a sequence of input-\\nlabel pairs triggers the model to mimic the overall\\nformat and complete the new example as expected\\nwhen the test input is given.\\nMore interestingly, keeping the format plays a\\nsigniﬁcant role in retaining a large portion of per-\\nformance gains by only using the inputs or only\\nusing the labels. For instance, with Direct MetaICL,\\nit is possible to retain 95% and 82% of improve-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='formance gains by only using the inputs or only\\nusing the labels. For instance, with Direct MetaICL,\\nit is possible to retain 95% and 82% of improve-\\nments from in-context learning (demonstrations\\nwith gold labels) by simply sampling random sen-\\ntences from a corpus and randomly pairing them\\nwith the label set (■ in Figure 10) in classiﬁcation\\nand multi-choice, respectively. Similarly, with the\\nchannel models, it is possible to retain 82%, 87%,\\n86% and 75% of improvements from in-context\\nlearning by simply pairing each input from the un-\\nlabeled training data with a random English word\\n(■ in Figure 10) in MetaICL classiﬁcation, GPT-\\nJ classiﬁcation, MetaICL multi-choice and GPT-J\\nmulti-choice, respectively. For all of these cases,\\nremoving inputs instead of using OOD inputs, or\\nremoving labels instead of using random English\\nwords is signiﬁcantly worse, indicating that keep-\\ning the format of the input-label pairs is key.\\n5.4 Impact of meta-training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='removing labels instead of using random English\\nwords is signiﬁcantly worse, indicating that keep-\\ning the format of the input-label pairs is key.\\n5.4 Impact of meta-training\\nDifferent from other models, MetaICL is trained\\nwith an in-context learning objective, in line with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='recent work that uses multi-task training on a\\nlarge collection of supervised datasets (called meta-\\ntraining) for generalization to new tasks (Agha-\\njanyan et al., 2021; Khashabi et al., 2020; Wei\\net al., 2022a; Sanh et al., 2022). We aim to better\\nunderstand the role of this meta-training in relation\\nwith our ﬁndings by closely examining the result of\\nMetaICL. In particular, we observe that the patterns\\nwe see so far are signiﬁcantly more evident with\\nMetaICL than with other models. For instance, the\\nground truth input-label mapping matters even less,\\nand keeping the format of the demonstrations mat-\\nters even more. There is nearly zero inﬂuence of\\nthe input-label mapping and the input distribution\\nin Direct MetaICL, and the input-label mapping\\nand the output space in Channel MetaICL.\\nBased on this observation, we hypothesize that\\nmeta-training encourages the model to exclu-\\nsively exploit simpler aspects of the demonstra-\\ntions and to ignore others. This is based on our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Based on this observation, we hypothesize that\\nmeta-training encourages the model to exclu-\\nsively exploit simpler aspects of the demonstra-\\ntions and to ignore others. This is based on our\\nintuition that (1) the input-label mapping is likely\\nharder to exploit, (2) the format is likely easier to\\nexploit, and (3) the space of the text that the model\\nis trained to generate is likely easier to exploit than\\nthe space of the text that the model conditions on.8\\n6 Discussion & Conclusion\\nIn this paper, we study the role of the demonstra-\\ntions with respect to the success of in-context learn-\\ning. We ﬁnd that the ground truth input-label map-\\nping in the demonstrations matters signiﬁcantly\\nless than one might think—replacing gold labels\\nwith random labels in the demonstrations only\\nmarginally lowers the performance. We then iden-\\ntify a series of aspects in the demonstrations and\\nexamine which aspect actually contributes to per-\\nformance gains. Results reveal that (1) gains are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='marginally lowers the performance. We then iden-\\ntify a series of aspects in the demonstrations and\\nexamine which aspect actually contributes to per-\\nformance gains. Results reveal that (1) gains are\\nmainly coming from independent speciﬁcation of\\nthe input space and the label space, (2) the models\\ncan still retain up to 95% of performance gains by\\nusing either the inputs only or the label set only if\\nthe right format is used, and (3) meta-training with\\nan in-context learning objective magniﬁes these\\ntrends. Together, our ﬁndings lead to a set of\\nbroader indications about in-context learning, as\\nwell as avenues for future work.\\nDoes the model learn at test time? If we take\\na strict deﬁnition of learning: capturing the input-\\n8That is, the direct model exploits the label space better\\nthan the input distribution, and the channel model exploits the\\ninput distribution better than the label space.\\nlabel correspondence given in the training data,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='than the input distribution, and the channel model exploits the\\ninput distribution better than the label space.\\nlabel correspondence given in the training data,\\nthen our ﬁndings suggest that LMs do not learn\\nnew tasks at test time. Our analysis shows that the\\nmodel may ignore the task deﬁned by the demon-\\nstrations and instead use prior from pretraining.\\nHowever, learning a new task can be interpreted\\nmore broadly: it may include adapting to speciﬁc\\ninput and label distributions and the format sug-\\ngested by the demonstrations, and ultimately get-\\nting to make a prediction more accurately. With\\nthis deﬁnition of learning, the model does learn\\nthe task from the demonstrations. Our experiments\\nindicate that the model does make use of aspects of\\nthe demonstrations and achieve performance gains.\\nCapacity of LMs. The model performs a down-\\nstream task without relying on the input-label corre-\\nspondence from the demonstrations. This suggests'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='the demonstrations and achieve performance gains.\\nCapacity of LMs. The model performs a down-\\nstream task without relying on the input-label corre-\\nspondence from the demonstrations. This suggests\\nthat the model has learned the (implicit notion of)\\ninput-label correspondence from the language mod-\\neling objective alone, e.g., associating a positive\\nreview with the word ‘positive’. This is in line\\nwith Reynolds and McDonell (2021) who claim\\nthat the demonstrations are for task location and\\nthe intrinsic ability to perform the task is obtained\\nat pretraining time.9\\nOn one hand, this suggests that the language\\nmodeling objective has led to great zero-shot ca-\\npacity, even if it is not always evident from the\\nnaive zero-shot accuracy. On the other hand, this\\nsuggests that in-context learning may not work on\\na task whose input-label correspondence is not al-\\nready captured in the LM. This leads to the research\\nquestion of how to make progress in NLP problems'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='a task whose input-label correspondence is not al-\\nready captured in the LM. This leads to the research\\nquestion of how to make progress in NLP problems\\nthat in-context learning does not solve: whether\\nwe need a better way of extracting the input-label\\nmappings that are already stored in the LM, a bet-\\nter variant of the LM objective that learns a wider\\nrange of task semantics, or explicit supervision\\nthrough ﬁne-tuning on the labeled data.\\nConnection to instruction-following models.\\nPrior work has found it promising to train the model\\nthat reads the natural language description of the\\ntask (called instructions) and performs a new task\\nat inference (Mishra et al., 2021b; Efrat and Levy,\\n2020; Wei et al., 2022a; Sanh et al., 2022). We\\nthink the demonstrations and instructions largely\\nhave the same role to LMs, and hypothesize that our\\n9However, while Reynolds and McDonell (2021) claims\\nthat the demonstrations are thus unnecessary, we think using'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='have the same role to LMs, and hypothesize that our\\n9However, while Reynolds and McDonell (2021) claims\\nthat the demonstrations are thus unnecessary, we think using\\nthe demonstrations is actually the most unambiguous and the\\neasiest way to prompt the model to perform a task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='ﬁndings hold for instruction-following models: the\\ninstructions prompt the model to recover the capac-\\nity it already has, but do not supervise the model to\\nlearn novel task semantics. This has been partially\\nveriﬁed by Webson and Pavlick (2022) who showed\\nthat the model performance does not degrade much\\nwith irrelevant or misleading instructions. We leave\\nmore analysis on instruction-following models for\\nfuture work.\\nSigniﬁcantly improved zero-shot performance.\\nOne of our key ﬁndings is that it is possible to\\nachieve nearly k-shot performance without using\\nany labeled data, by simply pairing each unlabeled\\ninput with a random label and using it as the demon-\\nstrations. This means our zero-shot baseline level\\nis signiﬁcantly higher than previously thought. 10\\nFuture work can further improve the zero-shot per-\\nformance with relaxed assumptions in access to the\\nunlabeled training data.\\nLimitation\\nEffect of types of tasks and datasets. This pa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Future work can further improve the zero-shot per-\\nformance with relaxed assumptions in access to the\\nunlabeled training data.\\nLimitation\\nEffect of types of tasks and datasets. This pa-\\nper focuses on the tasks from established NLP\\nbenchmarks that have real natural language inputs.\\nSynthetic tasks with more limited inputs may actu-\\nally use the ground truth labels more, as observed\\nby Rong (2021).\\nWe report macro-level analysis by examining the\\naverage performance over multiple NLP datasets,\\nbut different datasets may behave differently. Ap-\\npendix C.2 discusses this aspect, including ﬁnd-\\nings that there are larger gaps between using the\\nground truth labels and using the random labels\\nin some dataset-model pairs (e.g., in the most\\nextreme case, nearly 14% absolute on the ﬁnan-\\ncial_phrasebank dataset with GPT-J). Since the ﬁrst\\nversion of our paper, Kim et al. (2022) showed\\nthat using negated labels substantially lowers the\\nperformance in classiﬁcation. 11 We believe it is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='cial_phrasebank dataset with GPT-J). Since the ﬁrst\\nversion of our paper, Kim et al. (2022) showed\\nthat using negated labels substantially lowers the\\nperformance in classiﬁcation. 11 We believe it is\\nimportant to understand to what extend the model\\nneeds the ground truth labels to successfully per-\\nform in-context learning.\\nExtensions to generation. Our experiments are\\nlimited to classiﬁcation and multi-choice tasks. We\\nhypothesize that ground truth output may not be\\n10We take the perspective that using the unlabeled training\\ndata is permitted (Kodirov et al., 2015; Wang et al., 2019b;\\nSchick and Schütze, 2021).\\n11Note that Kim et al. (2022) estimate the random label per-\\nformance by interpolating with the performance using negated\\nlabels, while our paper samples the random labels at uniform.\\nnecessary for in-context learning in the open-set\\ntasks such as generation, but leave this to future\\nwork. Extending of our experiments to such tasks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='necessary for in-context learning in the open-set\\ntasks such as generation, but leave this to future\\nwork. Extending of our experiments to such tasks\\nis not trivial, because it requires a variation of the\\noutput which has incorrect input-output correspon-\\ndence while keeping the correct output distribution\\n(which is important based on our analysis in Sec-\\ntion 5).\\nSince the ﬁrst version of our paper, Madaan and\\nYazdanbakhsh (2022) conducted a similar analy-\\nsis with the chain of thought prompting (Wei et al.,\\n2022b) which generates a rationale to perform com-\\nplex tasks such as math problems. Madaan and\\nYazdanbakhsh (2022) show that, while simply us-\\ning a random rationale in the demonstrations (e.g.,\\npairing with a rationale from a different example)\\nsigniﬁcantly degrades the performance, other types\\nof counterfactual rationales (e.g., wrong equations)\\ndo not degrade the performance as much as we\\nthought. We refer to Madaan and Yazdanbakhsh'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='signiﬁcantly degrades the performance, other types\\nof counterfactual rationales (e.g., wrong equations)\\ndo not degrade the performance as much as we\\nthought. We refer to Madaan and Yazdanbakhsh\\n(2022) for more discussions on what aspects of the\\nrationale matter or do not matter.\\nAcknowledgements\\nWe thank Gabriel Ilharco, Julian Michael, Oﬁr\\nPress, UW NLP members and anonymous review-\\ners for their comments in the paper. This research\\nwas supported by NSF IIS-2044660, ONR N00014-\\n18-1-2826, a Sloan fellowship and gifts from AI2.\\nReferences\\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivas-\\ntava, Xilun Chen, Luke Zettlemoyer, and Sonal\\nGupta. 2021. Muppet: Massive multi-task rep-\\nresentations with pre-ﬁnetuning. arXiv preprint\\narXiv:2101.11038.\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\\net al. 2021. Efﬁcient large scale language mod-\\neling with mixtures of experts. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\\net al. 2021. Efﬁcient large scale language mod-\\neling with mixtures of experts. arXiv preprint\\narXiv:2112.10684.\\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\\nDanilo Giampiccolo, Bernardo Magnini, and Idan\\nSzpektor. 2006. The second pascal recognising tex-\\ntual entailment challenge. In Proceedings of the sec-\\nond PASCAL challenges workshop on recognising\\ntextual entailment.\\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\\npinosa Anke, and Leonardo Neves. 2020. TweetE-\\nval: Uniﬁed benchmark and comparative evaluation\\nfor tweet classiﬁcation. In Findings of EMNLP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\\nGiampiccolo. 2009. The ﬁfth pascal recognizing tex-\\ntual entailment challenge. In TAC.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\\nV oss, Gretchen Krueger, Tom Henighan, Rewon\\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\\nClemens Winter, Chris Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In\\nNeurIPS.\\nMichael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-\\nnandez, and Doug Downey. 2019. CODAH: An\\nadversarially-authored question answering dataset\\nfor common sense. In Proceedings of the 3rd Work-\\nshop on Evaluating Vector Space Representations\\nfor NLP.\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='for common sense. In Proceedings of the 3rd Work-\\nshop on Evaluating Vector Space Representations\\nfor NLP.\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,\\nand He He. 2021. Meta-learning via language model\\nin-context tuning. arXiv preprint arXiv:2110.07814.\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\\nAshish Sabharwal, Carissa Schoenick, and Oyvind\\nTafjord. 2018. Think you have solved question an-\\nswering? try arc, the ai2 reasoning challenge. ArXiv.\\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\\n2005. The pascal recognising textual entailment\\nchallenge. In Machine Learning Challenges Work-\\nshop.\\nOna de Gibert, Naiara Perez, Aitor García-Pablos, and\\nMontse Cuadros. 2018. Hate Speech Dataset from a\\nWhite Supremacy Forum. In Proceedings of the 2nd\\nWorkshop on Abusive Language Online (ALW2).\\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\\ndith Tonhauser. 2019. The commitmentbank: Inves-\\ntigating projection in naturally occurring discourse.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Workshop on Abusive Language Online (ALW2).\\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\\ndith Tonhauser. 2019. The commitmentbank: Inves-\\ntigating projection in naturally occurring discourse.\\nProceedings of Sinn und Bedeutung.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In NAACL.\\nT. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-\\nlian, Massimiliano Ciaramita, and Markus Leippold.\\n2020. Climate-fever: A dataset for veriﬁcation of\\nreal-world climate claims. ArXiv.\\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\nAvia Efrat and Omer Levy. 2020. The turking test: Can\\nlanguage models understand instructions? arXiv\\npreprint arXiv:2010.11982.\\nL Gao, S Biderman, S Black, L Golding, T Hoppe,\\nC Foster, J Phang, H He, A Thite, N Nabeshima,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='language models understand instructions? arXiv\\npreprint arXiv:2010.11982.\\nL Gao, S Biderman, S Black, L Golding, T Hoppe,\\nC Foster, J Phang, H He, A Thite, N Nabeshima,\\net al. 2021. The pile: an 800gb dataset of diverse\\ntext for language modeling 2020. arXiv preprint\\narXiv:2101.00027.\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\\nand Bill Dolan. 2007. The third pascal recognizing\\ntextual entailment challenge. In Proceedings of the\\nACL-PASCAL workshop on textual entailment and\\nparaphrasing.\\nAndrew Gordon, Zornitsa Kozareva, and Melissa\\nRoemmele. 2012. SemEval-2012 task 7: Choice\\nof plausible alternatives: An evaluation of common-\\nsense causal reasoning. In The First Joint Confer-\\nence on Lexical and Computational Semantics (Se-\\nmEval).\\nAri Holtzman, Peter West, Vered Schwartz, Yejin Choi,\\nand Luke Zettlemoyer. 2021. Surface form compe-\\ntition: Why the highest probability answer isn’t al-\\nways right. In EMNLP.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='and Luke Zettlemoyer. 2021. Surface form compe-\\ntition: Why the highest probability answer isn’t al-\\nways right. In EMNLP.\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\\nnaneh Hajishirzi. 2020. UniﬁedQA: Crossing for-\\nmat boundaries with a single qa system. In Findings\\nof EMNLP.\\nTushar Khot, Peter Clark, Michal Guerquin, Peter\\nJansen, and Ashish Sabharwal. 2020. Qasc: A\\ndataset for question answering via sentence compo-\\nsition. In AAAI.\\nJunyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho,\\nHwiyeol Jo, Sang-Woo Lee, Sang-goo Lee,\\nKang Min Yoo, and Taeuk Kim. 2022. Ground-truth\\nlabels matter: A deeper look into input-label demon-\\nstrations. arXiv preprint arXiv:2205.12685.\\nElyor Kodirov, Tao Xiang, Zhenyong Fu, and Shao-\\ngang Gong. 2015. Unsupervised domain adaptation\\nfor zero-shot learning. In Proceedings of the IEEE\\ninternational conference on computer vision.\\nHector J. Levesque, Ernest Davis, and Leora Morgen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='gang Gong. 2015. Unsupervised domain adaptation\\nfor zero-shot learning. In Proceedings of the IEEE\\ninternational conference on computer vision.\\nHector J. Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2012. The winograd schema challenge. In\\nProceedings of the Thirteenth International Confer-\\nence on Principles of Knowledge Representation\\nand Reasoning.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\\n2020. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In ACL.\\nQuentin Lhoest, Albert Villanova del Moral, Yacine\\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\\nPatry, Angelina McMillan-Major, Philipp Schmid,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Sylvain Gugger, Clément Delangue, Théo Matus-\\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\\ntac, Thibault Goehringer, Victor Mustar, François\\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\\nDatasets: A community library for natural language\\nprocessing. In EMNLP: System Demonstrations.\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\\nLawrence Carin, and Weizhu Chen. 2021. What\\nmakes good in-context examples for gpt- 3? arXiv\\npreprint arXiv:2101.06804.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\nRobert L Logan IV , Ivana Balaževic, Eric Wallace,\\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\\n2021. Cutting down on prompts and parameters:\\nSimple few-shot learning with language models.\\narXiv preprint arXiv:2106.13353.\\nYao Lu, Max Bartolo, Alastair Moore, Sebastian'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='2021. Cutting down on prompts and parameters:\\nSimple few-shot learning with language models.\\narXiv preprint arXiv:2106.13353.\\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\\nRiedel, and Pontus Stenetorp. 2021. Fantastically\\nordered prompts and where to ﬁnd them: Overcom-\\ning few-shot prompt order sensitivity.arXiv preprint\\narXiv:2104.08786.\\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\\nand patterns: For effective chain of thought, it takes\\ntwo to tango. arXiv preprint arXiv:2209.07686.\\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-\\nlenius, and Pyry Takala. 2014. Good debt or bad\\ndebt: Detecting semantic orientations in economic\\ntexts. J. Assoc. Inf. Sci. Technol.\\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\\nelli. 2014. A SICK cure for the evaluation of com-\\npositional distributional semantic models. In LREC.\\nClara H. McCreery, Namit Katariya, Anitha Kannan,\\nManish Chablani, and Xavier Amatriain. 2020. Ef-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='positional distributional semantic models. In LREC.\\nClara H. McCreery, Namit Katariya, Anitha Kannan,\\nManish Chablani, and Xavier Amatriain. 2020. Ef-\\nfective transfer learning for identifying similar ques-\\ntions: Matching user questions to covid-19 faqs.\\nIn Proceedings of the 26th ACM SIGKDD Interna-\\ntional Conference on Knowledge Discovery & Data\\nMining.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\\nSabharwal. 2018. Can a suit of armor conduct elec-\\ntricity? a new dataset for open book question answer-\\ning. In EMNLP.\\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2021a. Noisy channel language\\nmodel prompting for few-shot text classiﬁcation.\\narXiv preprint arXiv:2108.04106.\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2021b. MetaICL: Learning to\\nlearn in context. arXiv preprint.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\\nChoi, and Hannaneh Hajishirzi. 2021a. Refram-\\ning instructional prompts to gptk’s language. arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='learn in context. arXiv preprint.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\\nChoi, and Hannaneh Hajishirzi. 2021a. Refram-\\ning instructional prompts to gptk’s language. arXiv\\npreprint arXiv:2109.07830.\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\\nHannaneh Hajishirzi. 2021b. Cross-task generaliza-\\ntion via natural language crowdsourcing instructions.\\narXiv preprint arXiv:2104.08773.\\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\\nand Grigorios Tsoumakas. 2020. Ethos: an online\\nhate speech detection dataset. ArXiv.\\nSebastian Nagel. 2016. CC-News. http:\\n//web.archive.org/save/http:\\n//commoncrawl.org/2016/10/\\nnews-dataset-available.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nblog.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='blog.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a uniﬁed text-to-text trans-\\nformer. Journal of Machine Learning Research.\\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\\nand Sameer Singh. 2022. Impact of pretraining term\\nfrequencies on few-shot reasoning. arXiv preprint\\narXiv:2202.07206.\\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\\ngramming for large language models: Beyond the\\nfew-shot paradigm. In Extended Abstracts of the\\n2021 CHI Conference on Human Factors in Com-\\nputing Systems.\\nFrieda Rong. 2021. Extrapolating to unnatu-\\nral language processing with gpt-3’s in-context\\nlearning: The good, the bad, and the myste-\\nrious. https://ai.stanford.edu/blog/\\nin-context-learning.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='in-context-learning.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun\\nRaja, Manan Dey, M Saiful Bari, Canwen Xu, Ur-\\nmish Thakker, Shanya Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\\nJiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\\nGao, Tali Bers, Thomas Wolf, and Alexander M.\\nRush. 2022. Multitask prompted training enables\\nzero-shot task generalization. In ICLR.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Timo Schick and Hinrich Schütze. 2021. It’s not just\\nsize that matters: Small language models are also\\nfew-shot learners. In NAACL-HLT.\\nEmily Sheng and David Uthus. 2020. Investigating so-\\ncietal biases in a poetry composition system. In Pro-\\nceedings of the Second Workshop on Gender Bias in\\nNatural Language Processing.\\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\\nand Claire Cardie. 2019. DREAM: A challenge data\\nset and models for dialogue-based reading compre-\\nhension. TACL.\\nOyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau\\nYih, and Ashish Sabharwal. 2019a. Quarel: A\\ndataset and models for answering questions about\\nqualitative relationships. In AAAI.\\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter\\nClark. 2019b. QuaRTz: An open-domain dataset of\\nqualitative relationship questions. In EMNLP.\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A ques-\\ntion answering challenge targeting commonsense\\nknowledge. In NAACL-HLT.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Alon Talmor, Jonathan Herzig, Nicholas Lourie, and\\nJonathan Berant. 2019. Commonsenseqa: A ques-\\ntion answering challenge targeting commonsense\\nknowledge. In NAACL-HLT.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel Bowman. 2019a. Superglue: A\\nstickier benchmark for general-purpose language un-\\nderstanding systems. In NeurIPS.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R Bowman. 2018.\\nGlue: A multi-task benchmark and analysis plat-\\nform for natural language understanding. In Black-\\nboxNLP Workshop: Analyzing and Interpreting Neu-\\nral Networks for NLP.\\nBen Wang and Aran Komatsuzaki. 2021. GPT-\\nJ-6B: A 6 Billion Parameter Autoregressive\\nLanguage Model. https://github.com/\\nkingoflolz/mesh-transformer-jax.\\nWei Wang, Vincent W Zheng, Han Yu, and Chunyan\\nMiao. 2019b. A survey of zero-shot learning: Set-\\ntings, methods, and applications. ACM Transactions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='kingoflolz/mesh-transformer-jax.\\nWei Wang, Vincent W Zheng, Han Yu, and Chunyan\\nMiao. 2019b. A survey of zero-shot learning: Set-\\ntings, methods, and applications. ACM Transactions\\non Intelligent Systems and Technology (TIST).\\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\\nbased models really understand the meaning of their\\nprompts? In NAACL-HLT.\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\\ndrew M Dai, and Quoc V Le. 2022a. Finetuned lan-\\nguage models are zero-shot learners. In ICLR.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. arXiv preprint arXiv:2201.11903.\\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\\nand Tengyu Ma. 2022. An explanation of in-context\\nlearning as implicit bayesian inference. In ICLR.\\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\\nCrossﬁt: A few-shot learning challenge for cross-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='and Tengyu Ma. 2022. An explanation of in-context\\nlearning as implicit bayesian inference. In ICLR.\\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\\nCrossﬁt: A few-shot learning challenge for cross-\\ntask generalization in nlp. In EMNLP.\\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\\nSameer Singh. 2021. Calibrate before use: Improv-\\ning few-shot performance of language models. In\\nICML.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='A Full Datasets\\nWe include 26 datasets as follows: ﬁ-\\nnancial_phrasebank (Malo et al., 2014),\\npoem_sentiment (Sheng and Uthus, 2020),\\nmedical_questions_pairs (McCreery et al., 2020),\\nglue-mrpc (Dolan and Brockett, 2005), glue-\\nwnli (Levesque et al., 2012), climate_fever (Diggel-\\nmann et al., 2020), glue-rte (Dagan et al., 2005;\\nBar-Haim et al., 2006; Giampiccolo et al.,\\n2007; Bentivogli et al., 2009), superglue-\\ncb (de Marneffe et al., 2019), sick (Marelli et al.,\\n2014) , hate_speech18 (de Gibert et al., 2018),\\nethos-national_origin (Mollas et al., 2020), ethos-\\nrace (Mollas et al., 2020), ethos-religion (Mollas\\net al., 2020), tweet_eval-hate (Barbieri et al., 2020),\\ntweet_eval-stance_atheism (Barbieri et al., 2020),\\ntweet_eval-stance_feminist (Barbieri et al., 2020),\\nquarel (Tafjord et al., 2019a), openbookqa (Mi-\\nhaylov et al., 2018), qasc (Khot et al., 2020), com-\\nmonsense_qa (Talmor et al., 2019), ai2_arc (Clark\\net al., 2018), codah (Chen et al., 2019), superglue-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='haylov et al., 2018), qasc (Khot et al., 2020), com-\\nmonsense_qa (Talmor et al., 2019), ai2_arc (Clark\\net al., 2018), codah (Chen et al., 2019), superglue-\\ncopa (Gordon et al., 2012), dream (Sun et al.,\\n2019), quartz-with_knowledge (Tafjord et al.,\\n2019b), quartz-no_knowledge (Tafjord et al.,\\n2019b). The choice of datasets is made following\\nlow-resource datasets in Min et al. (2021b), with\\nthe exact same set of k-shot train data using 5\\nrandom seeds. We use the HuggingFace version\\nof the data (Lhoest et al., 2021) and use the\\ndevelopment data for evaluation, following Ye\\net al. (2021). See Table 2 for statistics.\\nB Experimental Details\\nExample template We follow Ye et al. (2021);\\nMin et al. (2021b); Logan IV et al. (2021) in us-\\ning the minimal format to transform the input to a\\nsequence (e.g. a concatenation of multiple inputs)\\nand using the label words from each dataset as it is.\\nWe also explore manual templates taken from prior\\nwork (Holtzman et al., 2021; Zhao et al., 2021) as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='and using the label words from each dataset as it is.\\nWe also explore manual templates taken from prior\\nwork (Holtzman et al., 2021; Zhao et al., 2021) as\\nreported in Section 4.2, although we ﬁnd that using\\nthese templates is not consistently better than using\\nminimal templates. We thus run main experiments\\nwith minimal templates. Example templates are\\nprovided in Table 3.\\nFormat of the demonstrations We follow the\\nstandard of each model for formatting the demon-\\nstrations, either from exploration in prior work or\\nthe example code provided in the ofﬁcial tutorial.\\nFor GPT-2, we separate the input and the label,\\nDataset # Train # Eval\\nTask category: Sentiment analysis\\nﬁnancial_phrasebank 1,811 453\\npoem_sentiment 892 105\\nTask category: Paraphrase detection\\nmedical_questions_pairs 2,438 610\\nglue-mrpc 3,668 408\\nTask category: Natural language inference\\nglue-wnli 635 71\\nclimate_fever 1,228 307\\nglue-rte 2,490 277\\nsuperglue-cb 250 56\\nsick 4,439 495\\nTask category: Hate speech detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='glue-mrpc 3,668 408\\nTask category: Natural language inference\\nglue-wnli 635 71\\nclimate_fever 1,228 307\\nglue-rte 2,490 277\\nsuperglue-cb 250 56\\nsick 4,439 495\\nTask category: Hate speech detection\\nhate_speech18 8,562 2,141\\nethos-national_origin 346 87\\nethos-race 346 87\\nethos-religion 346 87\\ntweet_eval-hate 8,993 999\\ntweet_eval-stance_atheism 461 52\\ntweet_eval-stance_feminist 597 67\\nTask category: Question answering\\nquarel 1,941 278\\nopenbookqa 4,957 500\\nqasc 8,134 926\\ncommonsense_qa 9,741 1,221\\nai2_arc 1,119 299\\nTask category: Sentence completion\\ncodah 1665 556\\nsuperglue-copa 400 100\\ndream 6116 2040\\nquartz-with_knowledge 2696 384\\nquartz-no_knowledge 2696 384\\nTable 2: 26 datasets used for experiments, classiﬁed\\ninto 6 task categories. # Train and # Test indicate the\\nnumber of training and test examples of the dataset.\\nNote that # train is based on the original training dataset\\nbut we use k random samples for k-shot evaluation.\\nand each demonstration example with a space. For'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Note that # train is based on the original training dataset\\nbut we use k random samples for k-shot evaluation.\\nand each demonstration example with a space. For\\nMetaICL, GPT-J and GPT-3, we separate the input\\nand the label with a newline (\\\\n), and each demon-\\nstration example with three newlines. For fairseq\\nmodels, we use a newline to separate the input and\\nthe label as well as each demonstration example.\\nDetails in variants of the demonstrations For\\n“demonstrations w/ a% accurate labels” ( 0 ≤\\na ≤ 100), we use k ×a/100 correct pairs and\\nk ×(1 −a/100) incorrect pairs in a random order,\\nas described in Algorithm 1. For “OOD demon-\\nstrations”, we use CC-News (Nagel, 2016) as an\\nexternal corpus. We consider the length of the text\\nduring sampling, so that sampled sentences have\\nsimilar length to the test input. For “demonstrations\\nwith random English words”, we use pypi.org/\\nproject/english-words for the set of En-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Direct\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Macro-F1 (%)\\nClassification\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nDirect\\nGPT-2\\nChannel\\nGPT-2\\nDirect\\nMetaICL\\nChannel\\nMetaICL\\nDirect\\nGPT-J\\nChannel\\nGPT-J\\nDirect\\nfairseq 6.7B\\nChannel\\nfairseq 6.7B\\nDirect\\nfairseq 13B\\nChannel\\nfairseq 13B\\nDirect\\nGPT-3\\nChannel\\nGPT-3\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70Accuracy (%)\\nMulti-choice\\nNo Demos Demos w/ gold labels Demos w/ random labels\\nFigure 11: Results of No-demonstration, Gold demonstration and Random demonstration on 3 classiﬁcation\\ndatasets (top) and 3 multi-choice datasets (bottom). Details in Section 4.1. This ﬁgure is for providing numbers\\nthat are comparable across models—full results with more datasets are reported in Figure 3.\\nAlgorithm 1 Forming the demonstrations with an\\naccuracy of a%.\\n1: procedure FORM DEMONS ({(xi, yi)}k'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='that are comparable across models—full results with more datasets are reported in Figure 3.\\nAlgorithm 1 Forming the demonstrations with an\\naccuracy of a%.\\n1: procedure FORM DEMONS ({(xi, yi)}k\\ni=1, a)\\n2: D ←[] // demonstration to be formed\\n3: n ←k ×a/100 // number of correct pairs\\n4: G← Sample(Range(1, k), n)\\n5: for i ∈Range(1, k) do\\n6: if i ∈G then // add correct pair\\n7: D.append((xi, yi))\\n8: else // add incorrect pair\\n9: D.append((xi, Sample(C−yi)))\\n10: return D\\nglish words, which consists of 61,569 words.\\nTable 4 provides a list of example demonstra-\\ntions for each method used in Section 5.\\nC More Experimental Results\\nC.1 Gold labels vs. random labels\\nFigure 11 shares the same interface as Figure 3, but\\nall models are evaluated on 3 classiﬁcation and 3\\nmulti-choice datasets and are thus comparable to\\neach other.\\nC.2 Random labels from true distribution of\\nlabels & Task breakdown\\nIn Section 4, random labels are sampled from the\\nlabel space from a uniform distribution. We ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='each other.\\nC.2 Random labels from true distribution of\\nlabels & Task breakdown\\nIn Section 4, random labels are sampled from the\\nlabel space from a uniform distribution. We ex-\\nperiment with another variant of demonstrations in\\nthe classiﬁcation tasks, where labels are randomly\\nsampled from the true distribution of labels on the\\ntraining data. This may have large impact if labels\\nare far from uniform on the training data. Results\\nindicate that performance drop from using gold\\nlabels is further reduced compared to using uni-\\nformly random labels: with Channel MetaICL, the\\ngap is reduced from 1.9% to 1.3% absolute, and\\nwith Channel GPT-J, the gap is reduced from 5.0%\\nto 3.5% absolute.\\nFigure 12 shows performance gap between using\\ngold labels and using random labels per dataset. We\\nﬁnd that the trend that the gap is smaller than pre-\\nviously thought is consistant across most datasets.\\nNonetheless, there are a few outlier datasets where\\nperformance gap is non-negligible, such as ﬁnan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='viously thought is consistant across most datasets.\\nNonetheless, there are a few outlier datasets where\\nperformance gap is non-negligible, such as ﬁnan-\\ncial_phrasebank and a few hate speech detection\\ndatasets. Future work may investigate on which\\ntasks the model makes more use of the correctly\\npaired training data.\\nC.3 More variants of the demonstrations\\nWe explored demonstrations with a con-\\nstant label where all labels in the demon-\\nstrations are replaced with a constant text,\\n“answer”. Speciﬁcally, a prediction is made via\\nargmaxy∈CP(y|x1, answer...xk, answer, x).\\nThis can be viewed as another way to remove the\\nimpact of the label space while keeping the impact\\nof the distribution of the input text. However,\\nresults are consistently worse than the results\\nof demonstrations with random English labels.\\nWe think this is because constant labels actually\\nchange the format of the demonstrations, since\\nthey can be viewed as part of a separator between\\ndifferent demonstration examples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='We think this is because constant labels actually\\nchange the format of the demonstrations, since\\nthey can be viewed as part of a separator between\\ndifferent demonstration examples.\\nWe also exploreddemonstrations with the test\\ninput where all inputs in the demonstrations are\\nreplaced with the test input, each paired with a ran-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='dom label. Speciﬁcally, a prediction is made via\\nargmaxy∈CP(y|x, ˜y1...x, ˜yk, x), where ˜yi (1 ≤\\ni ≤k) is randomly sampled at uniform from C.\\nThis variant is seemingly a reasonable choice given\\nthat it satisﬁes the condition that the inputs in the\\ndemonstrations come from the same distribution\\nas the test input (since they are identical), and us-\\ning random labels is as good as using gold labels.\\nNonetheless, we ﬁnd that this variant is signiﬁ-\\ncantly worse than most other methods with demon-\\nstrations. We think this is because using the con-\\nstant input for all demonstration example signiﬁ-\\ncantly changes the format of the sequence, since the\\ninput can be viewed as part of a separator between\\ndifferent demonstration examples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='financial_phrasebank\\ntweet_eval-stance_feminist\\npoem_sentiment-new\\nsuperglue-cb\\nethos-national_origin\\ntweet_eval-stance_atheism\\nsick\\nhate_speech18\\nglue-wnli\\nethos-religion\\nethos-race\\ncodah quarel\\ncommonsense_qa\\nmedical_questions_pairs\\nopenbookqa\\ntweet_eval-hate\\nquartz-with_knowledge\\nsuperglue-copa-new\\nclimate_fever\\nqasc dream ai2_arc\\nglue-mrpc\\nquartz-no_knowledge\\nglue-rte\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nPerformance drop (Channel MetaICL, uniform)\\nfinancial_phrasebank\\nsuperglue-cb\\npoem_sentiment-new\\nethos-race\\nsick\\ntweet_eval-stance_feminist\\ntweet_eval-hatehate_speech18ethos-religion\\nglue-rte\\nclimate_fever\\nethos-national_origin\\nglue-wnli\\nquartz-with_knowledge\\ncommonsense_qa\\nqasc\\nquartz-no_knowledge\\nopenbookqa\\ndream\\ntweet_eval-stance_atheism\\nai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nglue-mrpc\\nquarel codah\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nPerformance drop (Channel GPT -J, uniform)\\ntweet_eval-stance_feminist\\npoem_sentiment-newfinancial_phrasebank\\ntweet_eval-stance_atheism'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='glue-mrpc\\nquarel codah\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\nPerformance drop (Channel GPT -J, uniform)\\ntweet_eval-stance_feminist\\npoem_sentiment-newfinancial_phrasebank\\ntweet_eval-stance_atheism\\nethos-national_origin\\nsuperglue-cbethos-religion\\nglue-wnli\\nsick codah\\nclimate_fever\\nquarel\\ntweet_eval-hate\\nethos-race\\ncommonsense_qa\\nglue-mrpc\\nhate_speech18\\nmedical_questions_pairs\\nopenbookqa\\nquartz-with_knowledge\\nsuperglue-copa-new\\nqasc dream ai2_arc\\nquartz-no_knowledge\\nglue-rte\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\nPerformance drop (Channel MetaICL, true distribution)\\nsick\\npoem_sentiment-new\\nethos-race\\ntweet_eval-hate\\ntweet_eval-stance_feminist\\nfinancial_phrasebank\\nhate_speech18\\ntweet_eval-stance_atheism\\nclimate_fever\\nethos-national_origin\\nglue-rte\\nethos-religion\\nglue-wnli\\nquartz-with_knowledge\\ncommonsense_qa\\nqasc\\nglue-mrpc\\nquartz-no_knowledge\\nopenbookqa\\ndream ai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nquarel codah\\nsuperglue-cb\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='quartz-with_knowledge\\ncommonsense_qa\\nqasc\\nglue-mrpc\\nquartz-no_knowledge\\nopenbookqa\\ndream ai2_arc\\nsuperglue-copa-new\\nmedical_questions_pairs\\nquarel codah\\nsuperglue-cb\\n3\\n2\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nPerformance drop (Channel GPT -J, true distribution)\\nFigure 12: Performance gap from using the demonstrations with gold labels to using the demonstrations with\\nrandom labels. Datasets are sorted in descending order. The top two ﬁgures use random labels that are sampled\\nat uniform, with Channel MetaICL and Channel GPT-J, respectively. The bottom two ﬁgures use random labels\\nthat are sampled from a true distribution of labels on the training data, with Channel MetaICL and Channel GPT-J,\\nrespectively.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Dataset Type Example\\nMRPC Minimal sentence 1: Cisco pared spending to compensate for sluggish sales . [SEP] sentence 2: In\\nresponse to sluggish sales , Cisco pared spending . \\\\n {equivalent|not_equivalent}\\nManual Cisco pared spending to compensate for sluggish sales . \\\\n The question is: In response to\\nsluggish sales , Cisco pared spending . True or False? \\\\n The answer is:{True|False}\\nRTE Minimal sentence 1: The girl was found in Drummondville. [SEP] sentence 2: Drummondville\\ncontains the girl. \\\\n {entailment|not_entailment}\\nManual The girl was found in Drummondville. \\\\n The question is: Drummondville contains the\\ngirl. True or False? \\\\n The answer is:{True|False}\\nTweet_eval-hate Minimal The Truth about #Immigration \\\\n {hate |non-hate}\\nManual Tweet: The Truth about #Immigration \\\\n Sentiment: {against |favor}\\nSICK Minimal sentence 1: A man is screaming. [SEP] sentence 2: A man is scared. \\\\n\\n{contradiction|entailment|neutral}'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Manual Tweet: The Truth about #Immigration \\\\n Sentiment: {against |favor}\\nSICK Minimal sentence 1: A man is screaming. [SEP] sentence 2: A man is scared. \\\\n\\n{contradiction|entailment|neutral}\\nManual A man is screaming. \\\\n The question is: A man is scared. True or False? \\\\n The answer is:\\n{False|True|Not sure}\\npoem-sentiment Minimal willis sneered: \\\\n {negative |no_impact|positive}\\nManual willis sneered: \\\\n The sentiment is: {negative |no_impact|positive}\\nOpenbookQA Minimal What creates a valley? \\\\n {feet |rock|water|sand}\\nManual The question is: What creates a valley? \\\\n The answer is: {feet |rock|water|sand}\\nCommonsenseQA Minimal What blocks sunshine? \\\\n {summer |park|desktop|sea|moon}\\nManual The question is: What blocks sunshine? \\\\n The answer is: {summer |park|desktop|sea|moon}\\nCOPA Minimal Effect: I coughed. \\\\n {Cause: I inhaled smoke. |Cause: I lowered my voice.}\\nManual I coughed because {I inhaled smoke. |I lowered my voice.}'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='COPA Minimal Effect: I coughed. \\\\n {Cause: I inhaled smoke. |Cause: I lowered my voice.}\\nManual I coughed because {I inhaled smoke. |I lowered my voice.}\\nARC Minimal Which biome has the most vegetation? \\\\n {desert |forest|grassland|tundra}\\nManual The question is: Which biome has the most vegetation? \\\\n The answer is: {desert |forest|\\ngrassland|tundra}\\nTable 3: A list of minimal templates taken from Ye et al. (2021); Min et al. (2021b) and manual templates taken\\nfrom Holtzman et al. (2021); Zhao et al. (2021). Details provided in Appendix B. See Figure 6 for discussion in\\nempirical results. The input and the label are in the red text and in the blue text, respectively. Note that |is used to\\nseparate different options for the labels.\\nDemos\\nw/ gold labels\\n(Format \\x13 Input distribution \\x13 Label space \\x13 Input-label mapping \\x13)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n positive\\nPanostaja did not disclose the purchase price. \\\\n neutral\\nDemos'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Circulation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n positive\\nPanostaja did not disclose the purchase price. \\\\n neutral\\nDemos\\nw/ random labels\\n(Format \\x13 Input distribution \\x13 Label space \\x13 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n neutral\\nPanostaja did not disclose the purchase price. \\\\n negative\\nOOD Demos\\nw/ random labels\\n(Format \\x13 Input distribution \\x17 Label space \\x13 Input-label mapping \\x17)\\nColour-printed lithograph. Very good condition. Image size: 15 x 23 1/2 inches. \\\\n neutral\\nMany accompanying marketing claims of cannabis products are often well-meaning. \\\\n negative\\nDemos\\nw/ random English words\\n(Format \\x13 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008. \\\\n unanimity\\nPanostaja did not disclose the purchase price. \\\\n wave\\nDemos\\nw/o labels\\n(Format \\x17 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Panostaja did not disclose the purchase price. \\\\n wave\\nDemos\\nw/o labels\\n(Format \\x17 Input distribution \\x13 Label space \\x17 Input-label mapping \\x17)\\nCirculation revenue has increased by 5% in Finland and 4% in Sweden in 2008.\\nPanostaja did not disclose the purchase price.\\nDemos\\nlabels only\\n(Format \\x17 Input distribution \\x17 Label space \\x13 Input-label mapping \\x17)\\npositive\\nneutral\\nTable 4: Example demonstrations when using methods in Section 5. The ﬁnancial_phrasebank dataset with C=\\n{“positive”, “neutral”, “negative”}is used. Red text indicates the text is sampled from an external corpus; blue\\ntext indicates the labels are randomly sampled from the label set; purple text indicates a random English word.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='Rethinking the Role of Demonstrations:\\nWhat Makes In-Context Learning Work?\\nAnonymous ACL submission\\nWe are glad that all reviewers ﬁnd that the001\\npaper is novel (8jk5, LQ6N, 92YB, 7E5P), of002\\ninterest to the broader NLP community (LQ6N,003\\n92YB, 7E5P), supported by solid experiments004\\n(8jk5, LQ6N, 92YB, 7E5P), and well-written (8jk5,005\\nLQ6N, 92YB). Reviewers gave comments on more006\\ndiscussion, limitations and avenues for future work.007\\nWe will incorporate them in the next version of the008\\npaper.1009\\nAdding discussion on robustness of LMs (8jk5,010\\n7E5P): We think the fact that LMs do not use011\\nthe input-label correspondence does not necessar-012\\nily mean that LMs are robust to other aspects of013\\nthe demonstration. Nonetheless, it will be an inter-014\\nesting avenue for future work, given that LMs are015\\nhighly sensitive to small changes in the demonstra-016\\ntions (??).017\\nAdding discussion with respect to the model018\\nsize (8jk5): Absolute differences are similar019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='highly sensitive to small changes in the demonstra-016\\ntions (??).017\\nAdding discussion with respect to the model018\\nsize (8jk5): Absolute differences are similar019\\nacross different model sizes (Figure 3), but since020\\nthe large models have higher absolute performance,021\\nthe relative differences are larger with larger mod-022\\nels.023\\nWhen our ﬁndings hold or do not hold (8jk5):024\\nWe believe that the ﬁndings will hold only when025\\nsome notion of input-label correspondences are026\\nalready captured during pre-training—for tasks027\\nwhose input-label correspondences during pretrain-028\\ning is sparse, the demonstrations with random la-029\\nbels are highly unlikely to work. This has been030\\nshown by ? in a synthetic setup, and future work031\\ncan revisit this with more natural data.032\\nRisk in applying in-context learning, Rec-033\\nommendation to practitioners (8jk5, LQ6N):034\\nSince the models do not capture the correspondence035\\nfrom the demonstrations, it is possible that models036'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='ommendation to practitioners (8jk5, LQ6N):034\\nSince the models do not capture the correspondence035\\nfrom the demonstrations, it is possible that models036\\nare simply relying on some notation of input-label037\\ncorrespondence during pre-training. Thus, apply-038\\ning in-context learning for problems that were not039\\n1We answered reviewers’ questions on the OpenReview\\npage, and address higher-level comments here.\\nin the pretraining data would be potentially risky, 040\\nand practitioners may want to collect the labeled 041\\ndata and ﬁne-tune the model. 042\\nWhere do the gains from demonstrations come 043\\nfrom? (92YB): We think gains from demonstra- 044\\ntions are mainly due to the speciﬁcation of the in- 045\\nput distribution and the label space rather than the 046\\ninput-label correspondence, as Section 5 indicates. 047\\nWe will clarify this in the next version of the paper. 048\\nConcrete answer to “why” using random labels 049\\nkeeps reasonable performance (LQ6N): We 050'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='We will clarify this in the next version of the paper. 048\\nConcrete answer to “why” using random labels 049\\nkeeps reasonable performance (LQ6N): We 050\\nthink it is highly likely to be because the model 051\\nhas been exposed to some notion of input-label cor- 052\\nrespondence during pre-training, so that the demon- 053\\nstrations play a role of triggering them at inference. 054\\nConsideration when training large LMs 055\\n(LQ6N): Due to compute limitations, we were 056\\nnot be able to provide recommendations that 057\\nare empirically supported. Future work may 058\\ninvestigate aspects of language model pretraining 059\\nthat affect the ﬁndings, including the pretraining 060\\ndata and the objective. 061\\nWord ordering matters? (7E5P): We think it 062\\nis an important avenue for future work. For this 063\\npaper, we did not include it in our scope due to 064\\nrequiring multiple times more experiments. 065\\nMore task types, e.g., generation (7E5P): Ex- 066'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='paper, we did not include it in our scope due to 064\\nrequiring multiple times more experiments. 065\\nMore task types, e.g., generation (7E5P): Ex- 066\\ntending this work to generation tasks is not very 067\\ntrivial because designing the demonstrations that do 068\\nnot keep the input-output correspondence but keep 069\\nthe output distribution is difﬁcult. For instance, if 070\\nwe simply replace the output with a random out- 071\\nput as we did in the classiﬁcation tasks, it will 072\\ndestroy both the input-output correspondence and 073\\nthe output distribution. We hope future work can 074\\ninvestigate more in this direction. 075\\nStronger link with instruction-following models 076\\n(7E5P): We will add discussion on ? who ﬁnd 077\\nthat instructions that are irrelevant or even mislead- 078\\ning lead to performance gains as much as “good” 079\\n1\\narXiv:2202.12837v2  [cs.CL]  20 Oct 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='instructions do.080\\nLimitation081\\nThis paper focuses on the tasks from established082\\nNLP benchmarks that have real natural language083\\ninputs. Synthetic tasks with more limited inputs084\\nmay actually use the ground truth labels more, as085\\nobserved by ?. Our paper mainly includes macro-086\\nlevel analysis by examining the average perfor-087\\nmance over multiple NLP datasets, but different088\\ndatasets may behave differently. Appendix dis-089\\ncusses this aspect, including ﬁndings that there are090\\nlarger gaps between using the ground truth labels091\\nand using the random labels in some dataset-model092\\npairs (e.g., in the most extreme case, nearly 14%093\\nabsolute on the ﬁnancial_phrasebank dataset with094\\nGPT-J). We believe it is important to understand in095\\nwhich cases the ground truth labels matter or not,096\\nwhich we leave for future work. Furthermore, our097\\nwork is limited to classiﬁcation and multi-choice098\\ntasks. Extension to the open-set tasks such as gener-099'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:20:25+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2022-10-21T00:20:25+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19', 'source_file': 'incontext_learning.pdf', 'file_type': 'pdf'}, page_content='which we leave for future work. Furthermore, our097\\nwork is limited to classiﬁcation and multi-choice098\\ntasks. Extension to the open-set tasks such as gener-099\\nation is not trivial, since it is unclear how to remove100\\nthe input-output correspondence while keeping the101\\ncorrect output distribution. We leave the extension102\\nfor future work.103\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Benchmarking of Retrieval Augmented Generation: A \\nComprehensive Systematic Literature Review on Evaluation \\nDimensions, Evaluation Metrics and Datasets \\nSimon Knollmeyer1,* a , Oğuz Caymazer2,* b , Leonid Koval1c , Muhammad Uzair Akmal1d ,  \\nSaara Asif1e , Selvine G. Mathias1f  and Daniel Großmann1g  \\n1Technische Hochschule Ingolstadt, AImotion Bavaria, Esplanade 10, Ingolstadt, Germany \\n2University of Münster, Department of Information Systems, Münster, Germany \\n{Simon.Knollmeyer, Leonid.Koval, MuhammadUzair.Akmal, Saara.Asif, SelvineGeorge.Mathias, \\n \\nKeywords: Large Language Model,  Retrieval Augmented Generation,  Evaluation Dimensions, Evaluation Metrics, \\nDatasets, Systematic Literature Review. \\nAbstract: Despite the rapid advancements in the field of Large Language Models (LLM), traditional benchmarks have \\nproven to be inadequate for asse ssing the performance of Retrie val Augmented Generation (RAG) systems.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='proven to be inadequate for asse ssing the performance of Retrie val Augmented Generation (RAG) systems. \\nTherefore, this paper presents a comprehensive systematic literature review of evaluation dimensions, metrics, \\nand datasets for RAG systems. Thi s review identifies key evalua tion dimensions such as context relevance, \\nfaithfulness, answer relevance,  correctness, and citation quali ty. For each evaluation dimension, several \\nmetrics and evaluators are propo sed on how to assess them. This  paper synthesizes the findings from 12 \\nrelevant papers and presents a concept matrix that categorizes each evaluation approach. The results provide \\na foundation for the development of robust evaluation frameworks and suitable datasets that are essential for \\nthe effective implementation and deployment of RAG systems in real-world applications. \\n1 INTRODUCTION \\nThe rapid evolution of Artificial Intelligence (AI) \\nespecially in the field of Large Language Models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='1 INTRODUCTION \\nThe rapid evolution of Artificial Intelligence (AI) \\nespecially in the field of Large Language Models \\n(LLMs) attracts widespread attention due to their \\ngroundbreaking achievements in solving complex \\nproblems even surpassing the performance of humans \\nin certain fields (Benbya et al., 2024; Bubeck et al., \\n2023; OpenAI et al., 2023). The speed of AI \\ndevelopment in the area of LLMs outpaced methods \\nto assess their performance and accuracy, leading to \\nmajor flaws in existing traditional benchmarks to \\nevaluate LLMs output through reliable metrics  \\nimpeding their adoption (Hammond, 2024).  \\n \\na  https://orcid.org/0009-0002-1429-6992 \\nb  https://orcid.org/0009-0003-4096-3784 \\nc  https://orcid.org/0000-0003-4845-6579 \\nd  https://orcid.org/0009-0007-3961-1174 \\ne  https://orcid.org/0009-0006-1284-5635 \\nf  https://orcid.org/0000-0002-6549-0763 \\ng  https://orcid.org/0000-0002-7388-5757 \\n* co-authors of the paper, contributed equally'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='e  https://orcid.org/0009-0006-1284-5635 \\nf  https://orcid.org/0000-0002-6549-0763 \\ng  https://orcid.org/0000-0002-7388-5757 \\n* co-authors of the paper, contributed equally \\nDespite their potential, LLMs face substantial \\nchallenges, particularly in fully grasping contextual \\nfactors such as unique technical requirements within \\na specific industries, yet understanding these factors \\nis essential for effective decision-making (Benbya et \\nal., 2024). Even the most powerful models such as \\nGPT-4 struggle with hallucinations, lack of the ability \\nto update itself, and limited context (Bubeck et al., \\n2023; OpenAI et al., 2023). Several researchers point \\nout that these LLMs seem to rather memorize \\nfrequently occurring information encountered during \\ntheir pre-training and struggle with infrequent \\ninformation, i.e., which would typically occur in a \\nspecific industry (Kandpal et al., 2022; Mallen et al., \\n2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='their pre-training and struggle with infrequent \\ninformation, i.e., which would typically occur in a \\nspecific industry (Kandpal et al., 2022; Mallen et al., \\n2022). \\nKnollmeyer, S., Caymazer, O., Koval, L., Akmal, M., Asif, S., Mathias, S. and Großmann, D.\\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation Metrics and Datasets.\\nDOI: 10.5220/0013065700003838\\nPaper published under CC license (CC BY -NC-ND 4.0)\\nIn Proceedings of the 16th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K 2024) - Volume 3: KMIS, pages 137-148\\nISBN: 978-989-758-716-0; ISSN: 2184-3228\\nProceedings Copyright© 2024 by SCITEPRESS – Science and Technology Publications, Lda.\\n137'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='M o s t  p r o m i s i n g  a n d  c o m m o n  t o  s o l v e  t h i s  \\nproblem is to augment LLM with non-parametric less \\ncommon knowledge by providing them retrieved text \\nchunks from an external database (Asai et al., 2024; \\nY. Gao et al., 2023; Mallen et al., 2022; Wang et al., \\n2023; Zhang et al., 2023). This approach is known as \\nRetrieval Augmented Generation (RAG), and there \\nare several different RAG paradigms, ranging from \\nso-called naïve RAG to more advanced ones (Asai et \\nal., 2023; Y. Gao et al., 2023; Ma et al., 2023). \\nResearch results suggest that LLM RAGs outperform \\nLLMs, particularly in long-t ail knowledge questions \\n(Asai et al., 2023; Izacard et al., 2022; Ma et al., 2023; \\nMallen et al., 2022; Wang et al., 2023). \\nHowever, there is still uncertainty about the \\naccuracy of such approaches due to the lack of \\ncomprehensive evaluation frameworks to provide \\nevaluation dimensions and metrics to assess RAG \\nLLMs (Y. Gao et al., 2023; Wang et al., 2023). Thus,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='accuracy of such approaches due to the lack of \\ncomprehensive evaluation frameworks to provide \\nevaluation dimensions and metrics to assess RAG \\nLLMs (Y. Gao et al., 2023; Wang et al., 2023). Thus, \\nwe address the following research questions (RQ): \\n \\n• R Q 1 :  H o w  t o  e v a l u a t e  a  R A G - e n h a n c e d  \\nLLM comprehensively across different \\ndimensions and metrics? \\n• RQ2: What type of datasets are available for \\napplying the dimensions and metrics?  \\n \\n Therefore, the research contribution of this paper \\nl i e s  i n  a d d r e s s i n g  t h e  c u r r e n t  r e s e a r c h  g a p  b y  \\nproviding a systematic overview of how to evaluate \\nRAG pipelines comprehensively, offering insights \\ninto the development of robust evaluation \\ndimensions, metrics and possible datasets (Y. Gao et \\nal., 2023; Hammond, 2024; Wang et al., 2023). \\nThe paper is structured as follows: The next \\nsection introduces RAG. Section three explains the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='dimensions, metrics and possible datasets (Y. Gao et \\nal., 2023; Hammond, 2024; Wang et al., 2023). \\nThe paper is structured as follows: The next \\nsection introduces RAG. Section three explains the \\nchosen research method. Section four presents the \\nevaluation framework. Finally, the last section \\nprovides the conclusions. \\n2 RETRIEV AL AUGMENTED \\nGENERATION \\nTraditional pre-trained LLMs such as GPT and BERT \\nencode knowledge within their parameters, but \\nstruggle with tasks requi ring specific factual \\nknowledge which is not present in their parameters \\n(Y. Gao et al., 2023; Lewis et al., 2020). This problem \\nis evident in the fact that even the most powerful \\nmodels such as GPT-4 struggle with made-up facts \\nknown as hallucinations, a lack of ability to self-\\nupdate and limited context (Bubeck et al., 2023; \\nOpenAI et al., 2023). Even further increasing the size \\nof their parameters, i.e., the training dataset, in which \\nthe knowledge appears to be stored to include more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='OpenAI et al., 2023). Even further increasing the size \\nof their parameters, i.e., the training dataset, in which \\nthe knowledge appears to be stored to include more \\ninformation will be likely insufficient to address the \\nissue of long-tail knowledge (Kandpal et al., 2022; \\nMallen et al., 2022). \\nTherefore, Lewis et al. (2020) proposed RAG by \\ncombining non-parametric memory with parametric \\nmemory that uses a dense vector index of external \\ndocuments such as Wikipedia articles that can be \\ndynamically accessed using a retriever. Research \\nresults comparing the performance of RAG LLM \\nwith standalone LLM, suggest for the former superior \\nperformance (Asai et al., 2023; Izacard et al., 2022; \\nLewis et al., 2020; Ma et al., 2023; Mallen et al., \\n2022; Wang et al., 2023). \\nA naïve RAG pipeline involves three main steps. \\nFirstly, the documents containing specific \\ninformation are indexed (L ewis et al., 2020). The \\nmost common method is to split the documents into'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='A naïve RAG pipeline involves three main steps. \\nFirstly, the documents containing specific \\ninformation are indexed (L ewis et al., 2020). The \\nmost common method is to split the documents into \\nsmaller sections so-called chunks and store their \\nembedding in a vector database (Y. Gao et al., 2023). \\nI n  t h e  s e c o n d  s t e p ,  a  g i v e n  i n p u t  q u e r y  i s  l i k e w i s e  \\nembedded and then compared with the passages in the \\nvector database by calcu lating the similarity, \\nreturning a set of top-ranked chunks that are most \\nrelevant for the query (Y. Gao et al., 2023; Karpukhin \\net al., 2020). In the final step, the retrieved content \\nand the query are combined and prompted into an \\nLLM so that it can provide a coherent answer (Y. Gao \\net al., 2023; Lewis et al., 2020). \\nThis naïve setup can be modified by applying \\ndifferent advanced methods relating to pre- or post-\\nretrieval (Asai et al., 2023; Y. Gao et al., 2023; Ma et'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='et al., 2023; Lewis et al., 2020). \\nThis naïve setup can be modified by applying \\ndifferent advanced methods relating to pre- or post-\\nretrieval (Asai et al., 2023; Y. Gao et al., 2023; Ma et \\nal., 2023). For instance, Ma et al. (2023) propose \\nquery rewriting as an advanced pre-retrieval method \\nand report performance improvements.  \\nDespite the use of advanced methods, the RAG \\napproach can still be divided into the outlined steps of \\na naïve RAG for evaluation. However, there is a lack \\nof evaluation dimensions and metrics on how to \\nanalyse and assess such systems, e.g., which evaluation \\ndimensions to consider and what kind of metrics to \\ncalculate for which step (Y. Gao et al., 2023). \\n3 RESEARCH METHOD \\nThe Systematic Literature Review (SLR) is a well-\\nknown and established research method within \\nInformation Systems (IS)  research for reviewing \\nscientific articles based on a search process (Bell et \\nal., 2019; Paré et al., 2016). The term \"systematic\"'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Information Systems (IS)  research for reviewing \\nscientific articles based on a search process (Bell et \\nal., 2019; Paré et al., 2016). The term \"systematic\" \\nmeans that the research steps should be \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n138'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='understandable, reproducible, and grounded in a \\nstructured process that minimizes potential researcher \\nbias by providing a clear audit trail for decisions and \\nconclusions (Bell et al., 2019). SLR is especially \\nvaluable when summarizing and comparing \\nfragmented knowledge on a certain topic (Bell et al., \\n2019). Existing literature reveals a significant gap in \\ncomprehensive evaluation frameworks for assessing \\nRAG systems (Y. Gao et al., 2023; Wang et al., 2023). \\nTherefore, given the emerging and unexplored \\nresearch on evaluation dimensions for RAG, the SLR \\nis particularly appropriate. \\nThis paper ensures rigorous documentation by \\nusing the literature search process proposed by \\nBrocke et al. (2009), extended with the \\nrecommendations of Paré et al. (2016). It also follows \\nthe recommendation of Webster and Watson (2002) \\nto use a concept matrix for structuring and comparing \\nthe results. The complete adopted literature search \\nprocess is illustrated in Figure 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='the recommendation of Webster and Watson (2002) \\nto use a concept matrix for structuring and comparing \\nthe results. The complete adopted literature search \\nprocess is illustrated in Figure 1. \\n \\n \\nFigure 1: Systematic Literature Review process. \\nThe search process incl uded conducting a pre-\\nstudy on Google Scholar by a detailed screening of \\ntitles, abstracts and full texts to identify papers \\nspecifically addressing evaluation metrics for RAG \\nsystems (Y. Gao et al., 2023; Wang et al., 2023). \\nThese insights resulted in the following search string: \\n \\n• TITLE-ABS-KEY (“Retrieval Augmented \\nGeneration” AND “Evaluation Metric”) \\n \\nThe following inclusion and exclusion criteria \\nwere applied to filter relevant papers from the search \\nresults in Google Scholar, Scopus and the IS \\nconferences ICIS and ECIS: \\nInclusion Criteria: \\n• Papers from academic journals, conferences, \\nor gray literature.  \\n• Published in English. \\n \\nExclusion Criteria: \\n• Duplicates across databases.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='conferences ICIS and ECIS: \\nInclusion Criteria: \\n• Papers from academic journals, conferences, \\nor gray literature.  \\n• Published in English. \\n \\nExclusion Criteria: \\n• Duplicates across databases. \\n• Minimal relevance to evaluation metrics, or \\nlack of focus on RAG systems. \\n• No guidelines on metric implementation or \\napplication at the different RAG steps. \\n \\nThe search process commen ced with an abstract \\nscreening of the initial results, followed by a full-text \\nreview of the selected papers. Forward and backward \\ncitation searches were subsequently performed on \\nrelevant studies to identify additional literature. This \\ncomprehensive approach yielded a final sample of 12 \\npapers, as illustrated in Figure 1. \\nIn the final step, overarching categories from the \\nfinal sample were synthesized into a concept matrix \\n(Webster & Watson, 2002). Relevant concepts on \\nevaluation dimensions were identified and mapped to \\nthe RAG steps (cf. Section 2) to provide an accurate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(Webster & Watson, 2002). Relevant concepts on \\nevaluation dimensions were identified and mapped to \\nthe RAG steps (cf. Section 2) to provide an accurate \\noverview on how to evaluate each RAG phase. \\nThe concept matrix is shown in Table 1. It \\ncategorizes the sampled papers based on predictive \\nevaluation criteria and dataset characteristics. The \\nformer includes the columns \"Retrieval\" and \\n\"Generation\" relating to the RAG steps. The \\n\"Evaluator\" column indi cates whether lexical \\nmatching, semantic similarity or LLM as a judge is \\nused for evaluation. The dataset characteristics \\ninclude single-hop and multi-hop reasoning tasks, \\nsynthetic datasets (triples), and open-domain question \\nanswering (QA). Each paper is marked (\"X\") to show \\nthe criteria it addresses, p roviding a comprehensive \\noverview of their focus areas. In addition, the \\nfrequency of occurrence in the literature can be used \\nto determine how widespread or accepted a metric is.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='overview of their focus areas. In addition, the \\nfrequency of occurrence in the literature can be used \\nto determine how widespread or accepted a metric is. \\nEach proposed metric for the evaluation dimensions \\ndepending on the evaluator and the requirements for \\nthe data to calculate it are summarized in Table 2. \\n4  RESULTS \\nThis section starts with examining the first column of \\nthe concept matrix: predictive evaluation , which \\ninvolves assessing the performance of the RAG \\nsystem in retrieving accurat e context and effectively \\nutilizing it to generate responses (Guinet et al., 2024).  \\n \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n139'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 1: Concept Matrix with Evaluation Dimensions and Datasets. \\n \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n140'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 2: Summary of proposed Evaluation Dimensions, the corresponding Metrics and Dataset Requirements. \\n \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n141'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='The proposed evaluation pipeline outlines (cf. \\nFigure 2) the process of assessing the RAG approach \\nacross various evaluation dimensions and focuses on \\nthe retrieval and generation stages of a typical RAG \\nsystem. The evaluation pr ocess starts with the \\nretrieval step, emphasizing context relevance as a \\ncritical dimension to assess how effectively relevant \\ninformation is retrieved. \\nSubsequently, the focus then shifts to the \\ngeneration step , examining the evaluation \\ndimensions of answer relevance, correctness, \\nfaithfulness, a n d  citation quality to determine the \\naccuracy and reliability of the generated responses. \\nEach evaluation dimension is carefully defined, and \\nquantifying metrics are proposed according to the \\nsampled papers.   \\nHow and what type of metric is ultimately used to \\ncalculate the respective evaluation dimension \\ndepends on the chosen evaluator. Lexical matching \\nmetrics focus on exact word matching and simple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='How and what type of metric is ultimately used to \\ncalculate the respective evaluation dimension \\ndepends on the chosen evaluator. Lexical matching \\nmetrics focus on exact word matching and simple \\nstatistical calculations, such as keyword frequency or \\nposition-based measures like Mean Reciprocal Rank \\n(MRR), i.e., these metrics assess how closely the \\nwords in the documents match the query without \\nconsidering deeper meanings.  \\nSemantic similarity metrics, on the other hand, \\ngo beyond surface-level text comparison to \\nunderstand the underlying meanings and concepts by \\ncomparing their semanti cal similarity based on \\ncontext and conceptual relationships between words \\nand sentences. This approac h captures the intent of \\nthe query and the documents, evaluating relevance \\nthrough semantic similarity rather than just keyword \\noccurrence.  \\nFinally, LLM as a judge uses a LLM to evaluate \\ncontent by making it context-aware, prompting the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='through semantic similarity rather than just keyword \\noccurrence.  \\nFinally, LLM as a judge uses a LLM to evaluate \\ncontent by making it context-aware, prompting the \\nmodel to consider the coherence, factuality, and \\nrelevance of the information based on its \\ncomprehensive understanding of language and \\nknowledge. Therefore, the choice of the evaluator \\ndetermines the type of metric applied to assess each \\nevaluation dimension, depending on whether the \\nfocus is on exact word matching, conceptual \\nsimilarity, or a nuanced, context-aware judgment by \\nan advanced LLM.  \\nThe upcoming sub-sections follows the rationale \\nof first explaining the evaluation dimension and then \\nthe respective evaluator by detailing how to calculate \\nthe metric proposed in the sampled papers. \\n4.1 Context Relevance \\nThe evaluation dimension of context relevance  \\npertains to the retrieval step and assesses the degree \\nto which the retrieved context contains only the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='4.1 Context Relevance \\nThe evaluation dimension of context relevance  \\npertains to the retrieval step and assesses the degree \\nto which the retrieved context contains only the \\nnecessary information to answer the query, reducing \\ncomputational costs and improving efficiency by \\nminimizing irrelevant content (Es et al., 2023; Saad-\\nFalcon et al., 2023; Yu et al., 2024). Additionally, \\nwhen retrieved passages are too    LLMs often \\nstruggle to effectively utilize the information, \\nparticularly if the relevant details are embedded in the \\nm i d d l e  o f  t h e  p a s s a g e  ( E s  e t  a l . ,  2 0 2 3 ) .  H e n c e ,  \\nconcise query-relevant passages significantly \\nimprove the LLM generation quality (Es et al., 2023; \\nYu et al., 2024).  \\nRecall@k and MRR@k a r e  k e y  lexical metrics \\nfor evaluating the retrieval performance in RAG \\nsystems (Rackauckas et al., 2024; Yu et al., 2024). \\nEach metric provides a different perspective on the \\neffectiveness'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='for evaluating the retrieval performance in RAG \\nsystems (Rackauckas et al., 2024; Yu et al., 2024). \\nEach metric provides a different perspective on the \\neffectiveness\\n of the retrieval process. Recall@k \\n \\nFigure 2: Evaluation Pipeline with Evaluation Dimensions in a naïve RAG setup. \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n142'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='measures how many relevant passages are captured \\nwithin the top k retrieved chunks, even if some \\nirrelevant ones are included. The formula is as follows:  \\n \\nRecall@k = |Relevant Passages ∩ k-Passages|\\n|Relevant Passages|  (1)\\n \\nMRR@k calculates context relevance by \\nemphasizing the rank of the first relevant passage \\nacross multiple queries (Rackauckas et al., 2024). If a \\nrelevant passage appears in the top k results, its \\ncontribution to the MRR@k score is the inverse of its \\nrank, e.g., a passage ranked two contributes as ½ with \\nMRR@5 (Rackauckas et al., 2024). If no relevant \\npassage is found in the top k the contribution is zero. \\nThe formula for MRR@k is: \\n \\nMRR@k =1\\n|Q| \\u0dcd 1\\nrank୧\\n|୕|\\n୧ୀଵ\\n (2)\\n \\nBy using the LLM as a judge , it is possible to \\ncalculate an estimated context relevance score. Given \\na query q and its retrieved context c(q), the LLM is \\nprompted to extract a subset of sentences (𝑆\\n\\u0bd8௫௧) from \\nc(q) that are relevant to answering q by using the \\nfollowing prompt:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='a query q and its retrieved context c(q), the LLM is \\nprompted to extract a subset of sentences (𝑆\\n\\u0bd8௫௧) from \\nc(q) that are relevant to answering q by using the \\nfollowing prompt: \\n \\n\"Please extract relevant sentences from the \\nprovided context that can potentially help answer the \\nfollowing question. If no relevant sentences are \\nfound, or if you believe the question cannot be \\nanswered from the given context, return the phrase \\n\\'Insufficient Information\\'. While extracting candidate \\nsentences, you\\'re not allowed to make any changes to \\nsentences from the given context.\" (Es et al., 2023) \\n \\nThe prompt instructs the LLM to select only the \\nsentences that it considers relevant to q without \\nchanging the content. The Context Relevance Score \\n(CRS) is calculated by dividing the relevant \\nsentences extracted (𝑆\\n\\u0bd8௫௧) from the context c(q) by the \\ntotal number of sentences. This can be expressed with \\nthe following formula (Es et al., 2023): \\n \\nCRS = Number of extracted sentences Sୣ୶୲'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"sentences extracted (𝑆\\n\\u0bd8௫௧) from the context c(q) by the \\ntotal number of sentences. This can be expressed with \\nthe following formula (Es et al., 2023): \\n \\nCRS = Number of extracted sentences Sୣ୶୲\\nTotal number of sentences in c(q)  (3)\\n \\nThe CRS indicates the proportion of the context \\nthat is relevant. A higher score indicates that a greater \\nproportion of the retrieved context is focused and \\nrelevant for answering the query, while a lower score \\nindicates that much of the retrieved context contains \\nirrelevant information (Es et al., 2023; Saad-Falcon et \\nal., 2023; Yu et al., 2024). \\n4.2 Faithfulness \\nThe evaluation dimensions faithfulness refers to the \\ngeneration step and evaluates how well an LLM's \\nresponse is grounded in the retrieved context, i.e., all \\ninformation in the response  can be directly inferred \\nfrom it (Adlakha et al., 2023; Es et al., 2023; Hu et \\nal., 2024). This evaluation dimension is crucial to \\nidentify possible hallucinations in the answer of\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='from it (Adlakha et al., 2023; Es et al., 2023; Hu et \\nal., 2024). This evaluation dimension is crucial to \\nidentify possible hallucinations in the answer of \\nLLMs ensuring factual correctness (Adlakha et al., \\n2023; Es et al., 2023; Ravi et al., 2024). For instance, \\nAdlakha et al. (2023) found that GPT-4 had the \\nhighest agreement with human annotations, followed \\nby GPT-3.5 and k-precision.  \\nAs the primary lexical metric  Adlakha et al. \\n(2023) propose k-precision to evaluate the degree of \\nfaithfulness since it has the highest agreement with \\nhuman judgements. It can be calculated as the \\nproportion of tokens in the LLMs response that are \\npresent in the retrieved context, i.e., it is the overlap of \\nmatching tokens with the retrieved context divided by \\nthe total number of tokens in the response (Adlakha et \\nal., 2023). Hence, the formula is as follows: \\n \\nK-Precision = Matched Tokens\\nResponse Tokens (4)\\n \\nAnother way of calculating the faithfulness is by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='al., 2023). Hence, the formula is as follows: \\n \\nK-Precision = Matched Tokens\\nResponse Tokens (4)\\n \\nAnother way of calculating the faithfulness is by \\nusing LLMs such as GPT-4/3.5 as evaluators. The \\nLLMs are prompted to judge whether the response \\nand the retrieved context match from an ordinal scale \\nof “fully”, “partially”, or “not at all” (Adlakha et al., \\n2023). In the same way, Ravi et al. (2024) propose to \\nassess the responses through an evaluator LLM \\nwhether the responses are supported, contradicted or \\nnot supported by the retrieved context. In order to \\nquantify this ordinal scale, we can assign numerical \\nscores depending on the degree of support and take \\nthe average of all individu al response scores, i.e., \\nassign 1 for “fully”, 0.5 for “partially”, and 0 for “not \\nat all”. The formula for calculating the Faithfulness \\nCoefficient (FC) is as follows: \\n \\nFC = ∑ Faithfulness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (5)\\n \\nA similar method is used by Hu et al. (2024), who'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Coefficient (FC) is as follows: \\n \\nFC = ∑ Faithfulness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (5)\\n \\nA similar method is used by Hu et al. (2024), who \\npropose a framework that e xtracts “claim triplets” \\n(subject, predicate, object) to represent fine-grained \\nknowledge assertions within the LLM response. The \\npurpose of this extraction is to break down the answer \\ninto specific atomic cla ims that can be checked \\nindividually (Hu et al., 2024; Min et al., 2023). A \\njudging LLM evaluates each triplet as “entailment” \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n143'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(supported), “contradiction” (contradicted), or \\n“neutral” (unsupported) (Hu et al., 2024). Other \\nauthors also follow this approach of breaking down \\nthe statements from the LLM response into atomic \\nfacts to obtain a fine-grained measure of the \\nfaithfulness degree (Min et al., 2023). \\nEs et al. (2023) propose a process in which the \\nanswer 𝑎\\n௦(𝑞)  is considered faithful to the context \\n𝑐(𝑞) if the statements in the LLM response can be \\ndirectly inferred from the retrieved context. The \\nprocess begins by using a LLM as a judge  t o  \\ndecompose the LLM response into a set of statements, \\n𝑆(𝑎௦(𝑞)) , which involves breaking down longer \\nsentences into shorter ones (Es et al., 2023). For each \\nstatement 𝑠\\u0bdc in 𝑆, the judging LLM verifies if it can \\nbe inferred from the given context 𝑐(𝑞)  using a \\nverification function 𝑣(𝑠\\u0bdc,𝑐 (𝑞)) (Es et al., 2023). The \\njudging LLM assesses whether each statement is \\nsupported by the information in the retrieved context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='verification function 𝑣(𝑠\\u0bdc,𝑐 (𝑞)) (Es et al., 2023). The \\njudging LLM assesses whether each statement is \\nsupported by the information in the retrieved context \\nand provides a “yes” or “no” verdict for each \\nstatement (Es et al., 2023). The Faithfulness Score \\n(𝐅𝐒) is then calculated as the  ratio of the number of \\nsupported statements 𝑉  to the total number of \\nstatements S, which can be expressed as follows: \\n \\nFS = |V|\\n|S| (6)\\n4.3 Answer Relevance \\nThe evaluation dimensions answer relevance refers \\nto the generation step and assesses whether the LLM \\nr e s p o n s e  i s  d i r e c t l y  a d d r e s s i n g  t h e  q u e r y  ( E s  e t  a l . ,  \\n2023; Rackauckas et al., 2024; Saad-Falcon et al., \\n2023; Yu et al., 2024). This evaluation dimension \\npenalizes incomplete or redundant answers, \\nregardless of factuality (Es et al., 2023; Rackauckas \\net al., 2024). \\nBy using LLM as a judge  it is possible to \\ncalculate an estimation of the answer relevance (Es et'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='regardless of factuality (Es et al., 2023; Rackauckas \\net al., 2024). \\nBy using LLM as a judge  it is possible to \\ncalculate an estimation of the answer relevance (Es et \\nal., 2023; Rackauckas et al., 2024; Saad-Falcon et al., \\n2023). Given a generated answer 𝑎\\n௦(𝑞), the judging \\nLLM is prompted to generate 𝑛 potential questions 𝑞\\u0bdc \\nthat could be answered by using 𝑎௦(𝑞)  (Es et al., \\n2023). This is done using the following prompt: \\n \\nGenerate a question for the given answer: \\nanswer:[answer] (Es et al., 2023) \\n \\nSubsequently, text embeddings for all generated \\nquestions 𝑞\\u0bdc  and the original query 𝑞 are created to \\ncalculate in the next step the cosine similarity \\nbetween their embeddings (Es et al., 2023). The \\nAnswer Relevance Score (ARS)  is obtained by \\naveraging the similarity  between the generated \\nquestions 𝑞\\n\\u0bdc  and the original query 𝑞  using the \\nfollowing formula: \\n \\nARS = 1\\nn \\u0dcds i m(q, q୧)\\n୬\\n୧ୀଵ\\n (7)\\n \\nwhere 𝑠𝑖𝑚(𝑞, 𝑞\\u0bdc) represents the cosine similarity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='questions 𝑞\\n\\u0bdc  and the original query 𝑞  using the \\nfollowing formula: \\n \\nARS = 1\\nn \\u0dcds i m(q, q୧)\\n୬\\n୧ୀଵ\\n (7)\\n \\nwhere 𝑠𝑖𝑚(𝑞, 𝑞\\u0bdc) represents the cosine similarity \\nbetween the embeddings of the generated questions \\nq୧  and the original query q (Es et al., 2023). This \\nmetric effectively measures how well the generated \\nanswer matches the intent and content of the original \\nquestion (Es et al., 2023). \\n4.4 Correctness \\nThe evaluation dimension correctness r e f e r s  t o t he  \\ngeneration step and evaluates whether the LLM’s \\nresponse accurately matches the “golden passage” \\nprovided by human annotators (Adlakha et al., 2023; \\nT .  G a o  e t  a l . ,  2 0 2 3 ) .  T h i s  m e t r i c  focuses on the \\nfactual accuracy  of the information by comparing \\nthe LLM response with a reference answer (Adlakha \\net al., 2023; T. Gao et al., 2023; Guinet et al., 2024; \\nRackauckas et al., 2024). \\nAs a primary lexical metric  Adlakha et al. \\n(2023) propose using recall as it correlates well with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"et al., 2023; T. Gao et al., 2023; Guinet et al., 2024; \\nRackauckas et al., 2024). \\nAs a primary lexical metric  Adlakha et al. \\n(2023) propose using recall as it correlates well with \\nhuman annotations. Traditional metrics like Exact \\nMatch (EM), F1, and ROUGE are often too strict due \\nto their focus on exact word matching (Adlakha et al., \\n2023). Recall measures how much of the reference \\nanswer's essential content is captured in the model's \\nresponse without penalizing additional information \\n(Adlakha et al., 2023). \\nSome authors find that semantic similarity  \\nmetrics like BERTScore is less effective than recall \\nfor correctness due to lower alignment with human \\nannotations (Adlakha et al., 2023; T. Gao et al., \\n2023). These metrics do not account for factual \\naccuracy or logical consistency, as responses can be \\ntextually similar but factually incorrect (Adlakha et \\nal., 2023; T. Gao et al., 2023). \\nBy using LLMs such as GPT-3.5 and GPT-4 as\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='accuracy or logical consistency, as responses can be \\ntextually similar but factually incorrect (Adlakha et \\nal., 2023; T. Gao et al., 2023). \\nBy using LLMs such as GPT-3.5 and GPT-4 as \\nevaluators to judge the correctness of responses by \\nprompting them with the question, the reference \\nanswer, and the LLMs response to determine whether \\nthe model response is correct, partially correct, or \\nincorrect (Adlakha et al., 2 023; Rackauckas et al., \\n2024). This approach seems to yield the highest \\nagreement with human annotations (Adlakha et al., \\n2023; Rackauckas et al., 2024). Correctness can be \\nquantified by assigning scores: 1 for “fully correct”, \\n0.5 for “partially”, and 0 for “incorrect”. The \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n144'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Correctness Coefficient (CC)  is the average of all \\nresponse scores. Therefore, the formula is as follows: \\n \\nCC = ∑  Correctness Score୧\\n\\u0b52\\n୧ୀଵ\\nTotal number of responses  (8)\\n4.5 Citation Quality \\nThe evaluation dimension citation quality focuses on \\nassessing whether an LLM correctly cites its sources \\nwhen generating text (T. Gao et al., 2023). Citation \\nquality is calculated using two metrics: citation \\nrecall and citation precision. Citation recall ensures \\nthat every piece of information in the generated \\nresponse is fully supported by the cited passages, \\nwhile citation precision checks whether all cited \\npassages are relevant and necessary for the statements \\nmade (T. Gao et al., 2023).  \\nTo perform this evaluation automatically, the \\nLLM acts as a judge, which is prompted with a chain-\\nof-thought method instead of simple lexical matching \\n(T. Gao et al., 2023). The LLM checks if the \\nconcatenated text from the cited passages semantically'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='of-thought method instead of simple lexical matching \\n(T. Gao et al., 2023). The LLM checks if the \\nconcatenated text from the cited passages semantically \\nsupports the generated statements (T. Gao et al., 2023). \\nFor citation recall, it evaluates whether all generated \\nstatements are substantiated by the citations. A \\nstatement receives a recall score of 1 if it is fully \\nsupported by at least one citation, otherwise, it receives \\na score of 0 (T. Gao et al., 2023). For citation precision, \\nthe LLM identifies any \"irrelevant\" citations, i.e., those \\nthat do not independently support a statement or are not \\nnecessary when other citations already provide full \\nsupport (T. Gao et al., 2023) . A citation receives a \\nprecision score of 1 if it is relevant and contributes to \\nthe statement\\'s support and 0 if it is irrelevant (T. Gao \\net al., 2023). \\nThe robustness of these metrics is validated by \\ntheir strong correlation  with human judgements'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"the statement's support and 0 if it is irrelevant (T. Gao \\net al., 2023). \\nThe robustness of these metrics is validated by \\ntheir strong correlation  with human judgements  \\n(T. Gao et al., 2023). By averaging the citation recall \\nand precision scores across all statements and \\ncitations in the generated response, an overall citation \\nquality score can be calculated, providing a \\ncomprehensive measure of how accurately and \\nappropriately an LLM uses citations in its output. \\n5 DATASETS & APPLICATION \\nBuilding on the concept matrix's evaluation \\ndimensions presented above, it's essential to consider \\nhow different datasets relate to the evaluators of these \\ndimensions and its corresponding evaluation metrics. \\nThe choice between open-domain QA datasets and \\nsynthetic datasets like RAGAS or ARES, along with \\nthe type of reasoning required (single-hop vs. multi-\\nhop), plays a crucial role in ensuring the robustness \\nand reliability of these evaluations.\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='synthetic datasets like RAGAS or ARES, along with \\nthe type of reasoning required (single-hop vs. multi-\\nhop), plays a crucial role in ensuring the robustness \\nand reliability of these evaluations. \\nOpen-domain QA datasets such as SQuAD2.0, \\nHotpotQA, and Natural Questions are based on \\nWikipedia articles. These are mainly suitable for \\nlexical matching  enable comparisons across RAG \\nsystems (Kwiatkowski et al., 2019; Rajpurkar et al., \\n2018; Yang et al., 2018). These datasets often include \\n“golden passages” which make them ideal for \\nevaluating correctness and faithfulness by providing \\na factual reference (Kwiatkowski et al., 2019; \\nRajpurkar et al., 2018; Yang et al., 2018). For \\ninstance, SQuAD2.0 includes unanswerable queries \\nrequiring RAG systems to recognize when there is \\ninsufficient information to provide a valid answer \\n(Rajpurkar et al., 2018; Rau et al., 2024). Also \\ncalculating context relevance  is straightforward \\nbecause the datasets provide clear ground truth in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(Rajpurkar et al., 2018; Rau et al., 2024). Also \\ncalculating context relevance  is straightforward \\nbecause the datasets provide clear ground truth in \\nterms of which passages are relevant, making them \\nideal for recall-oriented metrics like Recall@k o r  \\nMRR@k (Adlakha et al., 2023; Hu et al., 2024). \\nHowever, evaluating answer relevance and citation \\nquality is more challenging with open-domain QA \\ndatasets since these typically focus on finding a single \\ncorrect answer rather than assessing nuanced citation \\npractices or multi-source relevance (Kwiatkowski et \\nal., 2019; Rajpurkar et al., 2018; Yang et al., 2018). \\nSynthetic datasets such as RAGAS and ARES \\nare specifically designed to evaluate the effectiveness \\nof RAG systems by minimizing reliance on human \\nannotations (Es et al., 2023; Saad-Falcon et al., 2023). \\nThese frameworks often use synthetic datasets that \\nonly require query-context-response triples, making \\nthem suitable to evaluate every evaluation dimension'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='These frameworks often use synthetic datasets that \\nonly require query-context-response triples, making \\nthem suitable to evaluate every evaluation dimension  \\n( E s  e t  a l . ,  2 0 2 3 ;  H u  e t  a l . ,  2 0 2 4 ;  M i n  e t  a l . ,  2 0 2 3 ;  \\nSaad-Falcon et al., 2023). Synthetic datasets \\ncombined with LLM judges align well with human \\nannotations, outperforming lexical and semantic \\nsimilarity metrics (Adlakha et al., 2023; Saad-Falcon \\net al., 2023). Additionally, this approach is model-\\nagnostic, allowing flexible use across different LLMs \\nand setups (Es et al., 2023; Saad-Falcon et al., 2023). \\nThis adaptability ensures that RAG systems can \\nbe effectively assessed and fine-tuned for diverse and \\ncomplex queries, enhanci ng their performance in \\npractical, real-world setting. Table 3 summarizes the \\nkey differences between the datasets by comparing \\nthem. \\nIn terms of reasoning, single-hop and multi-hop \\nqueries require different approaches and datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='key differences between the datasets by comparing \\nthem. \\nIn terms of reasoning, single-hop and multi-hop \\nqueries require different approaches and datasets \\nSingle-hop reasoning involves deriving an answer \\nfrom\\n a single piece of evidence, i.e., one retrieved  \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n145'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Table 3: Comparing Open Domain QA Datasets with Synthetic Datasets for evaluating RAG. \\nAspect O pen Domain QA Dataset S ynthetic Dataset\\nExamples SQuAD2.0, HotpotQA, Natural Questions, \\nQAMPARI RAGAS, ARES \\nReasoning Type \\nSingle-hop (e.g., SQuAD2.0, Natural \\nQuestions) \\nMulti-hop (e.g., HotpotQA, QAMPARI)\\nSingle-hop and adaptable to Multi-hop \\nEvaluation \\nDimensions \\nCorrectness, Faithfulness, Context Relevance \\n(basic) \\nCorrectness, Faithfulness, Context Relevance (multi-\\nretrieval), Answer Relevance, Citation Quality \\nEvaluators Lexical Matching Semantic S imilarity, LLM as a Judge \\nStrengths High reliability for correctness and \\nfaithfulness due to golden-passages \\nModel and vendor agnostic, adaptable for various \\nqueries and rapid evaluation of RAG without the need \\nfor human annotations or gold-passages \\nLimitations \\nLess effective in evaluating multi-source \\ncitations, complex context relevance, and \\nanswer relevance in multi-hop'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='for human annotations or gold-passages \\nLimitations \\nLess effective in evaluating multi-source \\ncitations, complex context relevance, and \\nanswer relevance in multi-hop\\nAssociated costs related to token usage and potential \\nlatency due to LLM judging, different performances \\ndepending on the employed LLM model \\n \\npassage, and is well-suited for the evaluation \\ndimensions correctness, faithfulness, and basic \\ncontext relevance . Popular single-hop datasets are \\nNatural Questions (Kwiatkowski et al., 2019) and \\nSQuAD2.0 (Rajpurkar et al., 2018), where the \\nrelevant information is contained within a single \\npassage, allowing the calculation of metrics \\npredominantly with lexical matching or simple \\nsemantic similarity, e.g., focusing on metrics such as \\nprecision and recall (Adlakha et al., 2023; Ravi et al., \\n2024). In contrast, multi-hop reasoning requires to \\nconnect multiple pieces of retrieval, usually from \\ndifferent documents or distant parts of the same'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='2024). In contrast, multi-hop reasoning requires to \\nconnect multiple pieces of retrieval, usually from \\ndifferent documents or distant parts of the same \\ndocument, to be combined in order to obtain a correct \\nanswer. This approach is better suited for evaluating \\ncontext relevance  for multiple retrieval, answer  \\nrelevance, i.e., how the combined context informs the \\nanswer, and citation quality that correctly attributes \\ninformation to multiple sources (Adlakha et al., 2023; \\nEs et al., 2023; T. Gao et al., 2023; Hu et al., 2024). \\nOpen-domain QA datasets like HotpotQA (Yang et \\nal., 2018) or QAMPARI (Amouyal et al., 2022) are \\nspecifically designed for multi-hop reasoning. These \\nrequire synthesizing information from multiple \\nretrieved contexts, which involves understanding \\ncomplex connections and contextual relevance that \\ngo beyond surface-level comparisons. Employing \\nLLMs as a judge for this evaluation is the most \\nsuitable option since LLMs can comprehend the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='complex connections and contextual relevance that \\ngo beyond surface-level comparisons. Employing \\nLLMs as a judge for this evaluation is the most \\nsuitable option since LLMs can comprehend the \\ncombination of multiple contexts better by making \\nmore nuanced judgement than simple lexical \\nmatching or semantical similarity of concepts (Es et \\nal., 2023; T. Gao et al., 2023; Min et al., 2023; Saad-\\nFalcon et al., 2023). \\n \\n \\n6 PRACTICAL APPLICATION \\nIn order to apply evaluation dimensions and select the \\nappropriate datasets, it is necessary to understand \\ndataset requirements. For instance, the evaluation \\ndimension faithfulness requires data regarding the \\nretrieved passages of the RAG to be tested and a \\nreference retrieval (golden retrieval), but these are \\noften not available in real-world operations. \\nTherefore, synthetic datas ets that are applicable \\nreference-free would be more suitable for practical \\noperation, and they also have a high alignment with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Therefore, synthetic datas ets that are applicable \\nreference-free would be more suitable for practical \\noperation, and they also have a high alignment with \\nhuman annotations (Es et al., 2023; Saad-Falcon et \\nal., 2023).  \\nIt is also necessary to select suitable metrics \\naccording to the objective of optimizing the RAG \\nsystem. For this purpose, a distinction should first be \\nmade as to whether the performance of the retrieval \\nor generation step should be considered. A suitable \\nmetric is then selected according to a specific \\nproblem. For example, if the factuality of the answer \\nis to be increased, correctness is a more suitable \\nmetric than faithfulness.  \\nFinally, it is important to build an automated, \\nrobust and reliable evaluation pipeline that can be \\nused to evaluate the RAG system (Es et al., 2023). \\n7 CONCLUSION \\nThis paper proposes a comprehensive evaluation \\nframework specifically for RAG by conducting an \\nSLR and providing an extensive overview of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='7 CONCLUSION \\nThis paper proposes a comprehensive evaluation \\nframework specifically for RAG by conducting an \\nSLR and providing an extensive overview of \\ncurrently existing evaluati on approaches. Since the \\nintroduction of RAG in 2020 (Lewis et al., 2020), it \\nhas taken considerable time for methods to be \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n146'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='developed and established in the literature for \\nevaluating these approaches. Despite the rigorous \\nmethodology, it remains possible that some papers \\nwere overlooked due to the rapid pace of \\ndevelopments in the field.  \\nWith reference to RQ1, our evaluation framework \\nintroduces robust evaluation dimensions and metrics \\nto assess the different steps within RAG. Moreover, \\nthis paper advances the understanding of RAG \\nevaluation by providing reliable dimensions and \\nmetrics (Y. Gao et al., 2023; Wang et al., 2023). \\nFurthermore, Section 5 gives a comprehensive \\nsummary about RQ2 by providing an overview of the \\navailable datasets to apply the proposed evaluation \\ndimensions and metrics and what kind of \\nrequirements to consider.  \\nAs a future avenue of research, a practical \\napplication of these metrics should be conducted to \\nvalidate their use and alignment with human \\npreferences. In addition, the provision of all the \\nmetrics presented could be examined within a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='application of these metrics should be conducted to \\nvalidate their use and alignment with human \\npreferences. In addition, the provision of all the \\nmetrics presented could be examined within a \\nframework to simplify practical application. \\nACKNOWLEDGEMENTS \\nThe presented paper was produced as part of the \\nresearch project MoFaPro. This project is funded by \\nthe AUDI AG. The present  approach was developed \\nwithin the institute \"AImotion Bavaria\" at the \\nTechnische Hochschule Ingolstadt. This work is part \\nof Oğuz Caymazer\\'s master\\'s thesis and was \\nconducted during his internship at the AUDI AG. \\nREFERENCES \\nAdlakha, V., Behnamghader, P., Lu, x. H., Meade, n., & \\nReddy, S. (2023). Evaluating correctness and \\nfaithfulness of instruction-following models for \\nquestion answering. Https://doi.org/10.48550/arxiv.2 \\n307.16877 \\nAmouyal, S. J., Wolfson, T ., Rubin, O., Yoran, O., \\nHerzig, J., & Be rant, J. (2022). QAMPARI: An Open-\\ndomain Question Answering Benchmark for Questions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='307.16877 \\nAmouyal, S. J., Wolfson, T ., Rubin, O., Yoran, O., \\nHerzig, J., & Be rant, J. (2022). QAMPARI: An Open-\\ndomain Question Answering Benchmark for Questions \\nwith Many Answers from Multiple Paragraphs. \\nhttps://doi.org/10.48550/arXiv.2205.12665 \\nAsai, A., Wu, Z., Wang, Y [Yizhong], Sil, A., & \\nHajishirzi, H. (2023). Self-RAG: Learning to Retrieve, \\nGenerate, and Critique through Self-Reflection. \\nhttps://doi.org/10.48550/arXiv.2310.11511 \\nAsai, A., Zhong, Z., Chen, D [Danqi], Koh, P. W., \\nZettlemoyer, L., Hajishirzi, H., & Yih, W. (2024). \\nReliable, Adaptable, and Attributable Language \\nModels with Retrieval. https://doi.org/10.48 \\n550/arXiv.2403.03187 \\nBell, E., Bryman, A., & Harley, B. (2019). Business \\nresearch methods  (Fifth edition). Oxford University \\nPress.  \\nBenbya, H., Strich, F., & Ta mm, T. (2024). Navigating \\nGenerative Artificial Intelligence Promises and Perils \\nfor Knowledge and Creative Work. Journal of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Press.  \\nBenbya, H., Strich, F., & Ta mm, T. (2024). Navigating \\nGenerative Artificial Intelligence Promises and Perils \\nfor Knowledge and Creative Work. Journal of the \\nAssociation for Information Systems , 25(1), 23–36. \\nhttps://doi.org/10.17705/1jais.00861 \\nBrocke, J., Simons, A., Niehaves, B., Riemer, K., \\nPlattfaut, R., & Cleven, A. (2009). Reconstructing the \\nGiant: On the Importance of Rigour in Documenting \\nthe Literature Search Process. In European Conference \\non Information Systems (Chair), 17th European Conf. \\non Information Systems . https://www.wi.uni-\\nmuenster.de/research/publications/3069 \\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., \\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., \\nLundberg, S., Nori, H., Palangi, H., Ri beiro, M. T., & \\nZhang, Y [Yi]. (2023). Sparks of Artificial General \\nIntelligence: Early experiments with GPT-4. \\nhttps://doi.org/10.48550/arXiv.2303.12712 \\nEs, S., James, J., Espinosa-Anke, L., & Schockaert, S.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Zhang, Y [Yi]. (2023). Sparks of Artificial General \\nIntelligence: Early experiments with GPT-4. \\nhttps://doi.org/10.48550/arXiv.2303.12712 \\nEs, S., James, J., Espinosa-Anke, L., & Schockaert, S. \\n(2023). RAGAS: Automated Evaluation of Retrieval \\nAugmented Generation. https://doi.org/10.48550/a \\nrXiv.2309.15217 \\nGao, T., Yen, H., Yu, J., & Chen, D [Danqi]. (2023). \\nEnabling Large Language Models to Generate Text \\nwith Citations. https://doi.org/10.48550/arXiv.2305. \\n14627 \\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y \\n[Yi], Sun, J., Wang, M., & Wang, H. (2023). Retrieval-\\nAugmented Generation for Large Language Models: A \\nSurvey. https://doi.org/10.48550/arXiv.2312.10997 \\nGuinet, G., Omidvar-Tehrani, B., Deoras, A., & Callot, L. \\n(2024). Automated Evaluation of Retrieval-Augmented \\nLanguage Models with Task-Specific Exam \\nGeneration. https://github.com/amazon-science/auto-\\nrag-eval https://doi.org/10.48550/arXiv.2405.13622'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='(2024). Automated Evaluation of Retrieval-Augmented \\nLanguage Models with Task-Specific Exam \\nGeneration. https://github.com/amazon-science/auto-\\nrag-eval https://doi.org/10.48550/arXiv.2405.13622 \\nHammond, G. (2024, April 10). Speed of AI development \\nstretches risk assessments to breaking point. Financial \\nTimes. https://www.ft.com/content/499c8935-f46e-\\n4ec8-a8e2-19e07e3b0438 \\nHu, X [Xiangkun], Ru, D., Qiu, L., Guo, Q., Zhang, T., \\nXu, Y., Luo, Y., Liu, P., Zhang, Y [Yue], & Zhang, Z \\n[Zheng]. (2024). RefChecker: Reference-based Fine-\\ngrained Hallucination Checker and Benchmark for \\nLarge Language Models. https://doi.org/10.48550/a \\nrXiv.2405.14486 \\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., \\nPetroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., \\nRiedel, S., & Grave, E. (2022). Atlas: Few-shot \\nLearning with Retrieval Augmented Language Models. \\nhttps://doi.org/10.48550/arXiv.2208.03299 \\nKandpal, N., Deng, H., R oberts, A., Wa llace, E., &'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Riedel, S., & Grave, E. (2022). Atlas: Few-shot \\nLearning with Retrieval Augmented Language Models. \\nhttps://doi.org/10.48550/arXiv.2208.03299 \\nKandpal, N., Deng, H., R oberts, A., Wa llace, E., & \\nRaffel, C. (2022). Large Language Models Struggle to \\nLearn Long-Tail Knowledge. https://doi.org/10.48 \\n550/arXiv.2211.08411 \\nBenchmarking of Retrieval Augmented Generation: A Comprehensive Systematic Literature Review on Evaluation Dimensions, Evaluation\\nMetrics and Datasets\\n147'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Karpukhin, V., Oğuz, B., Min , S., Lewis, P., Wu, L., \\nEdunov, S., Chen, D [Danqi], & Yih, W. (2020). Dense \\nPassage Retrieval for Open-Domain Question \\nAnswering. https://doi.org/10.48550/arXiv.2004.04906 \\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., \\nParikh, A., Alberti, C., Eps tein, D., Polosukhin, I., \\nDevlin, J., Lee, K., Toutanova, K., Jones, L., \\nKelcey, M., Chang, M.‑W., Dai, A. M., Uszkoreit, J., \\nLe, Q., & Petrov, S. (2019). Natural Questions: A \\nBenchmark for Question Answering Research. \\nTransactions of the Association for Computational \\nLinguistics, 7, 453–466. https://doi.org/10.116 \\n2/tacl_a_00276 \\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., \\nGoyal, N., Küttler, H., Lewis, M., Yih, W., \\nRocktäschel, T., Riedel , S., & Kiela, D. (2020). \\nRetrieval-Augmented Generation for Knowledge-\\nIntensive NLP Tasks. https://doi.org/10.4 \\n8550/arXiv.2005.11401 \\nMa, X., Gong, Y., He, P., Zhao, H., & Duan, N. (2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='Retrieval-Augmented Generation for Knowledge-\\nIntensive NLP Tasks. https://doi.org/10.4 \\n8550/arXiv.2005.11401 \\nMa, X., Gong, Y., He, P., Zhao, H., & Duan, N. (2023). \\nQuery Rewriting for Retrieval-Augmented Large \\nLanguage Models. https://doi.org/10.48550/arXiv. \\n2305.14283 \\nMallen, A., Asai, A., Zhong, V.,  Das, R., Khashabi, D., & \\nHajishirzi, H. (2022). When Not to Trust Language \\nModels: Investigating Effectiveness of Parametric and \\nNon-Parametric Memories. https://doi.org/10.48550/ \\narXiv.2212.10511 \\nMin, S., Krishna, K., Lyu, X., Lewis, M., Yih, W., Koh, P., \\nIyyer, M., Zettlemoyer, L.,  & Hajishirzi, H. (2023). \\nFActScore: Fine-grained Atomic Evaluation of Factual \\nPrecision in Long Form Text Generation. In H. \\nBouamor, J. Pino, & K. Bali (Eds.), Proceedings of the \\n2023 Conference on Empirical Methods in Natural \\nLanguage Processing (pp. 12076–12100). Association \\nfor Computational Linguistics. \\nhttps://doi.org/10.18653/v1/2023.emnlp-main.741'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='2023 Conference on Empirical Methods in Natural \\nLanguage Processing (pp. 12076–12100). Association \\nfor Computational Linguistics. \\nhttps://doi.org/10.18653/v1/2023.emnlp-main.741 \\nOpenAI, Achiam, J., Adler, S. , Agarwal, S., Ahmad, L., \\nAkkaya, I., Aleman, F. L., Almeida, D., \\nAltenschmidt, J., Altman, S. , Anadkat, S., Avila, R., \\nBabuschkin, I., Balaji, S., Balcom, V., Baltescu, P., \\nBao, H., Bavarian, M., Belgum, J., . . . Zoph, B. \\n(2023). GPT-4 Technical Report. \\nhttps://doi.org/10.48550/arXiv.2303.08774 \\nParé, G., Tate, M., Johnstone, D., & Kitsiou, S. (2016). \\nContextualizing the twin concepts of systematicity and \\ntransparency in information systems literature reviews. \\nEuropean Journal of Information Systems, 25(6), 493–\\n508. https://doi.org/10.1057/s41303-016-0020-3 \\nRackauckas, Z., Câmara, A., & Zavrel, J. (2024). \\nEvaluating RAG-Fusion with RAGElo: an Automated \\nElo-based Framework. https://doi.org/10.48550/ \\narXiv.2406.14783'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content=\"Rackauckas, Z., Câmara, A., & Zavrel, J. (2024). \\nEvaluating RAG-Fusion with RAGElo: an Automated \\nElo-based Framework. https://doi.org/10.48550/ \\narXiv.2406.14783 \\nRajpurkar, P., Jia, R., & Liang, P. (2018). Know What You \\nDon't Know: Unanswerable Questions for SQuAD. \\nhttps://doi.org/10.48550/arXiv.1806.03822 \\nRau, D., Déjean, H.,  Chirkova, N., Form al, T., Wa ng, S., \\nNikoulina, V., & Clinchant, S. (2024). BERGEN: A \\nBenchmarking Library fo r Retrieval-Augmented \\nGeneration. https://doi.org/10.48550/arXiv.2407. \\n01102 \\nRavi, S. S., Mielczarek, B., Kannappan, A., Kiela, D., & \\nQian, R. (2024). Lynx: An Open Source Hallucination \\nEvaluation Model. https://arxiv.org/abs/2407.08488  \\nSaad-Falcon, J., Khattab, O., Potts, C., & Zaharia, M. \\n(2023). ARES: An Automated Evaluation Framework \\nfor Retrieval-Augmented Generation Systems. \\nhttps://doi.org/10.48550/arXiv.2311.09476 \\nWang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., \\nJiayang, C., Yao, Y., Gao, W., Hu, X [Xuming], Qi, Z.,\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='for Retrieval-Augmented Generation Systems. \\nhttps://doi.org/10.48550/arXiv.2311.09476 \\nWang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., \\nJiayang, C., Yao, Y., Gao, W., Hu, X [Xuming], Qi, Z., \\nWang, Y [Yidong], Yang, L., Wang, J [Jindong], \\nXie, X., Zhang, Z [Zheng], & Zhang, Y [Yue]. (2023). \\nSurvey on Factuality in Large Language Models: \\nKnowledge, Retrieval and Domain-Specificity. \\nhttps://doi.org/10.48550/arXiv.2310.07521 \\nWebster, J., & Watson, R. T. (2002). Analyzing the Past to \\nPrepare for the Future: Writing a Literature Review. \\nMIS Quarterly, 26(2), xiii–xxiii. https://www.jstor.org/ \\nstable/4132319?seq=1#metadata_info_tab_contents \\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., \\nSalakhutdinov, R., & Manning, C. D. (2018). \\nHotpotQA: A Dataset for Diverse, Explainable Multi-\\nhop Question Answering. https://doi.org/10.48550/ \\narXiv.1809.09600 \\nYu, H., Gan, A., Zhang, K., Tong, S., Liu, Q., & Liu, Z. \\n(2024). Evaluation of Retr ieval-Augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.15', 'creator': 'LaTeX with hyperref package', 'creationdate': '2024-11-22T00:10:14+00:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2024-11-22T00:10:14+00:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.5496 (1.40.15)', 'source': '..\\\\data\\\\pdf_files\\\\metrics.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'metrics.pdf', 'file_type': 'pdf'}, page_content='hop Question Answering. https://doi.org/10.48550/ \\narXiv.1809.09600 \\nYu, H., Gan, A., Zhang, K., Tong, S., Liu, Q., & Liu, Z. \\n(2024). Evaluation of Retr ieval-Augmented \\nGeneration: A Survey. https://doi.org/10.48550/ \\narXiv.2405.07437 \\nZhang, Z [Zihan], Fang, M., & Chen, L. (2024). \\nRetrievalQA: Assessing Adaptive Retrieval-Augmented \\nGeneration for Short-form Open-Domain Question \\nAnswering. https://doi.org/10.48550/arXiv.2402.16457 \\nZhang, Z [Zihan], Fang, M., Chen, L., Namazi-Rad, M.‑R., \\n& Wang, J [Jun]. (2023). How Do Large Language \\nModels Capture the Ever-changing World Knowledge? \\nA Review of Recent Advances. https://doi.org/10. \\n48550/arXiv.2310.07343      \\n \\nKMIS 2024 - 16th International Conference on Knowledge Management and Information Systems\\n148'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented Generation (RAG) has\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents innovative opportunities for the conceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. I NTRODUCTION\\nL'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='of RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. I NTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='constituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='mance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, Advanced\\nRAG paradigm focuses on optimizing the retrieval phase,\\naiming to enhance retrieval efficiency and strengthen the\\nutilization of retrieved chunks. As shown in Figure 1 ,typical'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='RAG paradigm focuses on optimizing the retrieval phase,\\naiming to enhance retrieval efficiency and strengthen the\\nutilization of retrieved chunks. As shown in Figure 1 ,typical\\nstrategies involve pre-retrieval processing and post-retrieval\\nprocessing. For instance, query rewriting is used to make\\nthe queries more clear and specific, thereby increasing the\\naccuracy of retrieval [10], and the reranking of retrieval results\\nis employed to enhance the LLM’s ability to identify and\\nutilize key information [11].\\nDespite the improvements in the practicality of Advanced\\nRAG, there remains a gap between its capabilities and real-\\nworld application requirements. On one hand, as RAG tech-\\nnology advances, user expectations rise, demands continue to\\nevolve, and application settings become more complex. For\\ninstance, the integration of heterogeneous data and the new\\ndemands for system transparency, control, and maintainability.\\nOn the other hand, the growth in application demands has'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='instance, the integration of heterogeneous data and the new\\ndemands for system transparency, control, and maintainability.\\nOn the other hand, the growth in application demands has\\nfurther propelled the evolution of RAG technology.\\nAs shown in Figure 2, to achieve more accurate and efficient\\ntask execution, modern RAG systems are progressively inte-\\ngrating more sophisticated function, such as organizing more\\nrefined index base in the form of knowledge graphs, integrat-\\ning structured data through query construction methods, and\\nemploying fine-tuning techniques to enable encoders to better\\nadapt to domain-specific documents.\\nIn terms of process design, the current RAG system has\\nsurpassed the traditional linear retrieval-generation paradigm.\\nResearchers use iterative retrieval [12] to obtain richer con-\\ntext, recursive retrieval [13] to handle complex queries, and\\nadaptive retrieval [14] to provide overall autonomy and flex-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Researchers use iterative retrieval [12] to obtain richer con-\\ntext, recursive retrieval [13] to handle complex queries, and\\nadaptive retrieval [14] to provide overall autonomy and flex-\\nibility. This flexibility in the process significantly enhances\\narXiv:2407.21059v1  [cs.CL]  26 Jul 2024'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='2\\nFig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\\nquestions, both encounter limitations and struggle to provide satisfactory\\nanswers. Despite the fact that Advanced RAG improves retrieval accuracy\\nthrough hierarchical indexing, pre-retrieval, and post-retrieval processes, these\\nrelevant documents have not been used correctly.\\nthe expressive power and adaptability of RAG systems, en-\\nabling them to better adapt to various application scenarios.\\nHowever, this also makes the orchestration and scheduling of\\nworkflows more complex, posing greater challenges to system\\ndesign. Specifically, RAG currently faces the following new\\nchallenges:\\nComplex data sources integration. RAG are no longer\\nconfined to a single type of unstructured text data source but\\nhave expanded to include various data types, such as semi-\\nstructured data like tables and structured data like knowledge\\ngraphs [15]. Access to heterogeneous data from multiple'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='have expanded to include various data types, such as semi-\\nstructured data like tables and structured data like knowledge\\ngraphs [15]. Access to heterogeneous data from multiple\\nsources can provide the system with a richer knowledge\\nbackground, and more reliable knowledge verification capa-\\nbilities [16].\\nNew demands for system interpretability, controllability,\\nFig. 2. Case of current Modular RAG.The system integrates diverse data\\nand more functional components. The process is no longer confined to linear\\nbut is controlled by multiple control components for retrieval and generation,\\nmaking the entire system more flexible and complex.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='3\\nand maintainability. With the increasing complexity of sys-\\ntems, system maintenance and debugging have become more\\nchallenging. Additionally, when issues arise, it is essential to\\nquickly pinpoint the specific components that require opti-\\nmization.\\nComponent selection and optimization. More neural net-\\nworks are involved in the RAG system, necessitating the\\nselection of appropriate components to meet the needs of spe-\\ncific tasks and resource configurations. Moreover, additional\\ncomponents enhance the effectiveness of RAG but also bring\\nnew collaborative work requirements [17]. Ensuring that these\\nmodels perform as intended and work efficiently together to\\nenhance the overall system performance is crucial.\\nWorkflow orchestration and scheduling. Components\\nmay need to be executed in a specific order, processed in paral-\\nlel under certain conditions, or even judged by the LLM based\\non different outputs. Reasonable planning of the workflow is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='may need to be executed in a specific order, processed in paral-\\nlel under certain conditions, or even judged by the LLM based\\non different outputs. Reasonable planning of the workflow is\\nessential for improving system efficiency and achieving the\\ndesired outcomes [18].\\nTo address the design, management, and maintenance chal-\\nlenges posed by the increasing complexity of RAG systems,\\nand to meet the ever-growing and diverse demands and ex-\\npectations, this paper proposes Modular RAG architecture.\\nIn modern computing systems, modularization is becoming\\na trend. It can enhance the system’s scalability and maintain-\\nability and achieve efficient task execution through process\\ncontrol.\\nThe Modular RAG system consists of multiple independent\\nyet tightly coordinated modules, each responsible for handling\\nspecific functions or tasks. This architecture is divided into\\nthree levels: the top level focuses on the critical stages of\\nRAG, where each stage is treated as an independent module.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='specific functions or tasks. This architecture is divided into\\nthree levels: the top level focuses on the critical stages of\\nRAG, where each stage is treated as an independent module.\\nThis level not only inherits the main processes from the\\nAdvanced RAG paradigm but also introduces an orchestration\\nmodule to control the coordination of RAG processes. The\\nmiddle level is composed of sub-modules within each module,\\nfurther refining and optimizing the functions. The bottom level\\nconsists of basic units of operation—operators. Within the\\nModular RAG framework, RAG systems can be represented\\nin the form of computational graphs, where nodes represent\\nspecific operators. The comparison of the three paradigms is\\nshown in the Figure 3. Modular RAG evolves based on the\\nprevious development of RAG. The relationships among these\\nthree paradigms are ones of inheritance and development.\\nAdvanced RAG is a special case of Modular RAG, while Naive\\nRAG is a special case of Advanced RAG.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='three paradigms are ones of inheritance and development.\\nAdvanced RAG is a special case of Modular RAG, while Naive\\nRAG is a special case of Advanced RAG.\\nThe advantages of Modular RAG are significant, as it\\nenhances the flexibility and scalability of RAG systems. Users\\ncan flexibly combine different modules and operators accord-\\ning to the requirements of data sources and task scenarios. In\\nsummary, the contributions of this paper are as follows:\\n• This paper proposes a new paradigm called modular\\nRAG, which employs a three-tier architectural design\\ncomprising modules, sub-modules, and operators to de-\\nfine the RAG system in a unified and structured manner.\\nThis design not only enhances the system’s flexibility and\\nscalability but also, through the independent design of\\noperators, strengthens the system’s maintainability and\\ncomprehensibility.\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='operators, strengthens the system’s maintainability and\\ncomprehensibility.\\n• Under the framework of Modular RAG, the orchestration\\nof modules and operators forms the RAG Flow, which\\ncan flexibly express current RAG methods. This paper has\\nfurther summarized six typical flow patterns and specific\\nmethods have been analyzed to reveal the universality of\\nmodular RAG in practical scenarios.\\n• The Modular RAG framework offers exceptional flexi-\\nbility and extensibility. This paper delves into the new\\nopportunities brought by Modular RAG and provides a\\nthorough discussion on the adaptation and expansion of\\nnew methods in different application scenarios, offering\\nguidance for future research directions and practical ex-\\nploration.\\nII. R ELATED WORK\\nThe development of RAG technology can be summarized\\nin three stages. Initially, retrieval-augmented techniques were\\nintroduced to improve the performance of pre-trained lan-\\nguage models on knowledge-intensive tasks [19], [20]. In'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='in three stages. Initially, retrieval-augmented techniques were\\nintroduced to improve the performance of pre-trained lan-\\nguage models on knowledge-intensive tasks [19], [20]. In\\nspecific implementations, Retro [21] optimized pre-trained\\nautoregressive models through retrieval augmentation, while\\nAtlas [22] utilized a retrieval-augmented few-shot fine-tuning\\nmethod, enabling language models to adapt to diverse tasks.\\nIRCOT [23] further enriched the reasoning process during\\nthe inference phase by combining chain-of-thought and multi-\\nstep retrieval processes. Entering the second stage, as the\\nlanguage processing capabilities of LLMs significantly im-\\nproved, retrieval-augmented techniques began to serve as a\\nmeans of supplementing additional knowledge and providing\\nreferences, aiming to reduce the hallucination. For instance,\\nRRR [24] improved the rewriting phase, and LLMlingua [25]\\nremoved redundant tokens in retrieved document chunks.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='references, aiming to reduce the hallucination. For instance,\\nRRR [24] improved the rewriting phase, and LLMlingua [25]\\nremoved redundant tokens in retrieved document chunks.\\nWith the continuous progress of RAG technology, research\\nhas become more refined and focused, while also achieving\\ninnovative integration with other technologies such as graph\\nneural networks [26] and fine-tuning techniques [27]. The\\noverall pipeline has also become more flexible, such as using\\nLLMs to proactively determine the timing of retrieval and\\ngeneration [14], [28].\\nThe development of RAG technology has been acceler-\\nated by LLM technology and practical application needs.\\nResearchers are examining and organizing the RAG frame-\\nwork and development pathways from different perspectives.\\nBuilding upon the enhanced stages of RAG, Gao et al., [2] sub-\\ndivided RAG into enhancement during pre-training, inference,\\nand fine-tuning stages. Based on the main processes of RAG,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Building upon the enhanced stages of RAG, Gao et al., [2] sub-\\ndivided RAG into enhancement during pre-training, inference,\\nand fine-tuning stages. Based on the main processes of RAG,\\nrelevant works on RAG were organized from the perspectives\\nof retrieval, generation, and augmentation methods. Huang\\net al., [29] categorize RAG methods into four main classes:\\npre-retrieval, retrieval, post-retrieval, generation, and provide\\na detailed discussion of the methods and techniques within\\neach class. Hu et al., [30] discuss Retrieval-Augmented Lan-\\nguage Models (RALMs) form three key components, including\\nretrievers, language models, augmentations, and how their\\ninteractions lead to different model structures and applications.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='4\\nFig. 3. Comparison between three RAG paradigms. Modular RAG has evolved from previous paradigms and aligns with the current practical needs of RAG\\nsystems.\\nThey emphasize the importance of considering robustness,\\naccuracy, and relevance when evaluating RALMs and pro-\\npose several evaluation methods. Ding et al., [31] provide a\\ncomprehensive review from the perspectives of architecture,\\ntraining strategies, and applications. They specifically discuss\\nfour training methods of RALMs: training-free methods, in-\\ndependent training methods, sequence training methods, and\\njoint training methods, and compare their advantages and\\ndisadvantages. Zhao et al., [32]analyze the applications of\\nRAG technology in various fields such as text generation,\\ncode generation, image generation, and video generation from\\nthe perspective of augmented intelligence with generative\\ncapabilities.\\nThe current collation of RAG systems primarily focuses\\non methods with a fixed process, mainly concerned with'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the perspective of augmented intelligence with generative\\ncapabilities.\\nThe current collation of RAG systems primarily focuses\\non methods with a fixed process, mainly concerned with\\noptimizing the retrieval and generation stages. However, it has\\nnot turned its attention to the new characteristics that RAG\\nresearch is continuously evolving, namely the characteristics\\nof process scheduling and functional componentization. There\\nis currently a lack of comprehensive analysis of the overall\\nRAG system, which has led to research on paradigms lagging\\nbehind the development of RAG technology.\\nIII. F RAMEWORK AND NOTATION\\nFor query Q = {qi}, a typical RAG system mainly consists\\nof three key components. 1) Indexing. Given documents D =\\n{d1, d2, . . . , dn} , where di represents the document chunk.\\nIndexing is the process of converting di into vectors through\\nan embedding model fe(·) , and then store vectors in vector\\ndatabase.\\nI = {e1, e2, . . . , en} and e i = fe(di) ∈ Rd (1)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Indexing is the process of converting di into vectors through\\nan embedding model fe(·) , and then store vectors in vector\\ndatabase.\\nI = {e1, e2, . . . , en} and e i = fe(di) ∈ Rd (1)\\nNotation Description\\nq The original query\\ny The output of LLM\\nD A document retrieval repository composed of chunks di.\\nR(q, D) Retriever,find similar chunks from D based on q.\\nF RAG Flow\\nP RAG Flow pattern\\nfqe Query expansion function\\nfqc Query transform function\\nfcomp Chunk compression function\\nfsel Chunk selection function\\nfr Routing function\\nM Module in modular RAG\\nop The specific operators within the Module.\\nTABLE I\\nIMPORTANT NOTATION\\n2) Retrieval . Transform the query into a vector using the\\nsame encoding model, and then filter out the top k document\\nchunks that are most similar based on vector similarity.\\nR : topk\\ndi∈D\\nSim(q, di) → Dq (2)\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='R : topk\\ndi∈D\\nSim(q, di) → Dq (2)\\nDq = {d1, d2, . . . , dk} represents the relevant documents for\\nquestion q. The similarity function Sim(·) commonly used are\\ndot product or cosine similarity.\\nSim(q, di) = eq · edi or eq · edi\\n∥eq∥ · ∥edi∥ (3)\\n3) Generation . After getting the relevant documents. The\\nquery q and the retrieved document Dq chunks are inputted\\ntogether to the LLM to generate the final answer, where [·, ·]\\nstands for concatenation.\\ny = LLM([Dq, q]) (4)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='5\\nWith the evolution of RAG technology, more and more func-\\ntional components are being integrated into systems. Modular\\nRAG paradigm includes three levels, ranging from large to\\nsmall:\\nL1 Module (M = {Ms}). The core process in RAG\\nsystem.\\nL2 Sub-module (Ms = {Op}).The functional modules in\\nmodule.\\nL3 Operator (Op = {fθi}). The the specific functional\\nimplementation in a module or sub-module. As a result, a\\nModular RAG system can be represented as:\\nG = {q, D,M, {Ms}, {Op}} (5)\\nThe arrangement between modules and operators constitutes\\nthe RAG Flow F = ( Mϕ1 , . . . , Mϕn) where ϕ stands for\\nthe set of module parameters. A modular rag flow can be\\ndecomposed into a graph of sub-functions. In the simplest\\ncase,the graph is a linear chain.\\nNaiveRAG : q\\nR(q,D)\\n− − − − − − − − − − − →\\nText−Embedding\\nDq LLM([q,Dq])\\n− − − − − − − − − − − →\\nOpenAI/GPT −4\\ny\\n(6)\\nIV. M ODULE AND OPERATOR\\nThis chapter will specifically introduce modules and op-\\nerators under the Modular RAG framework. Based on the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Dq LLM([q,Dq])\\n− − − − − − − − − − − →\\nOpenAI/GPT −4\\ny\\n(6)\\nIV. M ODULE AND OPERATOR\\nThis chapter will specifically introduce modules and op-\\nerators under the Modular RAG framework. Based on the\\ncurrent stage of RAG development, we have established\\nsix main modules: Indexing, Pre-retrieval, Retrieval, Post-\\nretrieval, Generation, and Orchestration.\\nA. Indexing\\nIndexing is the process of split document into manageable\\nchunks and it is a key step in organizing a system. Indexing\\nfaces three main challenges. 1) Incomplete content represen-\\ntation.The semantic information of chunks is influenced by the\\nsegmentation method, resulting in the loss or submergence of\\nimportant information within longer contexts. 2) Inaccurate\\nchunk similarity search . As data volume increases, noise in\\nretrieval grows, leading to frequent matching with erroneous\\ndata, making the retrieval system fragile and unreliable. 3)\\nUnclear reference trajectory. The retrieved chunks may orig-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='retrieval grows, leading to frequent matching with erroneous\\ndata, making the retrieval system fragile and unreliable. 3)\\nUnclear reference trajectory. The retrieved chunks may orig-\\ninate from any document, devoid of citation trails, potentially\\nresulting in the presence of chunks from multiple different\\ndocuments that, despite being semantically similar, contain\\ncontent on entirely different topics.\\n1) Chunk Optimization: The size of the chunks and the\\noverlap between the chunks play a crucial role in the overall\\neffectiveness of the RAG system. Given a chunk di, its chunk\\nsize is denoted as Li = |di|, and the overlap is denoted as\\nLo\\ni = |di ∩ di+1|. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise [17].\\nSliding Window using overlapping chunks in a sliding win-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='time and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise [17].\\nSliding Window using overlapping chunks in a sliding win-\\ndow enhances semantic transitions. However, it has limitations\\nsuch as imprecise context size control, potential truncation of\\nwords or sentences, and lacking semantic considerations.\\nMetadata Attachment. Chunks can be enriched with meta-\\ndata like page number, file name, author, timestamp, sum-\\nmary, or relevant questions. This metadata allows for filtered\\nretrieval, narrowing the search scope.\\nSmall-to-Big [33] separate the chunks used for retrieval\\nfrom those used for synthesis. Smaller chunks enhance re-\\ntrieval accuracy, while larger chunks provide more context.\\nOne approach is to retrieve smaller summarized chunks and\\nreference their parent larger chunks. Alternatively, individual\\nsentences could be retrieved along with their surrounding text.\\n2) Structure Organization: One effective method for en-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='reference their parent larger chunks. Alternatively, individual\\nsentences could be retrieved along with their surrounding text.\\n2) Structure Organization: One effective method for en-\\nhancing information retrieval is to establish a hierarchical\\nstructure for the documents. By constructing chunks structure,\\nRAG system can expedite the retrieval and processing of\\npertinent data.\\nHierarchical Index . In the hierarchical structure of docu-\\nments, nodes are arranged in parent-child relationships, with\\nchunks linked to them. Data summaries are stored at each\\nnode, aiding in the swift traversal of data and assisting the\\nRAG system in determining which chunks to extract. This\\napproach can also mitigate the illusion caused by chunk\\nextraction issues. The methods for constructing a structured\\nindex primarily include: 1) Structural awareness based on\\nparagraph and sentence segmentation in docs. 2) Content\\nawareness based on inherent structure in PDF, HTML, and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='index primarily include: 1) Structural awareness based on\\nparagraph and sentence segmentation in docs. 2) Content\\nawareness based on inherent structure in PDF, HTML, and\\nLatex. 3) Semantic awareness based on semantic recognition\\nand segmentation of text.\\nKG Index [34]. Using Knowledge Graphs (KGs) to struc-\\nture documents helps maintain consistency by clarifying con-\\nnections between concepts and entities, reducing the risk of\\nmismatch errors. KGs also transform information retrieval\\ninto instructions intelligible to language models, improving re-\\ntrieval accuracy and enabling contextually coherent responses.\\nThis enhances the overall efficiency of the RAG system.\\nFor example, organizing a corpus in the format of graph\\nG = {V, E, X}, where node V = {vi}n\\ni=1 represent document\\nstructures (e.g.passage, pages, table) , edge E ⊂ V × Vrep-\\nresent semantic or lexical similarity and belonging relations,\\nand node features X = {Xi}n\\ni=1 represent text or markdown\\ncontent for passage.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='resent semantic or lexical similarity and belonging relations,\\nand node features X = {Xi}n\\ni=1 represent text or markdown\\ncontent for passage.\\nB. Pre-retrieval\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nThe primary challenges in this module include: 1) Poorly\\nworded queries . The question itself is complex, and the\\nlanguage is not well-organized. 2) Language complexity and\\nambiguity. Language models often struggle when dealing\\nwith specialized vocabulary or ambiguous abbreviations with\\nmultiple meanings. For instance, they may not discern whether\\nLLM refers to Large Language Model or a Master of Laws in\\na legal context.\\n1) Query Expansion : Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='6\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nfqe(q) = {q1, q2, . . . , qn} ∀qi ∈ {q1, q2, . . . , qn}, qi /∈ Q\\n(7)\\nMulti-Query uses prompt engineering to expand queries\\nvia LLMs, allowing for parallel execution. These expansions\\nare meticulously designed to ensure diversity and coverage.\\nHowever, this approach can dilute the user’s original intent.\\nTo mitigate this, the model can be instructed to assign greater\\nweight to the original query.\\nSub-Query. By decomposing and planning for complex\\nproblems, multiple sub-problems are generated. Specifically,\\nleast-to-most prompting [35] can be employed to decom-\\npose the complex problem into a series of simpler sub-\\nproblems. Depending on the structure of the original problem,\\nthe generated sub-problems can be executed in parallel or\\nsequentially. Another approach involves the use of the Chain-\\nof-Verification (CoVe) [36]. The expanded queries undergo'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the generated sub-problems can be executed in parallel or\\nsequentially. Another approach involves the use of the Chain-\\nof-Verification (CoVe) [36]. The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing hallu-\\ncinations.\\n2) Query Transformation: Retrieve and generate based on\\na transformed query instead of the user’s original query.\\nfqt(q) = q′ (8)\\nRewrite. Original queries often fall short for retrieval in\\nreal-world scenarios. To address this, LLMs can be prompted\\nto rewrite. Specialized smaller models can also be employed\\nfor this purpose [24]. The implementation of the query rewrite\\nmethod in Taobao has significantly improved recall effective-\\nness for long-tail queries, leading to an increase in GMV [10].\\nHyDE [37]. In order to bridge the semantic gap between\\nquestions and answers, it constructs hypothetical documents\\n(assumed answers) when responding to queries instead of\\ndirectly searching the query. It focuses on embedding simi-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='questions and answers, it constructs hypothetical documents\\n(assumed answers) when responding to queries instead of\\ndirectly searching the query. It focuses on embedding simi-\\nlarity from answer to answer rather than seeking embedding\\nsimilarity for the problem or query. In addition, it also in-\\ncludes reverse HyDE, which generate hypothetical query for\\neach chunks and focuses on retrieval from query to query.\\nStep-back Prompting [38]. The original query is abstracted\\ninto a high-level concept question (step-back question). In the\\nRAG system, both the step-back question and the original\\nquery are used for retrieval, and their results are combined\\nto generate the language model’s answer.\\n3) Query Construction: In addition to text data, an in-\\ncreasing amount of structured data, such as tables and graph\\ndata, is being integrated into RAG systems. To accommodate\\nvarious data types, it is necessary to restructure the user’s\\nquery. This involve converting the query into another query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='data, is being integrated into RAG systems. To accommodate\\nvarious data types, it is necessary to restructure the user’s\\nquery. This involve converting the query into another query\\nlanguage to access alternative data sources, with common\\nmethods including Text-to-SQL or Text-to-Cypher . In many\\nscenarios, structured query languages (e.g., SQL, Cypher)\\nare often used in conjunction with semantic information and\\nmetadata to construct more complex queries.\\nfqc(q) = q∗, q∗ ∈ Q∗ = {SQL, Cypher, . . .} (9)\\nC. Retrieval\\nThe retrieval process is pivotal in RAG systems. By lever-\\naging powerful embedding models, queries and text can be\\nefficiently represented in latent spaces, which facilitates the\\nestablishment of semantic similarity between questions and\\ndocuments, thereby enhancing retrieval. Three main consider-\\nations that need to be addressed include retrieval efficiency,\\nquality, and the alignment of tasks, data and models.\\n1) Retriever Selection: With the widespread adoption of'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='ations that need to be addressed include retrieval efficiency,\\nquality, and the alignment of tasks, data and models.\\n1) Retriever Selection: With the widespread adoption of\\nRAG technology, the development of embedding models has\\nbeen in full swing. In addition to traditional models based\\non statistics and pre-trained models based on the encoder\\nstructure, embedding models fine-tuned on LLMs have also\\ndemonstrated powerful capabilities [39]. However, they often\\ncome with more parameters, leading to weaker inference\\nand retrieval efficiency. Therefore, it is crucial to select the\\nappropriate retriever based on different task scenarios.\\nSparse Retriever uses statistical methods to convert queries\\nand documents into sparse vectors. Its advantage lies in its\\nefficiency in handling large datasets, focusing only on non-zero\\nelements. However, it may be less effective than dense vectors\\nin capturing complex semantics. Common methods include\\nTF-IDF and BM25.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='efficiency in handling large datasets, focusing only on non-zero\\nelements. However, it may be less effective than dense vectors\\nin capturing complex semantics. Common methods include\\nTF-IDF and BM25.\\nDense Retriever employs pre-trained language models\\n(PLMs) to provide dense representations of queries and doc-\\numents. Despite higher computational and storage costs, it\\noffers more complex semantic representations. Typical models\\ninclude BERT structure PLMs, like ColBERT, and multi-task\\nfine-tuned models like BGE [40] and GTE [41].\\nHybrid Retriever is to use both sparse and dense retrievers\\nsimultaneously. Two embedding techniques complement each\\nother to enhance retrieval effectiveness. Sparse retriever can\\nprovide initial screening results. Additionally, sparse models\\nenhance the zero-shot retrieval capabilities of dense models,\\nparticularly in handling queries with rare entities, thereby\\nincreasing system robustness.\\n2) Retriever Fine-tuning: In cases where the context may'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='particularly in handling queries with rare entities, thereby\\nincreasing system robustness.\\n2) Retriever Fine-tuning: In cases where the context may\\ndiverge from pre-trained corpus, particularly in highly special-\\nized fields like healthcare, law, and other domains abundant in\\nproprietary terminology. While this adjustment demands addi-\\ntional effort, it can substantially enhance retrieval efficiency\\nand domain alignment.\\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval\\nmodel based on labeled domain data is typically done using\\ncontrastive learning. This involves reducing the distance be-\\ntween positive samples while increasing the distance between\\nnegative samples. The commonly used loss calculation is\\nshown in the following:\\nL(DR) = − 1\\nT\\nTX\\ni=1\\nlog e(sim(qi,d+\\ni ))\\ne(sim(qi,d+\\ni )) + PN\\nj=1 e(sim(qi,d−\\ni ))\\n(10)\\nwhere d+\\ni is the positive sample document corresponding to\\nthe i-th query, d−\\ni is several negative sample, T is the total'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='TX\\ni=1\\nlog e(sim(qi,d+\\ni ))\\ne(sim(qi,d+\\ni )) + PN\\nj=1 e(sim(qi,d−\\ni ))\\n(10)\\nwhere d+\\ni is the positive sample document corresponding to\\nthe i-th query, d−\\ni is several negative sample, T is the total\\nnumber of queries, N is the number of negative samples, and\\nDR is the fine-tuning dataset.\\nLM-supervised Retriever (LSR) . In contrast to directly\\nconstructing a fine-tuning dataset from the dataset, LSR uti-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='7\\nlizes the LM-generated results as supervisory signals to fine-\\ntune the embedding model during the RAG process.\\nPLSR(d|q, y) = ePLM(y|d,q)/β\\nP\\nd′∈D ePLM(y|d,q)/β) (11)\\nPLM (y|d, q) is LM probability of the ground truth output y\\ngiven the input context d and query q, and β is a hyper-\\nparamter.\\nAdapter. At times, fine-tuning a large retriever can be\\ncostly, especially when dealing with retrievers based on LLMs\\nlike gte-Qwen. In such cases, it can mitigate this by incorpo-\\nrating an adapter module and conducting fine-tuning. Another\\nbenefit of adding an adapter is the ability to achieve better\\nalignment with specific downstream tasks [42].\\nD. Post-retrieval\\nFeeding all retrieved chunks directly into the LLM is not an\\noptimal choice. Post-processing the chunks can aid in better\\nleveraging the contextual information. The primary challenges\\ninclude: 1) Lost in the middle . Like humans, LLM tends\\nto remember only the beginning or the end of long texts,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='leveraging the contextual information. The primary challenges\\ninclude: 1) Lost in the middle . Like humans, LLM tends\\nto remember only the beginning or the end of long texts,\\nwhile forgetting the middle portion [43]. 2) Noise/anti-fact\\nchunks. Retrieved noisy or factually contradictory documents\\ncan impact the final retrieval generation [44]. 3) Context\\nWindow. Despite retrieving a substantial amount of relevant\\ncontent, the limitation on the length of contextual information\\nin large models prevents the inclusion of all this content.\\n1) Rerank: Rerank the retrieved chunks without altering\\ntheir content or length, to enhance the visibility of the more\\ncrucial document chunks. Given the retrieved set Dq and a\\nre-ranking method frerank to obtain the re-ranked set:\\nDq\\nr = frerank(q, Dq) = {d′\\n1, d′\\n2, . . . , d′\\nk}\\nwheref(d′\\n1) ≥ f(d′\\n2) ≥ . . .≥ f(d′\\nk). (12)\\nRule-base rerank. Metrics are calculated to rerank chunks\\naccording to certain rules. Common metrics include: diversity,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='1, d′\\n2, . . . , d′\\nk}\\nwheref(d′\\n1) ≥ f(d′\\n2) ≥ . . .≥ f(d′\\nk). (12)\\nRule-base rerank. Metrics are calculated to rerank chunks\\naccording to certain rules. Common metrics include: diversity,\\nrelevance and MRR (Maximal Marginal Relevance) [45]. The\\nidea is to reduce redundancy and increase result diversity.\\nMMR selects phrases for the final key phrase list based on a\\ncombined criterion of query relevance and information novelty.\\nModel-base rerank. Utilize a language model to reorder the\\ndocument chunks, commonly based on the relevance between\\nthe chunks and the query. Rerank models have become an\\nimportant component of RAG systems, and relevant model\\ntechnologies are also being iteratively upgraded. The scope\\nreordering has also been extended to multimodal data such as\\ntables and images [46].\\n2) Compression: A common misconception in the RAG\\nprocess is the belief that retrieving as many relevant docu-\\nments as possible and concatenating them to form a lengthy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='tables and images [46].\\n2) Compression: A common misconception in the RAG\\nprocess is the belief that retrieving as many relevant docu-\\nments as possible and concatenating them to form a lengthy\\nretrieval prompt is beneficial. However, excessive context can\\nintroduce more noise, diminishing the LLM’s perception of\\nkey information. A common approach to address this is to\\ncompress and select the retrieved content.\\nDq\\nc = fcomp(q, Dq), where|dqc\\ni | < |dq\\ni | ∀dq\\ni ∈ Dq (13)\\n(Long)LLMLingua [47]. By utilizing aligned and trained\\nsmall language models, such as GPT-2 Small or LLaMA-\\n7B, the detection and removal of unimportant tokens from\\nthe prompt is achieved, transforming it into a form that is\\nchallenging for humans to comprehend but well understood by\\nLLMs. This approach presents a direct and practical method\\nfor prompt compression, eliminating the need for additional\\ntraining of LLMs while balancing language integrity and\\ncompression ratio.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='LLMs. This approach presents a direct and practical method\\nfor prompt compression, eliminating the need for additional\\ntraining of LLMs while balancing language integrity and\\ncompression ratio.\\n3) Selection: Unlike compressing the content of document\\nchunks, Selection directly removes irrelevant chunks.\\nDq\\ns = fsel(Dq) = {di ∈ D | ¬P(di)} (14)\\nWhere fsel is the function for deletion operation and P(di) is\\na conditional predicate indicating that document ( di) satisfies\\na certain condition. If document ( di) satisfies ( P(di)), it will\\nbe deleted. Conversely, documents for which ( ¬P(di)) is true\\nwill be retained.\\nSelective Context. By identifying and removing redundant\\ncontent in the input context, the input is refined, thus improv-\\ning the language model’s reasoning efficiency. In practice, se-\\nlective context assesses the information content of lexical units\\nbased on the self-information computed by the base language\\nmodel. By retaining content with higher self-information, this'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='lective context assesses the information content of lexical units\\nbased on the self-information computed by the base language\\nmodel. By retaining content with higher self-information, this\\nmethod offers a more concise and efficient textual representa-\\ntion, without compromising their performance across diverse\\napplications. However, it overlooks the interdependence be-\\ntween compressed content and the alignment between the\\ntargeted language model and the small language model utilized\\nfor prompting compression [48].\\nLLM-Critique. Another straightforward and effective ap-\\nproach involves having the LLM evaluate the retrieved content\\nbefore generating the final answer. This allows the LLM\\nto filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [49], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nE. Generation\\nUtilize the LLM to generate answers based on the user’s'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='to self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nE. Generation\\nUtilize the LLM to generate answers based on the user’s\\nquery and the retrieved contextual information. Select an\\nappropriate model based on the task requirements, considering\\nfactors such as the need for fine-tuning, inference efficiency,\\nand privacy protection.\\n1) Generator Fine-tuning: In addition to direct LLM usage,\\ntargeted fine-tuning based on the scenario and data character-\\nistics can yield better results. This is also one of the greatest\\nadvantages of using an on-premise setup LLMs.\\nInstruct-Tuning. When LLMs lack data in a specific do-\\nmain, additional knowledge can be provided to the LLM\\nthrough fine-tuning. General fine-tuning dataset can also be\\nused as an initial step. Another benefit of fine-tuning is the\\nability to adjust the model’s input and output. For example, it\\ncan enable LLM to adapt to specific data formats and generate'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='used as an initial step. Another benefit of fine-tuning is the\\nability to adjust the model’s input and output. For example, it\\ncan enable LLM to adapt to specific data formats and generate\\nresponses in a particular style as instructed [50].\\nReinforcement learning. Aligning LLM outputs with hu-\\nman or retriever preferences through reinforcement learning is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='8\\na potential approach [51]. For instance, manually annotating\\nthe final generated answers and then providing feedback\\nthrough reinforcement learning. In addition to aligning with\\nhuman preferences, it is also possible to align with the\\npreferences of fine-tuned models and retrievers.\\nDual Fine-tuing Fine-tuning both generator and retriever\\nsimultaneously to align their preferences. A typical approach,\\nsuch as RA-DIT [27], aligns the scoring functions between\\nretriever and generator using KL divergence. Retrieval likeli-\\nhood of each retrieved document d is calculated as :\\nPR(d|q) = e(sim(d,q))/γP\\nd∈Dq e(sim(d,q)/γ (15)\\nPLM (y|d, q) is the LM probability of the ground truth output y\\ngiven the input context d, question q, and γ is a hyperparamter.\\nThe overall loss is calculated as:\\nL = 1\\n|T|\\nTX\\ni=1\\nKL(PR(d|q)||PLSR(d|q, y|)) (16)\\n2) Verification : Although RAG enhances the reliability\\nof LLM-generated answers, in many scenarios, it requires to'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='The overall loss is calculated as:\\nL = 1\\n|T|\\nTX\\ni=1\\nKL(PR(d|q)||PLSR(d|q, y|)) (16)\\n2) Verification : Although RAG enhances the reliability\\nof LLM-generated answers, in many scenarios, it requires to\\nminimize the probability of hallucinations. Therefore, it can\\nfilter out responses that do not meet the required standards\\nthrough additional verification module. Common verification\\nmethods include knowledge-base and model-base .\\nyk = fverify (q, Dq, y) (17)\\nKnowledge-base verification refers to directly validating the\\nresponses generated by LLMs through external knowledge.\\nGenerally, it extracts specific statements or triplets from re-\\nsponse first. Then, relevant evidence is retrieved from verified\\nknowledge base such as Wikipedia or specific knowledge\\ngraphs. Finally, each statement is incrementally compared with\\nthe evidence to determine whether the statement is supported,\\nrefuted, or if there is insufficient information [52].\\nModel-based verification refers to using a small language'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the evidence to determine whether the statement is supported,\\nrefuted, or if there is insufficient information [52].\\nModel-based verification refers to using a small language\\nmodel to verify the responses generated by LLMs [53].\\nGiven the input question, the retrieved knowledge, and the\\ngenerated answer, a small language model is trained to de-\\ntermine whether the generated answer correctly reflects the\\nretrieved knowledge. This process is framed as a multiple-\\nchoice question, where the verifier needs to judge whether the\\nanswer reflects correct answer . If the generated answer does\\nnot correctly reflect the retrieved knowledge, the answer can\\nbe iteratively regenerated until the verifier confirms that the\\nanswer is correct.\\nF . Orchestration\\nOrchestration pertains to the control modules that govern the\\nRAG process. Unlike the traditional, rigid approach of a fixed\\nprocess, RAG now incorporates decision-making at pivotal\\njunctures and dynamically selects subsequent steps contingent'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='RAG process. Unlike the traditional, rigid approach of a fixed\\nprocess, RAG now incorporates decision-making at pivotal\\njunctures and dynamically selects subsequent steps contingent\\nupon the previous outcomes. This adaptive and modular ca-\\npability is a hallmark of modular RAG, distinguishing it from\\nthe more simplistic Naive and Advance RAG paradigm.\\n1) Routing: In response to diverse queries, the RAG system\\nroutes to specific pipelines tailored for different scenario, a\\nfeature essential for a versatile RAG architecture designed\\nto handle a wide array of situations. A decision-making\\nmechanism is necessary to ascertain which modules will be\\nengaged, based on the input from the model or supplementary\\nmetadata. Different routes are employed for distinct prompts\\nor components. This routing mechanism is executed through\\na function, denoted as fr(·), which assigns a score αi to\\neach module. These scores dictate the selection of the active'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='or components. This routing mechanism is executed through\\na function, denoted as fr(·), which assigns a score αi to\\neach module. These scores dictate the selection of the active\\nsubset of modules. Mathematically, the routing function is\\nrepresented as:\\nfr : Q → F (18)\\nwhere fr(·) maps the identified query to its corresponding\\nRAG flow.\\nMetadata routing involves extracting key terms, or entities,\\nfrom the query, applying a filtration process that uses these\\nkeywords and associated metadata within the chunks to refine\\nthe routing parameters. For a specific RAG flow, denoted as\\nFi, the pre-defined routing keywords are represented as the\\nset Ki = {ki1, ki2, . . . , kin}. The keyword identified within\\nthe query qi is designated as K′\\ni. The matching process for\\nthe query q is quantified by the key score equation:\\nscorekey(qi, Fj) = 1\\n|K′\\nj||Ki ∩ K′\\nj| (19)\\nThis equation calculates the overlap between the pre-defined\\nkeywords and those identified in the query, normalized by the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='scorekey(qi, Fj) = 1\\n|K′\\nj||Ki ∩ K′\\nj| (19)\\nThis equation calculates the overlap between the pre-defined\\nkeywords and those identified in the query, normalized by the\\ncount of keywords in K′\\nj. The final step is to determine the\\nmost relevant flow for the query q:\\nFi(q) = argmaxFj∈Fscore(q, Fj) (20)\\nSemantic routing routes to different modules based on the\\nsemantic information of the query. Given a pre-defined intent\\nΘ = {θ1, θ2, . . . , θn}, the possibility of intent for query q is\\nPΘ(θ|q) = ePLM(θ|q)\\nP\\nθ∈Θ ePLM(θ|q)) . Routing to specific RAG flow is\\ndetermined by the semantic score:\\nsocresemantic(q, Fj) = argmaxθj∈ΘP(Θ) (21)\\nThe function δ(·) serves as a mapping function that assigns\\nan intent to a distinct RAG flow Fi = δ(θi)\\nHybrid Routing can be implemented to improve query\\nrouting by integrating both semantic analysis and metadata-\\nbased approaches, which can be defined as follows:\\nαi = a·scorekey(q, Fj)+(1−α)·maxθj∈Θsocresemantic(q, Fj)\\n(22)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='routing by integrating both semantic analysis and metadata-\\nbased approaches, which can be defined as follows:\\nαi = a·scorekey(q, Fj)+(1−α)·maxθj∈Θsocresemantic(q, Fj)\\n(22)\\na is a weighting factor that balances the contribution of the\\nkey-based score and the semantic score.\\n2) Scheduling: The RAG system evolves in complexity\\nand adaptability, with the ability to manage processes through\\na sophisticated scheduling module. The scheduling module\\nplays a crucial role in the modular RAG , identifying critical\\njunctures that require external data retrieval, assessing the\\nadequacy of the responses, and deciding on the necessity for\\nfurther investigation. It is commonly utilized in scenarios that\\ninvolve recursive, iterative, and adaptive retrieval, ensuring'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='9\\nthat the system makes informed decisions on when to cease\\ngeneration or initiate a new retrieval loop.\\nRule judge. The subsequent steps are dictated by a set of\\nestablished rules. Typically, the system evaluates the quality of\\ngenerated answers through scoring mechanisms. The decision\\nto proceed or halt the process is contingent upon whether these\\nscores surpass certain predetermined thresholds, often related\\nto the confidence levels of individual tokens, which can be\\ndefined as follow:\\nyt =\\n(\\nˆst if all tokens of ˆst have probs ≥ τ\\nst = LM([Dqt, x, y<t]) otherwise\\nHere, ˆst represents the tentative answer, and st is the output\\nfrom the language model. The condition for accepting ˆst is that\\nall tokens within it must have associated probabilities greater\\nthan or equal to the threshold τ. If this condition is not met,\\nthe system reverts to generating a new answer.\\nLLM judge. The LLM independently determines the sub-\\nsequent course of action. Two primary approaches facilitate'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='the system reverts to generating a new answer.\\nLLM judge. The LLM independently determines the sub-\\nsequent course of action. Two primary approaches facilitate\\nthis capability. The first method leverages LLM ’s in-context\\nlearning capability, and make judgments through prompt\\nengineering. A significant advantage of this method is the\\nelimination of model fine-tuning. Nonetheless, the format of\\nthe judgment output is contingent upon the LLM’s adherence\\nto the provided instructions.\\nThe second approach involves the LLM generating specific\\ntokens that initiate targeted actions through fine-tuning. This\\ntechnique, with roots in the Toolformer [50], has been inte-\\ngrated into frameworks like Self-RAG [28]. This allows for a\\nmore direct control mechanism over the LLM’s actions, en-\\nhancing the system’s responsiveness to specific triggers within\\nthe conversational context. However, it requires generating a\\nlarge number of compliant instruction sets to fine-tune LLM.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='hancing the system’s responsiveness to specific triggers within\\nthe conversational context. However, it requires generating a\\nlarge number of compliant instruction sets to fine-tune LLM.\\nKnowledge-guide scheduling. Beyond the confines of rule-\\nbased methods and the complete reliance on LLMs for process\\ncontrol, a more adaptable intermediate approach emerges with\\nknowledge-guided scheduling [26]. These methods harness\\nthe power of knowledge graphs, to steer the retrieval and\\ngeneration processes. Specifically, it involves extracting infor-\\nmation relevant to the question from a knowledge graph and\\nconstructing a reasoning chain. This reasoning chain consists\\nof a series of logically interconnected nodes, each containing\\ncritical information for the problem-solving process. Based\\non the information from the nodes in this reasoning chain,\\ninformation retrieval and content generation can be performed\\nseparately. By integrating this approach, it enhance not only'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='on the information from the nodes in this reasoning chain,\\ninformation retrieval and content generation can be performed\\nseparately. By integrating this approach, it enhance not only\\nthe efficacy and precision of problem-solving but also the\\nclarity of the explanations provided.\\n3) Fusion: As RAG process has evolved beyond a linear\\npipeline, it frequently necessitates broadening the retrieval\\nscope or enhancing diversity by exploring multiple pipelines.\\nConsequently, after the expansion into various branches, the\\nfusion module effectively integrates the information, ensuring\\na comprehensive and coherent response. The fusion module’s\\nreliance is not just for merging answers but also for ensuring\\nthat the final output is both rich in content and reflective of\\nthe multifaceted nature of the inquiry.\\nLLM fusion .One of the most straightforward methods for\\nmulti-branch aggregation is to leverage the powerful capa-\\nbilities of LLMs to analyze and integrate information from'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='LLM fusion .One of the most straightforward methods for\\nmulti-branch aggregation is to leverage the powerful capa-\\nbilities of LLMs to analyze and integrate information from\\ndifferent branches. However, this approach also faces some\\nchallenges, particularly when dealing with long answers that\\nexceeds the LLM’s context window limitation. To mitigate this\\nissue, it is common practice to first summarize each branch’s\\nanswer, extracting the key information before inputting it into\\nthe LLM, thus ensuring that the most important content is\\nretained even within length constraints.\\nWeighted ensemble is based on the weighted values of\\ndifferent tokens generated from multiple branches, leading to\\nthe comprehensive selection of the final output. This approach\\ncan be calculated as :\\np(y|q, Dq) =\\nX\\nd∈Dq\\np(y|d, q) · λ(d, q) (23)\\nThe weight λ(d, q) is determined by the similarity score\\nbetween the document d and the input query q. This weight is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='can be calculated as :\\np(y|q, Dq) =\\nX\\nd∈Dq\\np(y|d, q) · λ(d, q) (23)\\nThe weight λ(d, q) is determined by the similarity score\\nbetween the document d and the input query q. This weight is\\ncalculated using the softmax function, which ensures that the\\nweights are normalized and sum up to one.\\nλ(d, q) = es(d,q)\\nP\\nd∈Dq es(d,q) (24)\\nRRF (Reciprocal Rank Fusion) is an ensemble technique\\nthat synthesizes multiple retrieval result rankings into a co-\\nhesive, unified list [54]. It employs a tailored weighted aver-\\naging approach to enhance collective predictive performance\\nand ranking precision. The method’s strength is its dynamic\\nweight assignment, which is informed by the interplay among\\nbranches. RRF is especially potent in scenarios characterized\\nby model or source heterogeneity, where it can markedly\\namplify the accuracy of predictions.\\nV. RAG F LOW AND FLOW PATTERN\\nThe collaboration between operators forms the workflow\\nof the module, which we refer to as RAG flow F ='),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='amplify the accuracy of predictions.\\nV. RAG F LOW AND FLOW PATTERN\\nThe collaboration between operators forms the workflow\\nof the module, which we refer to as RAG flow F =\\n(Mϕ1 , . . . , Mϕn), where ϕ stands for the set of module param-\\neters. A modular rag flow can be decomposed into a graph of\\nsub-functions. Through control logic, the operators can execute\\nin a predetermined pipeline, while also performing conditional,\\nbranching or looping when necessary. In the simplest case. the\\ngraph is a linear chain.\\nAfter conducting an in-depth analysis of current RAG meth-\\nods, we have identified a set of common RAG flow patterns,\\ndenoted as P. These patterns transcend various application\\ndomains and demonstrate a high level of consistency and\\nreusability, revealing the prevalent structures and behaviors in\\nprocess design. A RAG flow pattern can be defined as P =\\n{Mϕ1 : {Op1} →Mϕ2 : {Op2} →. . .→ Mϕn : {Opn}}\\nA. Linear Pattern\\nThe modules in the modular RAG system are organized in'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='process design. A RAG flow pattern can be defined as P =\\n{Mϕ1 : {Op1} →Mϕ2 : {Op2} →. . .→ Mϕn : {Opn}}\\nA. Linear Pattern\\nThe modules in the modular RAG system are organized in\\na linear way, and can be described as Algorithm 1.\\nPlinear = {M1 → M2 → . . .→ Mn} (25)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='10\\nFig. 4. Linear RAG flow pattern. Each module is processed in a fixed\\nsequential order.\\nFig. 5. RRR [24] is a typical linear flow that introduces a learnable query\\nrewrite module before retrieval. This module employs reinforcement based on\\nthe output results of the LLM.\\nThe linear flow pattern is the simplest and most com-\\nmonly used pattern. As shown in Figure 4, the full linear\\nRAG flow pattern mainly includes pre-retrieval processing,\\nretrieval, post-retrieval processing, and generation modules.\\nPlinearfull = {Mindexing → Mpre-retrieval → Mretrieval →\\nMpost-retrieval → Mgenerate}. If there are no pre-retrieval and\\npost-retrieval modules, it follows the Naive RAG paradigm.\\nAlgorithm 1 Linear RAG Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM, pre-processing function fpre, post-\\nprocessing function fpost\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='guage model LLM, pre-processing function fpre, post-\\nprocessing function fpost\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve documents related to the pre-\\nprocessed query\\n4: ˆDq′\\n← fpost(q′, Dq′\\n) // Post-process the retrieved docu-\\nments\\n5: ˆy ← LLM([q, ˆDq′\\n]) // Generate output using the lan-\\nguage model with the original query and post-processed\\ndocuments\\n6: return ˆy // Return the final output\\nCommon linear RAG flow involves a query transform\\nmodule (such as rewrite or HyDE operators) at the pre-retrieval\\nstage and utilize rerank at the post-retrieval stage. Rewrite-\\nRetrieve-Read (RRR) [24] is a typical linear structure. As\\nillustrated in Figure 5, the query rewrite module frewrite is a\\nsmaller trainable language model fine-tuned on T5-large, and\\nin the context of reinforcement learning, the optimization of\\nthe rewriter is formalized as a Markov decision process, with'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='smaller trainable language model fine-tuned on T5-large, and\\nin the context of reinforcement learning, the optimization of\\nthe rewriter is formalized as a Markov decision process, with\\nthe final output of the LLM serving as the reward. The retriever\\nutilizes a sparse encoding model, BM25.\\nB. Conditional Pattern\\nThe RAG flow with conditional structure involves select-\\ning different RAG pipeline based on different conditions,\\nas illustrated in Figure 6. A detailed definition is shown in\\nAlgorithm 2. Typically, pipleline selection is accomplished\\nFig. 6. The conditional flow pattern. There is a routing module that controls\\nwhich RAG flow the query is directed to. Typically, different flows are used for\\nvarious configurations to meet the general requirements of the RAG system.\\nFig. 7. Pre-retrieval branching flow pattern.Each branch performs retrieval\\nand generation separately, and then they are aggregated at the end.\\nthrough a routing module that determines the next module\\nin the flow.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='and generation separately, and then they are aggregated at the end.\\nthrough a routing module that determines the next module\\nin the flow.\\nPconditional = {Mi\\nfr\\n− →Mj ∨ Mk} (26)\\nWhere\\nfr\\n− →represents that based on routing function fr(·), the\\nflow can go to module Mj or Mk.\\nAlgorithm 2 Conditional RAG Flow Pattern\\nRequire: original query q, documents D, language model\\nLM, retriever R, routing function fr\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← QueryTransform(q) // Pre-process the initial query\\nif needed\\n3: D′ ← R(q′, D) // Retrieve or update documents related\\nto the query\\n4: Mnext ← fr(q′, D′) // Determine the next module using\\nthe routing function\\n5: if Mnext = Mj then\\n6: ˆy ← Mj(q′, D′) // Execute module Mj\\n7: else if Mnext = Mk then\\n8: ˆy ← Mk(q′, D′) Mk\\n9: end if\\n10: return ˆy\\nPipeline selection is determined by the nature of the ques-\\ntion, directing different flows tailored to specific scenarios. For\\nexample, the tolerance for responses generated by LLMs varies'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Pipeline selection is determined by the nature of the ques-\\ntion, directing different flows tailored to specific scenarios. For\\nexample, the tolerance for responses generated by LLMs varies\\nacross questions related to serious issues, political matters,\\nor entertainment topics. These routing flow often diverge in\\nterms of retrieval sources, retrieval processes, configurations,\\nmodels, and prompts.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='11\\nFig. 8. Post-retrieval branching flow pattern.Only one retrieval performed, and\\nthen generation is carried out separately for each retrieved document chunks,\\nfollowed by aggregation.\\nC. Branching\\nIn many cases, the RAG flow system may have multiple\\nparallel running branches , usually to increase the diver-\\nsity of generated results. Assuming multiple branches bi are\\ngenerated in module B = Msplit(·) = {b1, b2, . . . , bm}.\\nFor each branch bi ∈ B, the same or different RAG pro-\\ncesses can be executed, passing through multiple processing\\nmodules {M1, M2, . . . , Mk} to obtain branch output result\\npi = Mik(. . . Mi2(Mi1(bi)) . . .). The results of multiple\\nbranches are aggregated using an aggregation function to\\nobtain intermediate output results. ˆO = Mmerge({pi | bi ∈\\nB}). However, aggregation is not necessarily the end of the\\nRAG flow, as it can continue to connect to other modules,\\nMjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='B}). However, aggregation is not necessarily the end of the\\nRAG flow, as it can continue to connect to other modules,\\nMjn(. . . Mj2(Mj1( ˆO)) . . .). For example, after aggregating\\nmultiple model responses, they can continue through a val-\\nidation module. Therefore, the entire branch flow pattern can\\nbe represented as:\\nPbranch =Mjn(. . . Mj1(Mmerge({Mik\\n(. . . Mi1(bi) . . .) | bi ∈ Msplit(q)})) . . .) (27)\\nAlgorithm 3 Pre-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, query expand mod-\\nule Mexpand, retriever Mretrieve, language model LLM,\\nmerge module Mmerge\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q′ ← Mexpand(q) // Expand the original query to multiple\\nsub-queries\\n3: for all q′\\ni ∈ Q′ do\\n4: D′\\ni ← Mretrieve(q′\\ni, D) // Retrieve documents for each\\nsub-query\\n5: Gi ← ∅// Initialize an empty set for generated results\\nof the sub-query\\n6: for all d′\\nij ∈ D′\\ni do\\n7: yij ← LLM([q′\\ni, d′\\nij]) // Generate results for each\\ndocument of the sub-query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='sub-query\\n5: Gi ← ∅// Initialize an empty set for generated results\\nof the sub-query\\n6: for all d′\\nij ∈ D′\\ni do\\n7: yij ← LLM([q′\\ni, d′\\nij]) // Generate results for each\\ndocument of the sub-query\\n8: Oi ← Oi ∪ {yij} // Add generated results to the set\\n9: end for\\n10: ˆy ← Mmerge(Oi) // Merge generated results of the sub-\\nquery into the final result\\n11: end for\\n12: return ˆy\\nThe RAG flow with a branching structure differs from\\nthe conditional approach in that it involves multiple parallel\\nbranches, as opposed to selecting one branch from multiple\\noptions in the conditional approach. Structurally, it can be\\ncategorized into two types, which are depicted in Figure 7\\nand Figure 8.\\nPre-Retrieval Branching (Multi-Query, Parallel Retrieval).\\nAs shown in Algorithm 3, the process involves initially taking\\na query q and expanding it through a module Mexpand to gen-\\nerate multiple sub-queries Q′. Each sub-query q′\\ni is then used\\nto retrieve relevant documents via Mretrieve, forming document\\nsets D′'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='a query q and expanding it through a module Mexpand to gen-\\nerate multiple sub-queries Q′. Each sub-query q′\\ni is then used\\nto retrieve relevant documents via Mretrieve, forming document\\nsets D′\\ni. These document sets, along with the corresponding\\nsub-queries, are fed into a generation module Mgenerate to\\nproduce a set of answers Gi. Ultimately, all these generated\\nanswers are combined using a merging module Mmerge to\\nform the final result y. This entire flow can be mathematically\\nrepresented as:\\nPbranchpre =Mmerge(q′\\ni∈Mexpand(q){Mgenerate(q′\\ni, d′\\nij) |\\nd′\\nij ∈ Mretrieve(q′\\ni)}) (28)\\nPost-Retrieval Branching (Single Query, Parallel Genera-\\ntion). As shown in Algorithm 4, in the post-retrieval branching\\npattern, the process starts with a single query q which is\\nused to retrieve multiple document chunks through a retrieval\\nmodule Mretrieve, resulting in a set of documents Dq. Each\\ndocument dq\\ni from this set is then independently processed by'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='used to retrieve multiple document chunks through a retrieval\\nmodule Mretrieve, resulting in a set of documents Dq. Each\\ndocument dq\\ni from this set is then independently processed by\\na generation module Mgenerate to produce a set of generated\\nresults G. These results are subsequently merged using a\\nmerge module Mmerge to form the final result y. The process\\ncan be succinctly represented as y = Mmerge(Oi), where Oi is\\nthe collection of all generated results from each document dq\\ni\\nin Dq. Therefore, the entire process can be represented as:\\nPbranchpost = Mmerge({Mgenerate(dq\\ni ) | dq\\ni ∈ Mretrieve(q)})\\n(29)\\nAlgorithm 4 Post-retrieval Branching Flow Pattern\\nRequire: original query q, documents D, retriever R, lan-\\nguage model LLM, merge module Mmerge\\nEnsure: final output ˆy\\n1: Initialize:\\n2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve a set of documents based on\\nthe pre-processed query\\n4: G ← ∅// Initialize an empty set to store generated results'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='2: q′ ← fpre(q) // Pre-process the original query\\n3: Dq′\\n← R(q′, D) // Retrieve a set of documents based on\\nthe pre-processed query\\n4: G ← ∅// Initialize an empty set to store generated results\\n5: for all di ∈ Dq′\\ndo\\n6: yi ← LLM([q, di]) // Generate results independently\\nfor each document chunk using the language model\\n7: Oi ← Oi ∪ {yi} // Add the generated result to the set\\nof results\\n8: end for\\n9: ˆy ← Mmerge(Oi) // Merge all generated results using the\\nmerge function\\n10: return ˆy\\nREPLUG [55] embodies a classic post-retrieval branching\\nstructure, wherein the probability of each token is predicted\\nfor each branch. Through weighted possibility ensemble, the\\ndifferent branches are aggregated, and the final generation'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='12\\nFig. 9. The RAG flow in REPLUG [55], which follows a typical post-retrieval\\nbranching pattern. Each retrieved chunks undergoes parallel generation, and\\nthen they are aggregated using a weighted probability ensemble.\\nresult is used to fine-tune the retriever, known as Contriever,\\nthrough feedback.\\nD. Loop Pattern\\nThe RAG flow with a loop structure, as an important char-\\nacteristic of Modular RAG, involves interdependent retrieval\\nand generation steps. It typically includes a scheduling module\\nfor flow control. The modular RAG system can be abstracted\\nas a directed graph G = (V, E), where V is the set of vertices\\nrepresenting the various modules Mi in the system, and E is\\nthe set of edges representing the control flow or data flow be-\\ntween modules. If there is a vertex sequence Mi1 , Mi2 , ..., Min\\nsuch that Min can reach Mi1 (i.e., Min → Mi1 ), then this\\nRAG system forms a loop. If Mj is the successor module of\\nMi and Mi decides whether to return to Mj or a previous'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='such that Min can reach Mi1 (i.e., Min → Mi1 ), then this\\nRAG system forms a loop. If Mj is the successor module of\\nMi and Mi decides whether to return to Mj or a previous\\nmodule Mk through a Judge module, it can be represented\\nas: Mi\\nJudge\\n− − − →Mj or Mi\\nJudge\\n− − − →Mk where Mk is the\\npredecessor module of Mj. If Mi return to Mj, it can be\\nrepresented as: ∃Judge(Mi, Mj) s.t. (Mi, Mj) ∈ E and\\nJudge(Mi, Mj) = true. If the Judge module not to return\\nto any previous module, it can be represented as: ∀Mi ∈\\nV, Judge(Mi, Mj) = false for all Mj that are predecessors\\nof Mi. Loop pattern can be further categorized into iterative,\\nrecursive, and adaptive (active) retrieval approaches.\\nIterative retrieval At times, a single retrieval and genera-\\ntion may not effectively address complex questions requiring\\nextensive knowledge. Therefore, an iterative approach can be\\nused in RAG (see Algorithm 5), typically involving a fixed\\nnumber of iterations for retrieval. At step t, given the query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='extensive knowledge. Therefore, an iterative approach can be\\nused in RAG (see Algorithm 5), typically involving a fixed\\nnumber of iterations for retrieval. At step t, given the query\\nqt and the previous output sequence y<t = [ y0, . . . , yt−1] ,\\niterations proceed under the condition that t is less than the\\nmaximum allowed iterations T. In each loop, it retrieves a\\ndocument chunks Dt−1 using the last output yt−1 and the\\ncurrent query qt. Subsequently, a new output yt is generated.\\nThe continuation of the iteration is determined by a Judge\\nmodule, which makes its decision based on the yt, y<t, qt,\\nand the Dt−1.\\nAn exemplary case of iterative retrieval is ITER-\\nRETGEN [56] (Figure 11), which iterates retrieval-augmented\\ngeneration and generation-augmented retrieval. Retrieval-\\naugmented generation outputs a response to a task input based\\non all retrieved knowledge. In each iteration, ITER-RETGEN\\nleverages the model output from the previous iteration as a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='augmented generation outputs a response to a task input based\\non all retrieved knowledge. In each iteration, ITER-RETGEN\\nleverages the model output from the previous iteration as a\\nspecific context to help retrieve more relevant knowledge.\\nFig. 10. Loop flow pattern. Typically, a RAG system performs multiple rounds\\nof retrieval and generation. It can be categorized into three forms: iterative,\\nrecursive, and adaptive.\\nAlgorithm 5 Iterative RAG Flow Pattern\\nRequire: original query q, documents D, maximum iterative\\ntimes T, language model LLM, retriever R, initial output\\ny<1 = ∅\\nEnsure: final output ˆy\\n1: Initialize:\\n2: qt ← q // Initialize query for the first iteration\\n3: y<1 ← ∅// Initialize previous outputs as empty\\n4: t ← 1 // Initialize iteration step\\n5: while t ≤ T do\\n6: qt ← QueryTransform(y<t−1, qt−1) // Generate query\\nbased on previous output and original query\\n7: Dt ← R(yt−1||qt, D) // Retrieve or update documents\\nrelated to the current query'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='5: while t ≤ T do\\n6: qt ← QueryTransform(y<t−1, qt−1) // Generate query\\nbased on previous output and original query\\n7: Dt ← R(yt−1||qt, D) // Retrieve or update documents\\nrelated to the current query\\n8: yt ← LLM([y<t−1, qt, Dt]) // Generate output using\\nthe language model\\n9: y<t ← [y<t−1, yt] // Update the list of previous outputs\\n10: if Judge(yt, q) = false then\\n11: break\\n12: end if\\n13: t ← t + 1 // Increment iteration step\\n14: end while\\n15: yfinal = synthesizeOutput(y≤t) // Synthesize final output\\nfrom the list of outputs\\n16: return ˆy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='13\\nFig. 11. ITER-RETGEN [56] is a typical iterative structure. Multiple rounds\\nof retrieval and generation are performed within the limit of the maximum\\nnumber of iterations.\\nTermination of the loop is determined by a predefined number\\nof iterations.\\nRecursive retrieval The characteristic feature of recursive\\nretrieval (see Algorithm 6), as opposed to iterative retrieval, is\\nits clear dependency on the previous step and its continuous\\ndeepening of retrieval. Typically, it follows a tree-like structure\\nand there is a clear termination mechanism as an exit condition\\nfor recursive retrieval. In RAG systems, recursive retrieval usu-\\nally involves query transform, relying on the newly rewritten\\nquery for each retrieval.\\nAlgorithm 6 Recursive RAG Flow Pattern\\nRequire: initial query q, document D, retriever R, language\\nmodel LM, maximum recursive depth Kmax\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q ← {q}\\n3: k ← 0 // Initialize recursion depth\\n4: while Q ̸= ∅ and k < Kmax do'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='model LM, maximum recursive depth Kmax\\nEnsure: final output ˆy\\n1: Initialize:\\n2: Q ← {q}\\n3: k ← 0 // Initialize recursion depth\\n4: while Q ̸= ∅ and k < Kmax do\\n5: Q′ ← ∅// To store queries for the next recursion level\\n6: for all q ∈ Q do\\n7: Dq ← R(q, D) // Retrieve or update documents\\nrelated to the current query\\n8: Y ← LM([q, Dq]) // Generate outputs using the\\nlanguage model\\n9: Q′′ ← deriveNewQueries(q, Dq, Y) // Derive new\\nqueries from generated outputs\\n10: for all q′ ∈ Q′′ do\\n11: if q′ /∈ Q′ and q′ /∈ Q then\\n12: Q′ ← Q′ ∪ {q′}\\n13: end if\\n14: end for\\n15: end for\\n16: Q ← Q′ // Update the set of queries for the next\\nrecursion\\n17: k ← k + 1 // Increment recursion depth\\n18: end while\\n19: ˆy = synthesizeOutput(Y ) // Synthesize final output from\\ngenerated outputs\\n20: return ˆy\\nA typical implementation of recursive retrieval, such as\\nToC [13] (see Figure 12 ), involves recursively executing RAC\\n(Recursive Augmented Clarification) to gradually insert sub-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='20: return ˆy\\nA typical implementation of recursive retrieval, such as\\nToC [13] (see Figure 12 ), involves recursively executing RAC\\n(Recursive Augmented Clarification) to gradually insert sub-\\nnodes into the clarification tree from the initial ambiguous\\nquestion (AQ). At each expansion step, paragraph re-ranking\\nis performed based on the current query to generate a disam-\\nFig. 12. RAG flow of ToC [13]. A typical characteristic of this process is\\nthat each recursive retrieval uses the new query generated from the previous\\nstep, thereby progressively deepening analysis of the original complex query.\\nbiguous Question (DQ). The exploration of the tree concludes\\nupon reaching the maximum number of valid nodes or the\\nmaximum depth. Once the clarification tree is constructed,\\nToC gathers all valid nodes and generates a comprehensive\\nlong-text answer to address AQ.\\nAdaptive (Active) retrieval With the advancement of RAG,\\nthere has been a gradual shift beyond passive retrieval to the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='long-text answer to address AQ.\\nAdaptive (Active) retrieval With the advancement of RAG,\\nthere has been a gradual shift beyond passive retrieval to the\\nemergence of adaptive retrieval (see Algorithm 7) , also known\\nas active retrieval, which is partly attributed to the powerful\\ncapabilities of LLM. This shares a core concept with LLM\\nAgent [57]. RAG systems can actively determine the timing\\nof retrieval and decide when to conclude the entire process and\\nproduce the final result. Based on the criteria for judgment,\\nthis can be further categorized into Prompt-base and Tuning-\\nbase approaches.\\nAlgorithm 7 Active RAG Flow Pattern\\nRequire: original query Q, documents D, maximum iterative\\ntimes T, language model LLM, retriever R\\nEnsure: final output ˆy\\n1: Initialize:\\n2: t ← 1 // Initialize loop step\\n3: qt ← q // Initialize query for the first iteration\\n4: y<1 ← ∅// Initialize previous outputs as empty\\n5: while t ≤ T do\\n6: Qt ← QueryTransform(y<t−1, qt−1) // Derive new'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='3: qt ← q // Initialize query for the first iteration\\n4: y<1 ← ∅// Initialize previous outputs as empty\\n5: while t ≤ T do\\n6: Qt ← QueryTransform(y<t−1, qt−1) // Derive new\\nquery from previous output and query\\n7: if Evaluate(Qt, y<t−1) then\\n8: Dt ← R(qt, D) // Retrieve documents based on the\\nnew query\\n9: yt ← LLM([qt, Dt]) // Generate output using the\\nlanguage model\\n10: else\\n11: yt ← ∅// Set output as empty if query evaluation is\\nfalse\\n12: end if\\n13: y<t ← [y<t−1, yt] // Update the list of previous outputs\\n14: if isOutputAcceptable(yt, y<t, qt) = false then\\n15: break // Break if the output is not acceptable\\n16: end if\\n17: t ← t + 1 // Increment iteration step\\n18: end while\\n19: ˆy = synthesizeOutput(y≤t) // Synthesize final output from\\nthe list of outputs\\n20: return ˆy\\nPrompt-base. The prompt-base approach involves control-\\nling the flow using Prompt Engineering to direct LLM. A'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='14\\nFig. 13. RAG flow of FLARE [14]. The generated provisional answer will\\nundergo confidence assessment. If it does not meet the required confidence\\nlevel, the process will return to the retrieval stage and generate anew. The\\nassessment criteria are implemented through prompt\\nFig. 14. RAG flow of SELF-RAG [28]. First, it prompt GPT-4 to obtain\\na suitable instruct fine-tuning dataset to fine-tune the deployed open-source\\nLLM. This allows the model to output four specific tokens during generation,\\nwhich are used to control the RAG process.\\ntypical implementation example is FLARE [14]. Its core\\nconcept is that LLMs should only retrieve when essential\\nknowledge is lacking, to avoid unnecessary or inappropriate\\nretrieval in an enhanced LM. FLARE iteratively generates the\\nnext provisional sentence and checks for the presence of low-\\nprobability tokens. If found, the system retrieves relevant docu-\\nments and regenerates the sentence. Tuning-base. The tuning-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='next provisional sentence and checks for the presence of low-\\nprobability tokens. If found, the system retrieves relevant docu-\\nments and regenerates the sentence. Tuning-base. The tuning-\\nbased approach involves fine-tuning LLM to generate special\\ntokens, thereby triggering retrieval or generation. This concept\\ncan be traced back to Toolformer [50], where the generation of\\nspecific content assists in invoking tools. In RAG systems, this\\napproach is used to control both retrieval and generation steps.\\nA typical case is Self-RAG [28](see Figure 14). Given an\\ninput prompt and the preceding generation result, first predict\\nwhether the special token Retrieve is helpful for enhancing\\nthe continued generation through retrieval. Then, if retrieval\\nis needed, the model generates a critique token to evaluate the\\nretrieved passage’s relevance. and a critique token to evaluate\\nif the information in the response is supported by the retrieved'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='is needed, the model generates a critique token to evaluate the\\nretrieved passage’s relevance. and a critique token to evaluate\\nif the information in the response is supported by the retrieved\\npassage. Finally, a critique token evaluates the overall utility of\\nthe response and selects the optimal result as the final output.\\nE. Tuning Pattern\\nRAG is continuously integrating with more LLM-related\\ntechnologies. In Modular RAG, many components are com-\\nposed of trainable language models. Through fine-tuning, the\\nperformance of the components and the compatibility with\\nthe overall flow can be further optimized. This section will\\nintroduce three main patterns of fine-tuning stages, namely\\nretriever fine-tuning, generator fine-tuning, and dual fine-\\ntuning.\\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.\\n1) Retriever FT: In the RAG flow, common methods for'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='tuning.\\nFig. 15. Retriever fine-tuning pattern, mainly includes direct SFT, adding\\ntrainable adapter, LM-supervised retrieval and LLM Reward RL.\\n1) Retriever FT: In the RAG flow, common methods for\\nfine-tuning the retriever is shown in Figure 15 ,which include:\\n• Direct supervised fine-tuning of the retriever. Construct-\\ning a specialized dataset for retrieval and fine-tuning the\\ndense retriever. For example, using open-source retrieval\\ndatasets or constructing one based on domain-specific\\ndata.\\n• Adding trainable adapter modules. Sometimes, direct\\nfine-tuning of the API-base embedding model (e.g., Ope-\\nnAI Ada-002 and Cohere) is not feasible. Incorporating\\nan adapter module can enhance the representation of\\nyour data. Additionally, the adapter module facilitates\\nbetter alignment with downstream tasks, whether for task-\\nspecific (e.g., PRCA [42]) or general purposes (e.g.,\\nAAR [58]).\\n• LM-supervised Retrieval (LSR). Fine-tuning the retriever\\nbased on the results generated by LLM.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='specific (e.g., PRCA [42]) or general purposes (e.g.,\\nAAR [58]).\\n• LM-supervised Retrieval (LSR). Fine-tuning the retriever\\nbased on the results generated by LLM.\\n• LLM Reward RL. Still using the LLM output results as\\nthe supervisory signal. Employing reinforcement learning\\nto align the retriever with the generator. The whole re-\\ntrieval process is disassembled in the form of a generative\\nMarkov chain.\\n2) Generator FT: The primary methods for fine-tuning a\\ngenerator in RAG flow is shown in Figure 16, which include:\\n• Direct supervised fine-tuning . Fine-tuning through an\\nexternal dataset can supplement the generator with ad-\\nditional knowledge. Another benefit is the ability to\\ncustomize input and output formats. By setting the Q&A\\nformat, LLM can understand specific data formats and\\noutput according to instructions.\\n• Distillation. When using on-premise deployment of open-\\nsource models, a simple and effective Optimization\\nmethod is to use GPT-4 to batch construct fine-tuning'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='output according to instructions.\\n• Distillation. When using on-premise deployment of open-\\nsource models, a simple and effective Optimization\\nmethod is to use GPT-4 to batch construct fine-tuning\\ndata to enhance the capabilities of the open-source model.\\n• RL from LLM/human feedback. Reinforcement learning\\nbased on feedback from the final generated answers. In\\naddition to using human evaluations, powerful LLMs can\\nalso serve as an evaluative judge.\\n3) Dual FT: In the RAG system, fine-tuning both the\\nretriever and the generator simultaneously is a unique feature\\nof the RAG system. It is important to note that the emphasis\\nof system fine-tuning is on the coordination between the\\nretriever and the generator. An exemplary implementation is\\nRA-DIT [27], which fine-tunes both the LLM and the retriever.\\nThe LM-ft component updates the LLM to maximize the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='15\\nFig. 16. Generator fine-tuning pattern, The main methods include SFT,\\ndistillation and RL from LLM/human feedback.\\nFig. 17. Dual fine-tuning pattern. In this mode, both the retriever and\\ngenerator participate in fine-tuning, and their preferences will be aligned.\\nlikelihood of the correct answer given the retrieval-augmented\\ninstructions while the R-ft component updates the retriever\\nto minimize the KL-Divergence between the retriever score\\ndistribution and the LLM preference.\\nVI. D ISCUSSION\\nIn this chapter, we explore the innovative horizons opened\\nby the modular RAG paradigm. We examine its compatibility\\nwith cutting-edge methodologies in the progression of RAG\\ntechnology, emphasizing its scalability. It not only fosters a\\nfertile ground for model innovation but also paves the way for\\nseamless adaptation to the dynamic requirements of various\\napplications.\\nA. Opportunities in Modular RAG\\nThe benefits of Modular RAG are evident, providing a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='seamless adaptation to the dynamic requirements of various\\napplications.\\nA. Opportunities in Modular RAG\\nThe benefits of Modular RAG are evident, providing a\\nfresh and comprehensive perspective on existing RAG-related\\nwork. Through modular organization, relevant technologies\\nand methods are clearly summarized.\\nFrom a research perspective. Modular RAG is highly\\nscalable, it empowers researchers to introduce innovative mod-\\nules and operators, leveraging a deep understanding of RAG’s\\nevolving landscape. This flexibility enables the exploration of\\nnew theoretical and practical dimensions in the field.\\nFrom an application perspective . The modularity of RAG\\nsystems simplifies their design and implementation. Users can\\ntailor RAG flows to fit their specific data, use cases, and\\ndownstream tasks, enhancing the adaptability of the system\\nto diverse requirements. Developers can draw from existing\\nflow architectures and innovate by defining new flows and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='downstream tasks, enhancing the adaptability of the system\\nto diverse requirements. Developers can draw from existing\\nflow architectures and innovate by defining new flows and\\npatterns that are tailored to various application contexts and\\ndomains. This approach not only streamlines the development\\nprocess but also enriches the functionality and versatility of\\nRAG applications.\\nB. Compatibility with new methods\\nModular RAG paradigm demonstrates exceptional compati-\\nbility with new developments. To gain a deeper understanding\\nof this, we list three typical scalability cases, which clearly\\nshows that Modular RAG paradigm provides robust support\\nand flexibility for the innovation and development of RAG\\ntechnology.\\n1) Recombination of the current modules: In this scenario,\\nno new modules or operators are proposed; rather, specific\\nproblems are addressed through the combination of existing\\nmodules.DR-RAG [59] employs a two-stage retrieval strategy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='no new modules or operators are proposed; rather, specific\\nproblems are addressed through the combination of existing\\nmodules.DR-RAG [59] employs a two-stage retrieval strategy\\nand classifier selection mechanism, incorporating a branching\\nretrieval structure. In the first stage, retrieving chunks relevant\\nto the query. In the second stage, the query is combined\\nindividually with each chunk retrieved in the first stage, and a\\nparallel secondary retrieval is conducted. The retrieved content\\nis then input into a classifier to filter out the most relevant\\ndynamic documents. This ensures that the retrieved documents\\nare highly relevant to the query while reducing redundant\\ninformation. DR-RAG improved retrieval method significantly\\nenhances the accuracy and efficiency of answers, bolstering\\nRAG’s performance in multi-hop question-answering scenar-\\nios.\\n2) New flow without adding new operators.: This refers\\nto redesigning the processes for retrieval and generation to'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='RAG’s performance in multi-hop question-answering scenar-\\nios.\\n2) New flow without adding new operators.: This refers\\nto redesigning the processes for retrieval and generation to\\naddress more complex scenarios without proposing new mod-\\nules. The core idea of PlanRAG [18] lies in its introduction of\\na preliminary planning stage, a crucial step that occurs before\\nretrieval and generation. Initially, the system employs a judge\\nmodule to assess whether the current context necessitates the\\nformulation of a new plan or adjustments to an existing one.\\nWhen encountering a problem for the first time, the system\\ninitiates the planning process, while in subsequent interactions,\\nit decides whether to execute re-planning based on previous\\nplans and retrieved data.\\nNext, the system devises an execution plan tailored to the\\nquery, treating this process as a logical decomposition of\\ncomplex queries. Specifically, PlanRAG uses a query expan-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Next, the system devises an execution plan tailored to the\\nquery, treating this process as a logical decomposition of\\ncomplex queries. Specifically, PlanRAG uses a query expan-\\nsion module to extend and refine the query. For each derived\\nsub-query, the system conducts targeted retrieval. Following\\nretrieval, another judge module evaluates the current results to\\ndecide whether further retrieval is required or if it should return\\nto the planning stage for re-planning. Through this strategy,\\nPlanRAG is able to handle complex decision-making problems\\nthat require multi-step data analysis more efficiently.\\n3) New flow derived from new operators.: New operators\\noften introduce novel flow design, exemplified by Multi-Head\\nRAG [60]. Existing RAG solutions do not focus on queries that\\nmay require retrieving multiple documents with significantly\\ndifferent content. Such queries are common but difficult to\\nhandle because embeddings of these documents may be far'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='may require retrieving multiple documents with significantly\\ndifferent content. Such queries are common but difficult to\\nhandle because embeddings of these documents may be far\\napart in the embedding space. Multi-Head RAG addresses this\\nby designing a new retriever that uses the activations of the\\nmulti-head attention layers of the Transformer, rather than the\\ndecoder layers, as keys for retrieving multifaceted documents.\\nDifferent attention heads can learn to capture different aspects\\nof the data. By using the corresponding activation results,\\nembeddings that represent different aspects of the data items\\nand the query can be generated, thereby enhancing the retrieval\\naccuracy for complex queries.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='16\\nVII. C ONCLUSION\\nRAG is emerging as a pivotal technology for LLM applica-\\ntions. As technological landscapes evolve and the intricacies of\\napplication requirements escalate, RAG systems are being en-\\nhanced by integrating a diverse suite of technologies, thereby\\nachieving a higher level of complexity and functionality. This\\npaper introduces the innovative paradigm of Modular RAG.\\nThis approach systematically disassembles the complex archi-\\ntecture of RAG systems into well-defined, discrete functional\\nmodules. Each module is meticulously characterized by its\\nspecific operational functions, ensuring clarity and precision.\\nTherefore, the entire system is composed of those modules\\nand operators, akin to Lego bricks. By conducting an in-\\ndepth analysis of numerous studies, the paper also distills\\ncommon RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='common RAG design patterns and scrutinizes key case studies\\nto illustrate these patterns in practice.\\nModular RAG not only offers a structured framework for\\nthe design and application of RAG systems but also en-\\nables a scenario-based customization of these systems. The\\nmodularity inherent in this design facilitates ease of tracking\\nand debugging, significantly enhancing the maintainability and\\nscalability of RAG systems. Furthermore, Modular RAG opens\\nup new avenues for the future progression of RAG technology.\\nIt encourages the innovation of novel functional modules and\\nthe crafting of innovative workflows, thereby driving forward\\nthe frontiers of RAG systems.\\nREFERENCES\\n[1] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[2] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='lucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[2] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and\\nH. Wang, “Retrieval-augmented generation for large language models:\\nA survey,” arXiv preprint arXiv:2312.10997 , 2023.\\n[3] Z. Xu, M. J. Cruz, M. Guevara, T. Wang, M. Deshpande, X. Wang,\\nand Z. Li, “Retrieval-augmented generation with knowledge graphs\\nfor customer service question answering,” in Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2024, pp. 2905–2909.\\n[4] C. Zhang, S. Wu, H. Zhang, T. Xu, Y . Gao, Y . Hu, and E. Chen,\\n“Notellm: A retrievable large language model for note recommendation,”\\nin Companion Proceedings of the ACM on Web Conference 2024 , 2024,\\npp. 170–179.\\n[5] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='pp. 170–179.\\n[5] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[6] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[7] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[8] D. S. Asudani, N. K. Nagwani, and P. Singh, “Impact of word embedding\\nmodels on text analytics in deep learning environment: a review,”\\nArtificial intelligence review, vol. 56, no. 9, pp. 10 345–10 425, 2023.\\n[9] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='Y . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[10] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\\n“Large language model based long-tail query rewriting in taobao search,”\\narXiv preprint arXiv:2311.03758 , 2023.\\n[11] Y . Xi, J. Lin, W. Liu, X. Dai, W. Zhang, R. Zhang, R. Tang, and\\nY . Yu, “A bird’s-eye view of reranking: from list level to page level,”\\nin Proceedings of the Sixteenth ACM International Conference on Web\\nSearch and Data Mining , 2023, pp. 1075–1083.\\n[12] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[13] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.\\n[14] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,” arXiv\\npreprint arXiv:2305.06983, 2023.\\n[15] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,\\nand J. Larson, “From local to global: A graph rag approach to query-\\nfocused summarization,” arXiv preprint arXiv:2404.16130 , 2024.\\n[16] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[17] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi, Z. Wang,\\nS. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\\ngeneration,” arXiv preprint arXiv:2407.01219 , 2024.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='S. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented\\ngeneration,” arXiv preprint arXiv:2407.01219 , 2024.\\n[18] M. Lee, S. An, and M.-S. Kim, “Planrag: A plan-then-retrieval aug-\\nmented generation for generative large language models as decision\\nmakers,” arXiv preprint arXiv:2406.12430 , 2024.\\n[19] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158 , 2023.\\n[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems , vol. 33, pp. 9459–9474, 2020.\\n[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[21] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\\n“Improving language models by retrieving from trillions of tokens,” in\\nInternational conference on machine learning. PMLR, 2022, pp. 2206–\\n2240.\\n[22] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[23] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509 , 2022.\\n[24] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='ing for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[25] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” in Proceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track and\\nGovernment Track), J. Campbell, S. Larocca, J. Marciano, K. Savenkov,\\nand A. Yanishevsky, Eds. Orlando, USA: Association for Machine\\nTranslation in the Americas, Sep. 2022, pp. 202–209. [Online].\\nAvailable: https://aclanthology.org/2022.amta-upg.14\\n[26] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faith-\\nful and interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez,\\nJ. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\\ninstruction tuning,” arXiv preprint arXiv:2310.01352 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-augmented dual\\ninstruction tuning,” arXiv preprint arXiv:2310.01352 , 2023.\\n[28] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning\\nto retrieve, generate, and critique through self-reflection,” arXiv preprint\\narXiv:2310.11511, 2023.\\n[29] Y . Huang and J. Huang, “A survey on retrieval-augmented text gen-\\neration for large language models,” arXiv preprint arXiv:2404.10981 ,\\n2024.\\n[30] Y . Hu and Y . Lu, “Rag and rau: A survey on retrieval-augmented\\nlanguage model in natural language processing,” arXiv preprint\\narXiv:2404.19543, 2024.\\n[31] Y . Ding, W. Fan, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and\\nQ. Li, “A survey on rag meets llms: Towards retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2405.06211 , 2024.\\n[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,\\nand B. Cui, “Retrieval-augmented generation for ai-generated content:'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[32] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y . Geng, F. Fu, L. Yang, W. Zhang,\\nand B. Cui, “Retrieval-augmented generation for ai-generated content:\\nA survey,” arXiv preprint arXiv:2402.19473 , 2024.\\n[33] S. Yang, “Advanced rag 01: Small-to-\\nbig retrieval,” https://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='17\\n[34] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730 , 2023.\\n[35] D. Zhou, N. Sch ¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al. , “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[36] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[37] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\\n[38] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='and D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.\\n[39] H. Cao, “Recent advances in text embedding: A comprehensive review\\nof top-performing methods on the mteb benchmark,” arXiv preprint\\narXiv:2406.01607, 2024.\\n[40] BAAI, “Flagembedding,” https://github.com/FlagOpen/FlagEmbedding,\\n2023.\\n[41] Z. Li, X. Zhang, Y . Zhang, D. Long, P. Xie, and M. Zhang, “Towards\\ngeneral text embeddings with multi-stage contrastive learning,” arXiv\\npreprint arXiv:2308.03281, 2023.\\n[42] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao,\\n“Prca: Fitting black-box large language models for retrieval question an-\\nswering via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172 , 2023.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='[43] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\\nP. Liang, “Lost in the middle: How language models use long contexts,”\\narXiv preprint arXiv:2307.03172 , 2023.\\n[44] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[45] L. Xia, J. Xu, Y . Lan, J. Guo, and X. Cheng, “Learning maximal\\nmarginal relevance model via directly optimizing diversity evaluation\\nmeasures,” in Proceedings of the 38th international ACM SIGIR con-\\nference on research and development in information retrieval , 2015, pp.\\n113–122.\\n[46] Cohere, “Say goodbye to irrelevant search results: Cohere rerank is\\nhere,” https://txt.cohere.com/rerank/, 2023.\\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context sce-'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='here,” https://txt.cohere.com/rerank/, 2023.\\n[47] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context sce-\\nnarios via prompt compression,” arXiv preprint arXiv:2310.06839, 2023.\\n[48] R. Litman, O. Anschel, S. Tsiper, R. Litman, S. Mazor, and R. Man-\\nmatha, “Scatter: selective context attentional scene text recognizer,” in\\nproceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, 2020, pp. 11 962–11 972.\\n[49] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092 , 2023.\\n[50] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='can teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[51] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in neural\\ninformation processing systems , vol. 35, pp. 27 730–27 744, 2022.\\n[52] S. J. Semnani, V . Z. Yao, H. C. Zhang, and M. S. Lam, “Wikichat:\\nStopping the hallucination of large language model chatbots by few-\\nshot grounding on wikipedia,” arXiv preprint arXiv:2305.14292 , 2023.\\n[53] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[54] G. V . Cormack, C. L. Clarke, and S. Buettcher, “Reciprocal rank\\nfusion outperforms condorcet and individual rank learning methods,”\\nin Proceedings of the 32nd international ACM SIGIR conference on\\nResearch and development in information retrieval , 2009, pp. 758–759.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='in Proceedings of the 32nd international ACM SIGIR conference on\\nResearch and development in information retrieval , 2009, pp. 758–759.\\n[55] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box language\\nmodels,” arXiv preprint arXiv:2301.12652 , 2023.\\n[56] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[57] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou et al., “Metagpt: Meta programming for\\nmulti-agent collaborative framework,” arXiv preprint arXiv:2308.00352,\\n2023.\\n[58] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20251216181331', 'source': '..\\\\data\\\\pdf_files\\\\modular_rag.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'modular_rag.pdf', 'file_type': 'pdf'}, page_content='improves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[59] Z. Hei, W. Wei, W. Ou, J. Qiao, J. Jiao, Z. Zhu, and G. Song,\\n“Dr-rag: Applying dynamic document relevance to retrieval-augmented\\ngeneration for question-answering,” arXiv preprint arXiv:2406.07348 ,\\n2024.\\n[60] M. Besta, A. Kubicek, R. Niggli, R. Gerstenberger, L. Weitzen-\\ndorf, M. Chi, P. Iff, J. Gajda, P. Nyczyk, J. M ¨uller et al. , “Multi-\\nhead rag: Solving multi-aspect problems with llms,” arXiv preprint\\narXiv:2406.05085, 2024.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70f3e2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (320, 384)\n",
      "Adding 320 documents to vector store...\n",
      "Successfully added 320 documents to vector store\n",
      "Total documents in collection: 320\n"
     ]
    }
   ],
   "source": [
    "# Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in vector store\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "154f7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    '''Handles query-based retrieval from the vector store'''\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        '''\n",
    "        Initialize the RAG retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store (VectorStore): Instance of the vector store\n",
    "            embedding_manager (EmbeddingManager): Instance of the embedding manager\n",
    "        '''\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce9cf380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is In context learning?'\n",
      "Top K: 5, Score threshold: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_2d9d7fcd_60',\n",
       "  'content': 'poses better ways of formulating the problem (Zhao\\net al., 2021; Holtzman et al., 2021; Min et al.,\\n2021a), better ways of choosing labeled exam-\\nples for the demonstrations (Liu et al., 2021; Lu\\net al., 2021; Rubin et al., 2021), meta-training\\nwith an explicit in-context learning objective (Chen\\net al., 2021; Min et al., 2021b), and learning to\\nfollow instructions as a variant of in-context learn-\\ning (Mishra et al., 2021b; Efrat and Levy, 2020;\\nWei et al., 2022a; Sanh et al., 2022). At the\\nsame time, some work reports brittleness and over-\\nsensitivity for in-context learning (Lu et al., 2021;\\nZhao et al., 2021; Mishra et al., 2021a).\\nRelatively less work has been done to understand\\nwhy in-context learning works. Xie et al. (2022)\\nprovide theoretical analysis that in-context learn-\\ning can be formalized as Bayesian inference that\\nCirculation revenue has increased by 5% in Finland.         \\\\n    Positive \\nPanostaja did not disclose the purchase price.                  \\\\n    Neutral',\n",
       "  'metadata': {'moddate': '2022-10-21T00:20:25+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'subject': '',\n",
       "   'trapped': '/False',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'keywords': '',\n",
       "   'page': 1,\n",
       "   'page_label': '2',\n",
       "   'source_file': 'incontext_learning.pdf',\n",
       "   'creationdate': '2022-10-21T00:20:25+00:00',\n",
       "   'author': '',\n",
       "   'content_length': 996,\n",
       "   'total_pages': 19,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf',\n",
       "   'title': '',\n",
       "   'doc_index': 60,\n",
       "   'creator': 'LaTeX with hyperref'},\n",
       "  'similarity_score': 0.2288280725479126,\n",
       "  'distance': 0.7711719274520874,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_36ad46a2_115',\n",
       "  'content': 'in-context-learning.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\\n2021. Learning to retrieve prompts for in-context\\nlearning. arXiv preprint arXiv:2112.08633.\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun\\nRaja, Manan Dey, M Saiful Bari, Canwen Xu, Ur-\\nmish Thakker, Shanya Sharma, Eliza Szczechla,\\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\\nJiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\\nGao, Tali Bers, Thomas Wolf, and Alexander M.\\nRush. 2022. Multitask prompted training enables\\nzero-shot task generalization. In ICLR.',\n",
       "  'metadata': {'content_length': 865,\n",
       "   'file_type': 'pdf',\n",
       "   'moddate': '2022-10-21T00:20:25+00:00',\n",
       "   'title': '',\n",
       "   'doc_index': 115,\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf',\n",
       "   'author': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2022-10-21T00:20:25+00:00',\n",
       "   'page_label': '11',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'total_pages': 19,\n",
       "   'page': 10,\n",
       "   'source_file': 'incontext_learning.pdf',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.2168748378753662,\n",
       "  'distance': 0.7831251621246338,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_dae6144a_80',\n",
       "  'content': '(Section 5.1–5.3). We then additionally discuss the\\ntrend of the models meta-trained with an in-context\\nlearning objective (Section 5.4). For all experi-\\nments, models are evaluated on ﬁve classiﬁcation',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf',\n",
       "   'creationdate': '2022-10-21T00:20:25+00:00',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'subject': '',\n",
       "   'moddate': '2022-10-21T00:20:25+00:00',\n",
       "   'source_file': 'incontext_learning.pdf',\n",
       "   'doc_index': 80,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'page': 4,\n",
       "   'title': '',\n",
       "   'content_length': 202,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 19,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.18128979206085205,\n",
       "  'distance': 0.818710207939148,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_82d8687c_54',\n",
       "  'content': 'hurts performance on a range of classiﬁcation\\nand multi-choce tasks, consistently over 12 dif-\\nferent models including GPT-3. Instead, we\\nﬁnd that other aspects of the demonstrations\\nare the key drivers of end task performance, in-\\ncluding the fact that they provide a few exam-\\nples of (1) the label space, (2) the distribution\\nof the input text, and (3) the overall format of\\nthe sequence. Together, our analysis provides\\na new way of understanding how and why\\nin-context learning works, while opening up\\nnew questions about how much can be learned\\nfrom large language models through inference\\nalone.\\n1 Introduction\\nLarge language models (LMs) have shown impres-\\nsive performance on downstream tasks by simply\\nconditioning on a few input-label pairs (demonstra-\\ntions); this type of inference has been referred to as\\nin-context learning (Brown et al., 2020). Despite in-\\ncontext learning consistently outperforming zero-\\nshot inference on a wide range of tasks (Zhao et al.,',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'author': '',\n",
       "   'creationdate': '2022-10-21T00:20:25+00:00',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf',\n",
       "   'doc_index': 54,\n",
       "   'source_file': 'incontext_learning.pdf',\n",
       "   'page': 0,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'title': '',\n",
       "   'page_label': '1',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'subject': '',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 19,\n",
       "   'moddate': '2022-10-21T00:20:25+00:00',\n",
       "   'content_length': 976},\n",
       "  'similarity_score': 0.1679779291152954,\n",
       "  'distance': 0.8320220708847046,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_2812af2b_59',\n",
       "  'content': '2 Related Work\\nLarge language models have been key to strong per-\\nformance in a wide range of downstream tasks (De-\\nvlin et al., 2019; Radford et al., 2019; Liu et al.,\\n2019; Raffel et al., 2020; Lewis et al., 2020). While\\nﬁnetuning has been a popular approach to transfer\\nto new tasks (Devlin et al., 2019), it is often imprac-\\ntical to ﬁnetune a very large model (e.g. ≥10B pa-\\nrameters). Brown et al. (2020) propose in-context\\nlearning as an alternative way to learn a new task.\\nAs depicted in Figure 2, the LM learns a new task\\nvia inference alone by conditioning on a concatena-\\ntion of the training data as demonstrations, without\\nany gradient updates.\\nIn-context learning has been the focus of signif-\\nicant study since its introduction. Prior work pro-\\nposes better ways of formulating the problem (Zhao\\net al., 2021; Holtzman et al., 2021; Min et al.,\\n2021a), better ways of choosing labeled exam-\\nples for the demonstrations (Liu et al., 2021; Lu',\n",
       "  'metadata': {'page': 1,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'subject': '',\n",
       "   'page_label': '2',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'content_length': 956,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\incontext_learning.pdf',\n",
       "   'moddate': '2022-10-21T00:20:25+00:00',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'doc_index': 59,\n",
       "   'keywords': '',\n",
       "   'trapped': '/False',\n",
       "   'source_file': 'incontext_learning.pdf',\n",
       "   'creationdate': '2022-10-21T00:20:25+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'total_pages': 19},\n",
       "  'similarity_score': 0.1631106734275818,\n",
       "  'distance': 0.8368893265724182,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is In context learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0a7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83c32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
